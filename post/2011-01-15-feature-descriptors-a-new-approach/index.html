<!doctype html><html dir=ltr lang=en data-theme class=html><head><title>Eugene Khvedchenya
|
Feature descriptors: A new approach</title><meta charset=utf-8><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Eugene Khvedchenya"><meta name=description content="A personal blog about Computer Vision, AI, Kaggle, Open-Source and research. You will never read this in books."><link rel=stylesheet href=/scss/main.min.b2e0cb07595e3519ab1193bb421914e06c0e26b0cc561fef23b3c6131d4d2ffa.css integrity="sha256-suDLB1leNRmrEZO7QhkU4GwOJrDMVh/vI7PGEx1NL/o=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.31b0a1f317f55c529a460897848c97436bb138b19c399b37de70d463a8bf6ed5.css integrity="sha256-MbCh8xf1XFKaRgiXhIyXQ2uxOLGcOZs33nDUY6i/btU=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=canonical href=https://computer-vision-talks.com/post/2011-01-15-feature-descriptors-a-new-approach/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.560a26330d27ff44a44e83b53cd07a95d4230a65930d31c5c76a8d481e5b35bf.js integrity="sha256-VgomMw0n/0SkToO1PNB6ldQjCmWTDTHFx2qNSB5bNb8=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Feature descriptors: A new approach"><meta name=twitter:description content="Last year I was tightly connected with image processing and feature tracking/matching. For my needs I’ve used SURF and later RIFF descriptors. Both of them have strong advantages and but… SURF descriptor robustness are compensated by it’s computational cost. RIFF descriptor extracts much faster but not robust enough for my needs. My needs are very simple – doing markerless AR on mobile phone. So, we (me and two other co-authors) decided to develop our own descriptor."><meta property="og:title" content="Feature descriptors: A new approach"><meta property="og:description" content="Last year I was tightly connected with image processing and feature tracking/matching. For my needs I’ve used SURF and later RIFF descriptors. Both of them have strong advantages and but… SURF descriptor robustness are compensated by it’s computational cost. RIFF descriptor extracts much faster but not robust enough for my needs. My needs are very simple – doing markerless AR on mobile phone. So, we (me and two other co-authors) decided to develop our own descriptor."><meta property="og:type" content="article"><meta property="og:url" content="https://computer-vision-talks.com/post/2011-01-15-feature-descriptors-a-new-approach/"><meta property="article:section" content="post"><meta property="article:published_time" content="2011-01-15T00:00:00+00:00"><meta property="article:modified_time" content="2011-01-15T00:00:00+00:00"><meta property="og:site_name" content="Computer Vision Talks"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"Feature descriptors: A new approach","headline":"Feature descriptors: A new approach","alternativeHeadline":"","description":"
      
        Last year I was tightly connected with image processing and feature tracking\/matching. For my needs I’ve used SURF and later RIFF descriptors. Both of them have strong advantages and but… SURF descriptor robustness are compensated by it’s computational cost. RIFF descriptor extracts much faster but not robust enough for my needs. My needs are very simple – doing markerless AR on mobile phone. So, we (me and two other co-authors) decided to develop our own descriptor.


      


    ","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/computer-vision-talks.com\/post\/2011-01-15-feature-descriptors-a-new-approach\/"},"author":{"@type":"Person","name":"Eugene Khvedchenya"},"creator":{"@type":"Person","name":"Eugene Khvedchenya"},"accountablePerson":{"@type":"Person","name":"Eugene Khvedchenya"},"copyrightHolder":{"@type":"Person","name":"Eugene Khvedchenya"},"copyrightYear":"2011","dateCreated":"2011-01-15T00:00:00.00Z","datePublished":"2011-01-15T00:00:00.00Z","dateModified":"2011-01-15T00:00:00.00Z","publisher":{"@type":"Organization","name":"Eugene Khvedchenya","url":"https://computer-vision-talks.com/","logo":{"@type":"ImageObject","url":"https:\/\/computer-vision-talks.com\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/computer-vision-talks.com\/post\/2011-01-15-feature-descriptors-a-new-approach\/","wordCount":"490","genre":[],"keywords":["opencv"]}</script></head><body class="body theme--light"><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/images/4ECfkGJr_400x400.jpg alt="profile picture"><div class=sidebar__introduction-title><a href=/>Computer Vision Talks</a></div><div class=sidebar__introduction-description><p>A personal blog about Computer Vision, AI, Kaggle, Open-Source and research. You will never read this in books.</p></div></div><ul class=sidebar__list><li class=sidebar__list-item><a href=https://www.linkedin.com/in/cvtalks/ target=_blank rel=noopener aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://twitter.com/cvtalks target=_blank rel=noopener aria-label=Twitter title=Twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://github.com/BloodAxe target=_blank rel=noopener aria-label=GitHub title=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=mailto:ekhvedchenya@computer-vision-talks.com target=_blank rel=noopener aria-label=E-Mail title=E-Mail><i class="fas fa-envelope fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
Eugene Khvedchenya
2022</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.9c062c557275acbaba71f0c7cd4024da3e3cc825d248bc4b2130811b0965330b.js integrity="sha256-nAYsVXJ1rLq6cfDHzUAk2j48yCXSSLxLITCBGwllMws=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-10687369-5","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/ title>Home</a></li><li class=nav__list-item><a href=/post/ title>Posts</a></li><li class=nav__list-item><a href=/open-source title>Open-Source</a></li><li class=nav__list-item><a href=/challenges title>Wins</a></li><li class=nav__list-item><a href=/publications title>Publications</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>Feature Descriptors: A New Approach</h1><ul class=post__meta><li class=post__meta-item><em class="fas fa-calendar-day post__meta-icon"></em>
<span class=post__meta-text>Sat, Jan 15, 2011</span></li><li class=post__meta-item><em class="fas fa-stopwatch post__meta-icon"></em>
<span class=post__meta-text>3-minute read</span></li></ul><p>Last year I was tightly connected with image processing and feature tracking/matching. For my needs I’ve used SURF and later RIFF descriptors. Both of them have strong advantages and but… SURF descriptor robustness are compensated by it’s computational cost. RIFF descriptor extracts much faster but not robust enough for my needs. My needs are very simple – doing markerless AR on mobile phone.</p><p>So, we (me and two other co-authors) decided to develop our own descriptor. And we succeeded! Our descriptor (We called it LAZY) meets following requirements:</p><ul><li>Scale invariant</li><li>Rotation invariant</li><li>Lighting invariant</li><li>Invariant to slight affine transformations</li><li>Works a lot faster than SURF (At least 2-3 times in comparison with OpenCV’s SURF implementation)</li></ul><p>Here is video demonstration of how our descriptor works on the real video:</p><p>On the next week (Approximately in Wednesday, 26th January) in I’ll publish additional charts with detailed information about speed, robustness and comparison to SURF.</p><h2 id=comments>Comments</h2><p><strong><a href=#36 title="2011-01-17 16:06:06">EKhvedchenya</a>:</strong> For sure! However, it&rsquo;s not 100% production ready, and we will improve/change algorithm. So i will publish several posts, describing different parts of the algorithm. And then - write an article. (Maybe, i&rsquo;ll send it to the ICCV).</p><p><strong><a href=#35 title="2011-01-17 15:58:39">Stéphane Péchard</a>:</strong> Any plans on letting us know how it works?</p><p><strong><a href=#46 title="2011-01-22 17:23:14">Dewald</a>:</strong> Very nice. Looking forward to hearing more about LAZY. Nice name btw.</p><p><strong><a href=#101 title="2011-02-26 11:04:35">Shervin Emami</a>:</strong> Looks impressive! If you do post it online for free, then thanks! I can&rsquo;t wait to try it out :-)</p><p><strong><a href=#104 title="2011-02-28 14:50:22">Tony Marrero</a>:</strong> Great work guys ;) , will be looking forward to more of your posts.</p><p><strong><a href=#279 title="2011-04-21 06:29:25">yoshiboarder</a>:</strong> Wow! Very nice i wan&rsquo;t execute lazy algorithms. can you send me lazy demo source code?</p><p><strong><a href=#280 title="2011-04-21 09:55:41">EKhvedchenya</a>:</strong> No it&rsquo;s closed source yet. if one day it become open-source i&rsquo;ll let you know.</p><p><strong><a href=#305 title="2011-04-27 13:03:43">JaeSik</a>:</strong> It looks very impressive! Is this the result of LAZY on Iphone?</p><p><strong><a href=#306 title="2011-04-27 15:10:09">EKhvedchenya</a>:</strong> Descriptor extraction takes 2ms per frame (~100 keypoints on frame) on iPhone 3gs. However keypoint extraction takes a lot (0.5 seconds, now using SURF feature detector from OpenCV). So now i&rsquo;m working on lightweight and fast enough feature detector especially for LAZY descriptor.</p><p><strong><a href=#745 title="2011-11-28 23:04:03">Brett</a>:</strong> What&rsquo;s the status of this? Looks like the last post was a while ago. Is the algorithm published anywhere? Is there any way for other developers to try this out?</p><p><strong><a href=#746 title="2011-11-28 23:23:44">EKhvedchenya</a>:</strong> Hi Brett! The status is &ldquo;not enough time&rdquo;. To be honest i also want to finish the research of this descriptors, but my current work schedule consumes all my time and energy. It&rsquo;s challenging, interesting and i enjoy it. But from the other side i have no physical possibility to work on my own researches simultaneously. That&rsquo;s why development of LAZY descriptors is suspended for now. Thanks for the interest to it. I promised to myself to finish their research and publish an article about it at least.</p><p><strong><a href=#793 title="2011-12-29 09:17:17">Rehan</a>:</strong> Impressive, Can I get the code for research purpose? Thanks. Dr. Rehan</p><p><strong><a href=#794 title="2011-12-29 09:17:49">Rehan</a>:</strong> Impressive, Can I get the code for research purpose? Thanks. Dr. Rehan</p></div><div class=post__footer><span><a class=tag href=/tags/opencv/>opencv</a></span></div><div id=comment><h2>comments</h2><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//cvtalks.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
Eugene Khvedchenya
2022</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.9c062c557275acbaba71f0c7cd4024da3e3cc825d248bc4b2130811b0965330b.js integrity="sha256-nAYsVXJ1rLq6cfDHzUAk2j48yCXSSLxLITCBGwllMws=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-10687369-5","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>