<!doctype html><html dir=ltr lang=en data-theme class=html><head><title>Eugene Khvedchenya
|
Comparison of the OpenCV’s feature detection algorithms</title><meta charset=utf-8><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Eugene Khvedchenya"><meta name=description content="A personal blog about Computer Vision, AI, Kaggle, Open-Source and research. You will never read this in books."><link rel=stylesheet href=/scss/main.min.b2e0cb07595e3519ab1193bb421914e06c0e26b0cc561fef23b3c6131d4d2ffa.css integrity="sha256-suDLB1leNRmrEZO7QhkU4GwOJrDMVh/vI7PGEx1NL/o=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.31b0a1f317f55c529a460897848c97436bb138b19c399b37de70d463a8bf6ed5.css integrity="sha256-MbCh8xf1XFKaRgiXhIyXQ2uxOLGcOZs33nDUY6i/btU=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=canonical href=https://computer-vision-talks.com/post/2011-01-04-comparison-of-the-opencv-feature-detection-algorithms/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.560a26330d27ff44a44e83b53cd07a95d4230a65930d31c5c76a8d481e5b35bf.js integrity="sha256-VgomMw0n/0SkToO1PNB6ldQjCmWTDTHFx2qNSB5bNb8=" crossorigin=anonymous></script><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://computer-vision-talks.com/post/2011-01-04-comparison-of-the-opencv-feature-detection-algorithms/Average-feature-point-drift_thumb.png"><meta name=twitter:title content="Comparison of the OpenCV’s feature detection algorithms"><meta name=twitter:description content="Introduction
 “In computer vision and image processing the concept of feature detection refers to methods that aim at computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not. The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.”
 Wikipedia"><meta property="og:title" content="Comparison of the OpenCV’s feature detection algorithms"><meta property="og:description" content="Introduction
 “In computer vision and image processing the concept of feature detection refers to methods that aim at computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not. The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.”
 Wikipedia"><meta property="og:type" content="article"><meta property="og:url" content="https://computer-vision-talks.com/post/2011-01-04-comparison-of-the-opencv-feature-detection-algorithms/"><meta property="og:image" content="https://computer-vision-talks.com/post/2011-01-04-comparison-of-the-opencv-feature-detection-algorithms/Average-feature-point-drift_thumb.png"><meta property="article:section" content="post"><meta property="article:published_time" content="2011-01-04T00:00:00+00:00"><meta property="article:modified_time" content="2011-01-04T00:00:00+00:00"><meta property="og:site_name" content="Computer Vision Talks"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"Comparison of the OpenCV’s feature detection algorithms","headline":"Comparison of the OpenCV’s feature detection algorithms","alternativeHeadline":"","description":"
      
        Introduction\n “In computer vision and image processing the concept of feature detection refers to methods that aim at computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not. The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.”\n Wikipedia


      


    ","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/computer-vision-talks.com\/post\/2011-01-04-comparison-of-the-opencv-feature-detection-algorithms\/"},"author":{"@type":"Person","name":"Eugene Khvedchenya"},"creator":{"@type":"Person","name":"Eugene Khvedchenya"},"accountablePerson":{"@type":"Person","name":"Eugene Khvedchenya"},"copyrightHolder":{"@type":"Person","name":"Eugene Khvedchenya"},"copyrightYear":"2011","dateCreated":"2011-01-04T00:00:00.00Z","datePublished":"2011-01-04T00:00:00.00Z","dateModified":"2011-01-04T00:00:00.00Z","publisher":{"@type":"Organization","name":"Eugene Khvedchenya","url":"https://computer-vision-talks.com/","logo":{"@type":"ImageObject","url":"https:\/\/computer-vision-talks.com\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/computer-vision-talks.com\/post\/2011-01-04-comparison-of-the-opencv-feature-detection-algorithms\/","wordCount":"1155","genre":[],"keywords":["opencv"]}</script></head><body class="body theme--light"><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/images/4ECfkGJr_400x400.jpg alt="profile picture"><div class=sidebar__introduction-title><a href=/>Computer Vision Talks</a></div><div class=sidebar__introduction-description><p>A personal blog about Computer Vision, AI, Kaggle, Open-Source and research. You will never read this in books.</p></div></div><ul class=sidebar__list><li class=sidebar__list-item><a href=https://www.linkedin.com/in/cvtalks/ target=_blank rel=noopener aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://twitter.com/cvtalks target=_blank rel=noopener aria-label=Twitter title=Twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://github.com/BloodAxe target=_blank rel=noopener aria-label=GitHub title=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=mailto:ekhvedchenya@computer-vision-talks.com target=_blank rel=noopener aria-label=E-Mail title=E-Mail><i class="fas fa-envelope fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
Eugene Khvedchenya
2022</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.9c062c557275acbaba71f0c7cd4024da3e3cc825d248bc4b2130811b0965330b.js integrity="sha256-nAYsVXJ1rLq6cfDHzUAk2j48yCXSSLxLITCBGwllMws=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-10687369-5","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/ title>Home</a></li><li class=nav__list-item><a href=/post/ title>Posts</a></li><li class=nav__list-item><a href=/open-source title>Open-Source</a></li><li class=nav__list-item><a href=/challenges title>Wins</a></li><li class=nav__list-item><a href=/publications title>Publications</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>Comparison of the OpenCV’s Feature Detection Algorithms</h1><ul class=post__meta><li class=post__meta-item><em class="fas fa-calendar-day post__meta-icon"></em>
<span class=post__meta-text>Tue, Jan 4, 2011</span></li><li class=post__meta-item><em class="fas fa-stopwatch post__meta-icon"></em>
<span class=post__meta-text>6-minute read</span></li></ul><p><strong>Introduction</strong></p><blockquote><p>“In computer vision and image processing the concept of <strong>feature detection</strong> refers to methods that aim at computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not.
The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.”</p></blockquote><p>Wikipedia</p><p></p><p><strong>Update</strong>: Here is <a href=http://computer-vision-talks.com/2011/07/comparison-of-the-opencvs-feature-detection-algorithms-ii/>Updated Post</a> with added new detectors (ORB and SIFT).  </p><h2 id=overview>Overview</h2><p>OpenCV is free open-source library intended for use in image processing, computer vision and machine learning areas. It have a huge amount of different algorithms, but in this topic i will compare their existing feature detectors. At this moment OpenCV has stable 2.2 version and following types of descriptors: <strong>Fast</strong>, <strong>GoodFeaturesToTrack</strong>, <strong>Mser</strong>, <strong>Star</strong>, <strong>Sift</strong>, <strong>Surf</strong>. And few Adapters over detectors: GridAdapted, PyramidAdapted, DynamicAdapted. In this article noticed above detectors will be compared by speed, quality and repeatability in different lighting and scale.</p><h2 id=test-images>Test images</h2><p> <img src=mandril_thumb.jpg alt=mandril title=mandril><img src=barbara_thumb.jpg alt=barbara title=barbara> <img src=lena_thumb.jpg alt=lena title=lena><img src=peppers_thumb.jpg alt=peppers title=peppers></p><p>All images are 512x512 size. Processing was done on grayscale images.</p><h2 id=criteria>Criteria</h2><p><strong>Speed</strong> – the most important criteria for real-time image processing. To achieve smooth real time processing your code must process single frame in less than 30 milliseconds! It’s quite enough for four cores of Core i7 processor, but imagine your code will be running on mobile device, which have ten times lower computation power. We will test how fast our detectors are on the laptop CPU and on the mobile processor.</p><p>Test case: Detect feature points on the given image and measure spent time.</p><p><strong>Quality</strong> – detected features usually used in further tracking or matching (for SLAM, panorama stitching, object detection, etc..). So “feature consumer” expects that detected features are good enough for it. Detectors will be tested for Optical Flow KLT tracking and two-frames matching using SIFT and SURF descriptors. For tracking test I will apply some affine transformation for original image and detected on the previous step keypoints. With transformed image and transformed keypoints we can estimate quality of tracking.</p><p>Here is screenshots demonstrating transformed images and tracks of the features.</p><p><img src=Mandril-GFTT-Tracking_thumb.jpg alt=Mandril-GFTT-Tracking title=Mandril-GFTT-Tracking><img src=Barbara-SURF-Tracking_thumb.jpg alt=Barbara-SURF-Tracking title=Barbara-SURF-Tracking><img src=Lena-FAST-Tracking_thumb.jpg alt=Lena-FAST-Tracking title=Lena-FAST-Tracking><img src=Peppers-FAST-Tracking_thumb.jpg alt=Peppers-FAST-Tracking title=Peppers-FAST-Tracking></p><p>**Lighting and scale invariant – **feature detectors are expected to detect the same features either on large or small images of one object. That’s true also for lighting – slight brightness and contrast fluctuations shouldn’t affect on the feature detector significantly. Almost all modern cameras have automatic gain control which automatically adjust exposure trying to avoid over- or underexposed areas in image. Robust detection is critical for further processing.</p><p>Here is explanation screenshot:</p><p><img src=Barbara-STAR-Matching_thumb.jpg alt=Barbara-STAR-Matching title=Barbara-STAR-Matching><img src=Peppers-GFTT-Pyramid-Matching_thumb.jpg alt="Peppers-GFTT Pyramid-Matching" title="Peppers-GFTT Pyramid-Matching"></p><p>As you can see, the second picture for each pair are slightly brighter. This test simulates automatic camera gain option for many modern web-cams.</p><h2 id=estimation-marks>Estimation marks</h2><p><strong>Speed per frame</strong> – absolute total time in milliseconds spent to the feature detection of the single frame.</p><p><strong>Speed per keypoint</strong> – detection time for single keypoint. Evaluated as total time divided to number of detected keypoints. Helps us to estimate how cheap the detection actually is.</p><p><strong>Percent of tracked features</strong> – percent of successfully tracked features from original to transformed image. In ideal situation, value of this mark should be near 100%.</p><p><strong>Average tracking error</strong> – this is the average distance between position of tracked feature and their calculated position on transformed frame. This mark indicates accuracy of the feature detection. Large values indicates large number of false positive tracking or “drift” of feature point among frames.</p><p><strong>Features count deviation</strong> – difference between number of keypoints on reference frame and number of detected keypoints on transformed frame divided by number of keypoints on reference frame. Helps estimate how slight exposure changes affects feature detection.</p><p><strong>Average detection error</strong> – average distance between nearest keypoints on original and transformed frame.</p><h2 id=testing-hardware-configuration>Testing hardware configuration</h2><p>We will test all set of feature detectors on two configurations – laptop and mobile on the same data sets to compare performance and ensure that detection results are identical for both platforms.</p><p> 
<strong>MacBook Pro</strong>
<strong>iPhone 3GS</strong></p><p>CPU Model
Core 2 Duo T7400
ARM Cortex A8</p><p>CPU Clock
2160 MHz
600 MHz</p><p>CPU Cores number
2
1</p><h2 id=results>Results</h2><p>We start our analysis from number of detected feature points. And on the chart below we see that FAST detector finds thousands of keypoints, while other detectors finds only hundreds. Unlike the other detectors, keypoints  detected by FAST can contain too many “noise” features – which are not appropriate for further tracking.<img src=Number-of-detected-features_thumb.png alt="Number of detected features" title="Number of detected features"></p><p>Speed of feature detection will be examined using two criteria&rsquo;s – by total amount of time spent for the detection of keypoint on the whole frame and a time per single keypoint which is simply total time divided by number of detected keypoints. As expected, FAST detector provides best detection time per feature.</p><p><img src=Detection-cost_thumb.png alt="Detection cost" title="Detection cost"></p><p>Finally the last performance tests – how fast are feature detectors on mobile devices? Here is four charts (for four different images) that will answer this question:</p><p><img src=Feature-detection-comparison-peppers_thumb.png alt="Feature detection comparison - peppers" title="Feature detection comparison - peppers"></p><p><img src=Speed-comparision-barbara_thumb.png alt="Speed comparision - barbara" title="Speed comparision - barbara"></p><p><img src=Speed-comparison-lena_thumb.png alt="Speed comparison - lena" title="Speed comparison - lena"></p><p><img src=Speed-comparison-mandrill_thumb.png alt="Speed comparison - mandrill" title="Speed comparison - mandrill"></p><p>As we can see from charts above - on the mobile platform all feature detectors works ~3-5 times slower than on desktop. So without optimization for architecture of concrete processor real-time performance can’t be achieved (see conclusion).</p><p>Now to quality estimation. Following chart demonstrates average tracking error (in pixels) between reference frame and slightly transformed. Pyramid KLT tracker was used (cvCalcOpticalFlowPyrLK). STAR (Also known as CenSurE) detector found keypoints that were tracked best than other detectors.</p><p><img src=Average-tracking-error_thumb.png alt="Average tracking error" title="Average tracking error"></p><p>Chart below  gives us information about how many of features were tracked successfully. All detectors provide very good feature points, except for MSER detectors, which score is less that other’s detectors for all test images.</p><p><img src=Percent-of-tracked-feature_thumb.png alt="Percent of tracked feature" title="Percent of tracked feature"></p><p>A last chart shows how feature detectors co-variant with brightness changes. STAR and MSER detectors are very sensible to brightness variation, while the other detectors are not.</p><p><img src=Average-feature-point-drift_thumb.png alt="Average feature point drift" title="Average feature point drift"></p><p> </p><h2 id=conclusion>Conclusion</h2><p>The choice of the feature detector very much depends on a problem. For example, if you doing monocular SLAM, FAST detector is weapon of choice, because it’s fast and detects a lot of features. For image stitching, pattern recognition and other feature-descriptor related tasks, scale- and rotation-invariant detectors are preferable.</p><p>For desktop applications almost all feature detectors guarantee real-time (25 fps+) performance. But for mobile device the fastest detector (FAST) works only with ~10fps. Why such huge difference? First of all, laptop CPU is much more powerful. But also, cache size and processor architecture does matter. OpenCV supports SSE/SSE2/SSE3 intrinsic instructions which can deal 10x speedup because of instruction vectorization. Unfortunately, OpenCV supports intrinsic instructions only for x86 architecture. There are ARM NEON SIMD engine, and theoretically with their help we can significantly improve performance of feature detection on mobile platforms. But this require a lot of experience and low-level knowledge of CPU architecture. I will reveal benefits of the NEON intrinsic in my further posts.</p><p>This article highlights only few performance and quality aspects of the feature detection problem. I highly recommend you to read the article “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.4126&rep=rep1&type=pdf">Local Invariant Feature Detectors: A Survey</a>” the get much more knowledge of how different feature detectors work.</p></div><div class=post__footer><span><a class=tag href=/tags/opencv/>opencv</a></span></div><div id=comment><h2>comments</h2><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//cvtalks.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
Eugene Khvedchenya
2022</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.9c062c557275acbaba71f0c7cd4024da3e3cc825d248bc4b2130811b0965330b.js integrity="sha256-nAYsVXJ1rLq6cfDHzUAk2j48yCXSSLxLITCBGwllMws=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-10687369-5","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>