<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ghostwriter example</title>
    <link>https://example.com/tags/tutorials/index.xml</link>
    <description>Recent content on Ghostwriter example</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>My Name</copyright>
    <atom:link href="https://example.com/tags/tutorials/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Introducing CloudCV bootstrap</title>
      <link>https://example.com/introducing-cloudcv-bootstrap/</link>
      <pubDate>Tue, 14 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/introducing-cloudcv-bootstrap/</guid>
      <description>

&lt;p&gt;Here&amp;rsquo;s an open-source ready to use bootstrap project written in Node.js that lets
you to quickly build a REST service to host your image processing and computer vision code
in a cloud environment.
Please welcome: &lt;a href=&#34;https://github.com/CloudCV/cloudcv-bootstrap&#34;&gt;cloudcv-bootstrap&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;cloudcv-bootstrap.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I made this project aside of CloudCV to keep it simple but functionaly. It is self-contained
Node.js project that helps you to get quick results on building and deploying your first
server-based image processing service.&lt;/p&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Ready to use. No need to download extra dependencies. Just run &lt;code&gt;npm install&lt;/code&gt; and that&amp;rsquo;s all.&lt;/li&gt;
&lt;li&gt;Built-in REST-API support. As a bonus, a Swagger 2.0 specification file comes too. You can use it as a template to build client SDKs.&lt;/li&gt;
&lt;li&gt;Shipped with OpenCV 3.0.0&lt;/li&gt;
&lt;li&gt;Interopability between C++ and Node.js code&lt;/li&gt;
&lt;li&gt;Covered with unit tests&lt;/li&gt;
&lt;li&gt;Logging support&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With cloudcv-bootstrap you can quickly wrap your C++ code into web-service using simple and
clear syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;app.post(&#39;/api/v1/image/analyze/dominantColors/&#39;, function (req, res) {
    cv.analyzeImage(req.files.image.buffer, function(error, result) {
        res.setHeader(&amp;quot;Content-Type&amp;quot;, &amp;quot;application/json&amp;quot;);
        res.write(JSON.stringify(MapAnalyzeResult(result)));
        res.end();
    });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Error handling and logging here omited for the sake of simplicity, but this is full-functional snippet.
It accepts uploaded image using POST request and transfers image data to C++ backend.
&lt;a href=&#34;https://github.com/CloudCV/cloudcv-bootstrap&#34;&gt;cloudcv-bootstrap&lt;/a&gt; fully follows Node.js programming paradigm and schedule C++ code on libuv thread pool and leave main thread free for requests processing.&lt;/p&gt;

&lt;h2 id=&#34;quick-start&#34;&gt;Quick start&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/CloudCV/cloudcv-bootstrap.git
npm install
npm start &amp;amp;
curl localhost:3000/api/v1/image/analyze/dominantColors?image=https%3A%2F%2Fraw.githubusercontent.com%2FCloudCV%2Fcloudcv-bootstrap%2Fmaster%2Ftest%2Fdata%2Fopencv-logo.jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Produces:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;{
    &amp;quot;aspect&amp;quot;:
    {
        &amp;quot;width&amp;quot;:599,
        &amp;quot;height&amp;quot;:555
    },
    &amp;quot;size&amp;quot;:
    {
        &amp;quot;width&amp;quot;:0,
        &amp;quot;height&amp;quot;:0
    },
    &amp;quot;dominantColors&amp;quot;:
    [
        {&amp;quot;color&amp;quot;:[252,252,252],&amp;quot;totalPixels&amp;quot;:201655,&amp;quot;interclassVariance&amp;quot;:7.83907795204613e-37,&amp;quot;error&amp;quot;:0},
        {&amp;quot;color&amp;quot;:[252,0,0],&amp;quot;totalPixels&amp;quot;:43612,&amp;quot;interclassVariance&amp;quot;:7.83907795204613e-37,&amp;quot;error&amp;quot;:0},
        {&amp;quot;color&amp;quot;:[0,0,252],&amp;quot;totalPixels&amp;quot;:43591,&amp;quot;interclassVariance&amp;quot;:7.83907795204613e-37,&amp;quot;error&amp;quot;:0},
        {&amp;quot;color&amp;quot;:[0,252,0],&amp;quot;totalPixels&amp;quot;:43587,&amp;quot;interclassVariance&amp;quot;:7.83907795204613e-37,&amp;quot;error&amp;quot;:0}
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations, you&amp;rsquo;ve just computed dominant colors of the OpenCV logo image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CloudCV/cloudcv-bootstrap/master/test/data/opencv-logo.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;extending-with-your-code&#34;&gt;Extending with your code&lt;/h2&gt;

&lt;p&gt;This module uses node-gyp build system. It produces Node C++ addon and require you to do minimal changes into this module:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Have C++ code you want to host&lt;/li&gt;
&lt;li&gt;Write module binding&lt;/li&gt;
&lt;li&gt;Register it&lt;/li&gt;
&lt;li&gt;Write unit tests&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s go step by step using camera calibration as example. For quick results we won&amp;rsquo;t reinvent the wheel and use code from OpenCV samples. I will just refactor it slightly. Here&amp;rsquo;s our public interface of calibration algorithm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;enum PatternType {
    CHESSBOARD = 0,
    CIRCLES_GRID = 1,
    ACIRCLES_GRID = 2
};

class CameraCalibrationAlgorithm
{
public:
    typedef std::vector&amp;lt;cv::Point3f&amp;gt;               VectorOf3DPoints;
    typedef std::vector&amp;lt;cv::Point2f&amp;gt;               VectorOf2DPoints;
    typedef std::vector&amp;lt;std::vector&amp;lt;cv::Point3f&amp;gt; &amp;gt; VectorOfVectorOf3DPoints;
    typedef std::vector&amp;lt;std::vector&amp;lt;cv::Point2f&amp;gt; &amp;gt; VectorOfVectorOf2DPoints;
    typedef std::vector&amp;lt;cv::Mat&amp;gt;                   VectorOfMat;

    CameraCalibrationAlgorithm(cv::Size patternSize, PatternType type);

    bool detectCorners(const cv::Mat&amp;amp; frame, VectorOf2DPoints&amp;amp; corners2d) const;

    bool calibrateCamera(
        const VectorOfVectorOf2DPoints&amp;amp; gridCorners,
        const cv::Size imageSize,
        cv::Mat&amp;amp; cameraMatrix,
        cv::Mat&amp;amp; distCoeffs
    ) const;

protected:

    // .. plenty of helper methods

private:
    cv::Size                 m_patternSize;
    PatternType              m_pattern;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We want to wrap it into V8 code. First, we need to register corresponding function that we will expose to JS:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void RegisterModule(Handle&amp;lt;Object&amp;gt; target)
{
    // ...

    NODE_SET_METHOD(target, &amp;quot;calibrationPatternDetect&amp;quot;, calibrationPatternDetect);
    NODE_SET_METHOD(target, &amp;quot;calibrateCamera&amp;quot;,          calibrateCamera);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Implementation of &lt;code&gt;calibrationPatternDetect&lt;/code&gt; and &lt;code&gt;calibrateCamera&lt;/code&gt; needs to parse input arguments, schedule a task to thread pool and invoke a user-passed callback on completition.
Marshalling between C++ and V8 is tricky.
Fortunately, NaN module does a great help on data marshalling.
To simplity developer&amp;rsquo;s life even more &lt;a href=&#34;https://github.com/CloudCV/cloudcv-bootstrap&#34;&gt;cloudcv-bootstrap&lt;/a&gt; offers complex data marshalling and argument checking:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;NAN_METHOD(calibrationPatternDetect)
{
    TRACE_FUNCTION;
    NanEscapableScope();

    Local&amp;lt;Object&amp;gt;   imageBuffer;
    Local&amp;lt;Function&amp;gt; callback;
    cv::Size        patternSize;
    PatternType     pattern;
    std::string     error;

    if (NanCheck(args)
        .Error(&amp;amp;error)
        .ArgumentsCount(4)
        .Argument(0).IsBuffer().Bind(imageBuffer)
        .Argument(1).Bind(patternSize)
        .Argument(2).StringEnum&amp;lt;PatternType&amp;gt;({ 
            { &amp;quot;CHESSBOARD&amp;quot;,     PatternType::CHESSBOARD }, 
            { &amp;quot;CIRCLES_GRID&amp;quot;,   PatternType::CIRCLES_GRID }, 
            { &amp;quot;ACIRCLES_GRID&amp;quot;,  PatternType::ACIRCLES_GRID } }).Bind(pattern)
        .Argument(3).IsFunction().Bind(callback))
    {
        LOG_TRACE_MESSAGE(&amp;quot;Parsed function arguments&amp;quot;);
        NanCallback *nanCallback = new NanCallback(callback);
        NanAsyncQueueWorker(new DetectPatternTask(
            CreateImageSource(imageBuffer), 
            patternSize, 
            pattern, 
            nanCallback));
    }
    else if (!error.empty())
    {
        LOG_TRACE_MESSAGE(error);
        NanThrowTypeError(error.c_str());
    }

    NanReturnUndefined();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may read about NanCheck in separate post: &lt;a href=&#34;http://computer-vision-talks.com/articles/how-to-convert-args-from-js-to-cpp&#34;&gt;NanCheck&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;data-marshalling&#34;&gt;Data marshalling&lt;/h2&gt;

&lt;p&gt;Natively, marshaller supports:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C++ plain types&lt;/strong&gt;:
 - char, unsigned char
 - short, unsighed short
 - int, unsigned int
 - long, unsigned long
 - float, double
 - T[N]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STL types&lt;/strong&gt;:
 - std::array&lt;T,N&gt;
 - std::pair&lt;A,B&gt;
 - std::vector&lt;T&gt;
 - std::map&lt;K,V&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OpenCV types&lt;/strong&gt;:
 - cv::Point2i, cv::Point2f, cv::Point2d
 - cv::Size&lt;em&gt;&lt;int&gt;, cv::Size&lt;/em&gt;&lt;float&gt;, cv::Size_&lt;double&gt;
 - cv::Mat&lt;/p&gt;

&lt;p&gt;Data marshalling of user-defined structures implemented in similar to boost::serialization fashion:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;struct CalibrationResult
{
    cv::Mat  m_distCoeffs;
    cv::Mat  m_cameraMatrix;
    bool     m_calibrationSuccess;

    template &amp;lt;typename Archive&amp;gt;
    void serialize(Archive&amp;amp; ar)
    {
        ar &amp;amp; serialization::make_nvp(&amp;quot;calibrationSuccess&amp;quot;, m_calibrationSuccess);

        if (Archive::is_loading::value || m_calibrationSuccess)
        {
            ar &amp;amp; serialization::make_nvp(&amp;quot;cameraMatrix&amp;quot;,m_cameraMatrix);
            ar &amp;amp; serialization::make_nvp(&amp;quot;distCoeffs&amp;quot;,  m_distCoeffs);
        }
    }
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;User-defined types will be marshalled to regular V8 object containing fields serialized within &lt;code&gt;serialize()&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;To marshal C++ object to V8 object:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;CalibrationResult cpp_result = ...;
auto v8_result = marshal(cpp_result);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To marshal from V8 object to C++ object:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;v8::Local&amp;lt;v8::Value&amp;gt; v8_result = ...;
auto cpp_result = marshal&amp;lt;CalibrationResult&amp;gt;(v8_result);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;roadmap&#34;&gt;Roadmap&lt;/h2&gt;

&lt;p&gt;This is very beta version of cloudcv-bootstrap and it&amp;rsquo;s codebase about to change.
Please keep in mind that and feel free to ask for help in &lt;a href=&#34;https://twitter.com/cvtalks&#34;&gt;twitter&lt;/a&gt; or on &lt;a href=&#34;https://github.com/CloudCV/cloudcv-bootstrap/issues&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;According to plan:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Add Dockerfile to run this code in a container environment&lt;/li&gt;
&lt;li&gt;Write more documentation on data marshalling&lt;/li&gt;
&lt;li&gt;Implement easier REST API mapping and arguments checking&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>How to debug node.js addons in Visual Studio</title>
      <link>https://example.com/how-to-debug-nodejs-addons-in-visual-studio/</link>
      <pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/how-to-debug-nodejs-addons-in-visual-studio/</guid>
      <description>

&lt;p&gt;While working on &lt;a href=&#34;https://cloudcv.io&#34;&gt;CloudCV&lt;/a&gt; I encountered problems in node.js addon written in native code. For CloudCV I use node.js with C++ Addon to separate high-performance algorithms (C++) from high-level networking API which node provides.&lt;/p&gt;

&lt;p&gt;In this tutorial I&amp;rsquo;m going to reveal best practices on debugging C++ Addons for Node.js (0.12) using Visual Studio 2013.&lt;/p&gt;

&lt;div class=&#34;embed-responsive embed-responsive-4by3&#34;&gt;
  &lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;https://www.youtube.com/embed/eqhv42jVN6s&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Continue reading if you want to read in details why this works.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This article is valid for Node.js version 0.12. It should also works fine for further releases, however few things may change. I will try to keep this post up to date. Please, feel free to drop a comment for this article or write me on &lt;a href=&#34;https://twitter.com/cvtalks&#34;&gt;@cvtalks&lt;/a&gt; if you have troubles following this tutorial.&lt;/p&gt;

&lt;h2 id=&#34;before-we-start&#34;&gt;Before we start&lt;/h2&gt;

&lt;p&gt;You will need a Visual Studio 2013 (&lt;a href=&#34;http://go.microsoft.com/?linkid=9832256&#34;&gt;Express edition&lt;/a&gt; should be enough). Also, please come and grab &lt;a href=&#34;http://nodejs.org/dist/v0.12.0/node-v0.12.0.tar.gz&#34;&gt;Node.js source&lt;/a&gt;] code. We will use it later to build Debug configuration of Node.js. I assume you
already have production (Release) version of Node and NPM as we will need &lt;a href=&#34;https://github.com/TooTallNate/node-gyp&#34;&gt;node-gyp&lt;/a&gt; to generate and build C++ Addon project. So please ensure you&amp;rsquo;ve installed them before going to next step:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install -g node-gyp
npm install -g nan
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;build-debug-configuration-of-node-js&#34;&gt;Build Debug configuration of node.js&lt;/h2&gt;

&lt;p&gt;This is very straighforward step.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download &lt;a href=&#34;http://nodejs.org/dist/v0.12.0/node-v0.12.0.tar.gz&#34;&gt;Node.js source&lt;/a&gt;] code.&lt;/li&gt;
&lt;li&gt;Extract it somewhere to your filesystem. For demonstration, I assume it will be in &lt;code&gt;c:\Develop\node-v0.12.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Navigate to &lt;code&gt;c:\Develop\node-v0.12.0&lt;/code&gt; and run &lt;code&gt;vcbuild.bat debug nosign x64&lt;/code&gt;. This batch script will build Debug configuration of node.js for 64-bit architecture (if you&amp;rsquo;re on 32-bit platform omit this flag). A &lt;code&gt;nosign&lt;/code&gt; flag tells to skip executable signing which is ok since we&amp;rsquo;re not going to distribute it, and finally &lt;code&gt;debug&lt;/code&gt; forces compiler to generate debug symbols for node executable.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;build-node-debug-step-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If everything goes fine, you should see the following output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;c:\Develop\node-v0.12.0&amp;gt;vcbuild.bat debug nosign x64
ctrpp not found in WinSDK path--using pre-gen files from tools/msvs/genfiles.
creating  icu_config.gypi

...

creating  config.gypi
creating  config.mk
Project files generated.

...

debugger-agent.vcxproj -&amp;gt; c:\Develop\node-v0.12.0\Debug\lib\debugger-agent.lib
v8_nosnapshot.vcxproj -&amp;gt; ..\..\..\..\build\Debug\lib\v8_nosnapshot.lib
openssl-cli.vcxproj -&amp;gt; c:\Develop\node-v0.12.0\Debug\\openssl-cli.exe
mksnapshot.vcxproj -&amp;gt; ..\..\..\..\build\Debug\\mksnapshot.exe
v8_snapshot.vcxproj -&amp;gt; ..\..\..\..\build\Debug\lib\v8_snapshot.lib
node.vcxproj -&amp;gt; c:\Develop\node-v0.12.0\Debug\\node.exe
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;build-node-debug-step-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you have problems on this step, please refer to &lt;a href=&#34;https://github.com/joyent/node/wiki/installation#building-on-windows&#34;&gt;building node.js&lt;/a&gt; official documentation.&lt;/p&gt;

&lt;h2 id=&#34;building-debug-configuration-of-c-addon&#34;&gt;Building Debug configuration of C++ Addon&lt;/h2&gt;

&lt;p&gt;A C++ Addon for nodejs is nothing but ordinary DLL. Therefore Visual Studio can load it and let you do step-by-step debugging inside node.js app.
Let&amp;rsquo;s start with simple scenario. I will use &lt;a href=&#34;https://github.com/rvagg/nan/tree/master/examples/async_pi_estimate&#34;&gt;Pi estimation&lt;/a&gt; example from &lt;a href=&#34;https://github.com/rvagg/nan&#34;&gt;NaN&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;Your C++ Addon code can look as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;/*********************************************************************
 * NAN - Native Abstractions for Node.js
 *
 * Copyright (c) 2015 NAN contributors
 *
 * MIT License &amp;lt;https://github.com/rvagg/nan/blob/master/LICENSE.md&amp;gt;
 ********************************************************************/

var addon = require(&#39;./build/Release/addon&#39;);
var calculations = process.argv[2] || 100000000;

function printResult(type, pi, ms) {
  console.log(type, &#39;method:&#39;)
  console.log(&#39;\tπ ≈ &#39; + pi
        + &#39; (&#39; + Math.abs(pi - Math.PI) + &#39; away from actual)&#39;)
    console.log(&#39;\tTook &#39; + ms + &#39;ms&#39;);
    console.log()
}

function runSync () {
  var start = Date.now();
  // Estimate() will execute in the current thread,
  // the next line won&#39;t return until it is finished
    var result = addon.calculateSync(calculations);
  printResult(&#39;Sync&#39;, result, Date.now() - start)
}

function runAsync () {
  // how many batches should we split the work in to?
    var batches = process.argv[3] || 16;
    var ended = 0;
    var total = 0;
    var start = Date.now();

    function done (err, result) {
        total += result;

    // have all the batches finished executing?
        if (++ended == batches) {
            printResult(&#39;Async&#39;, total / batches, Date.now() - start)
        }
    }

  // for each batch of work, request an async Estimate() for
  // a portion of the total number of calculations
    for (var i = 0; i &amp;lt; batches; i++) {
        addon.calculateAsync(calculations / batches, done);
    }
}

runSync()
runAsync()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And you also should have gyp file which tells node-gyp how to build your addon:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-gyp&#34;&gt;{
  &amp;quot;targets&amp;quot;: [
    {
      &amp;quot;target_name&amp;quot;: &amp;quot;addon&amp;quot;,
      &amp;quot;sources&amp;quot;: [
        &amp;quot;addon.cc&amp;quot;,
        &amp;quot;pi_est.cc&amp;quot;,
        &amp;quot;sync.cc&amp;quot;,
        &amp;quot;async.cc&amp;quot;
      ],
      &amp;quot;include_dirs&amp;quot;: [&amp;quot;&amp;lt;!(node -e \&amp;quot;require(&#39;nan&#39;)\&amp;quot;)&amp;quot;]
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming you&amp;rsquo;re in root C++ addon directory, you can now generate a solution for Visual Studio to build your addon:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;node-gyp configure rebuild --nodedir=&amp;quot;c:\Develop\node-v0.12.0&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;build-addon-step-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;What is really important here, is &lt;code&gt;--nodedir=&amp;quot;c:\Develop\node-v0.12.0&amp;quot;&lt;/code&gt; flag, which indicates to link against node in specified
folder rather than system wide available.&lt;/p&gt;

&lt;p class=&#34;bg-info lead&#34;&gt;
This is very important to match Debug node with Debug C++ Addon, otherwise you will have 
linker issues caused by inconsistent CRT&#39;s.
&lt;/p&gt;

&lt;p&gt;After node-gyp finishes, a .node file (this is .dll, don&amp;rsquo;t be misleaded by .node extension) will be generated in /build/Debug/ folder.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;build-addon-step-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now it&amp;rsquo;s time to write a small node.js script that utilizes our addon:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nodejs&#34;&gt;var addon = require(&#39;./build/Release/addon&#39;);

function printResult(type, pi, ms) {
  console.log(type, &#39;method:&#39;)
  console.log(&#39;\tπ ≈ &#39; + pi
        + &#39; (&#39; + Math.abs(pi - Math.PI) + &#39; away from actual)&#39;)
    console.log(&#39;\tTook &#39; + ms + &#39;ms&#39;);
    console.log()
}

function runAsync () {

    var start = Date.now();

    function done (err, result) {
        console.log(&#39;\tπ ≈ &#39; + pi + &#39; (&#39; + Math.abs(pi - Math.PI) + &#39; away from actual)&#39;)
        console.log(&#39;\tTook &#39; + ms + &#39;ms&#39;);
        console.log()

        printResult(&#39;Async&#39;, total / batches, Date.now() - start)
    }

    addon.calculateAsync(1024, done);
}

runAsync()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may wonder now, how to debug it. Patience, we&amp;rsquo;re almost there.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;generated-project.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Navigate to &lt;code&gt;build/&lt;/code&gt; directory of your C++ Addon and
open solution file.&lt;/li&gt;
&lt;li&gt;Ensure that you have active Debug configuration.&lt;/li&gt;
&lt;li&gt;Navigate to project properties and open Debugging tab there.&lt;/li&gt;
&lt;li&gt;Modify command name to &amp;ldquo;c:\Develop\node-v0.12.0\Node.exe&amp;rdquo; (Change this path if you extracted node somewhere else)&lt;/li&gt;
&lt;li&gt;Set a command argument to full name of your node.js script.&lt;/li&gt;
&lt;li&gt;Change working directory to a place where your script is. This step important when you have complex nodej.js application with
dependencies.&lt;/li&gt;
&lt;li&gt;Set breakpoint somewhere in your C++ Addon code.&lt;/li&gt;
&lt;li&gt;Now hit &amp;lsquo;Debug&amp;rsquo; (F5) and enjoy!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;vsproject-settings.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;how-it-works&#34;&gt;How it works&lt;/h2&gt;

&lt;p&gt;When building Debug builds, Visual Studio trades speed for easy debugging. This means not only slower code, but it also preserves debug
symbols (a table of function addresses/names/file locations). When you start a debugger on process, VS attachs to it and tries to load
symbols for this binary and all dynamically loaded libraries it uses.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;vsproject-debugging.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As a consequence, debug symbols for C++ addon is being loaded which allows you to see program execution location in your IDE,
do step-by-step debugging and change/rebuild/debug as usual.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I hope you find this post useful. From my experience debugging node.js &amp;lt;-&amp;gt; C++ interop can be nasty. Personally I follow this
scenario for debugging CloudCV C++ backend. This saves a lot of time and nerves. Unleash your node.js with C++ and happy debugging!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tile-based image processing</title>
      <link>https://example.com/tile-based-image-processing/</link>
      <pubDate>Thu, 04 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/tile-based-image-processing/</guid>
      <description>

&lt;p&gt;How would you design an algorithm to process 40Mpx image? 100Mpx? What about gigapixel-sized panorams? Obviously, it should differs from those that are intended for 640x480 images. Here I want to present you implementation of the very simple but powerful approach called &amp;ldquo;Tile-based image processing&amp;rdquo;. I will show you how to make this using OpenCV.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;tiles.png&#34; alt=&#34;Tile based image processing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s define a few restrictions in order to simplify our implementation. In this tutorial I will consider a &amp;lsquo;pass-through&amp;rsquo; pipeline - when we apply some function to input image and give an output image of the same size as an output.&lt;/p&gt;

&lt;p&gt;It is possible to extend this approach to work with many input images, but for the sake of simplicity I&amp;rsquo;ll omit this for now.&lt;/p&gt;

&lt;p&gt;Consider a following algorithm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Take a source image for RGB color space.&lt;/li&gt;
&lt;li&gt;Convert in to grayscale color space (unsigned byte).&lt;/li&gt;
&lt;li&gt;Compute Sobel derivatives (signed short).&lt;/li&gt;
&lt;li&gt;Take a Dx, Dy for each pixel and compute it&amp;rsquo;s magnitude and orientation.&lt;/li&gt;
&lt;li&gt;Leave only those, which magnitude is larger than threshold.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using OpenCV it could look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;cv::Mat source = cv::imread(&amp;quot;input.jpg&amp;quot;);
cv::Mat grayscale, dx, dy;
cv::cvtColor(source, grayscale);
cv::Sobel(grayscale, dx, 1, 0);
cv::Sobel(grayscale, dy, 0, 1);
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;problems-with-straighforward-implementation&#34;&gt;Problems with straighforward implementation&lt;/h2&gt;

&lt;p&gt;This routine require &lt;code&gt;N + 2 * N * sizeof(signed short)&lt;/code&gt; bytes of additional memory for straightforward implementation, where N is number of pixels in source image. Large number of intermediate buffers can cause memory issues for memory restricted devices (mobile phones, embedded systems).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;On iOS, in particular, your app might get terminated by iOS watchdog for high peak RAM usage, despite the fact you use this memory only for a temp buffers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Second issue with large amount of buffers is cache-misses. Large buffers are likely to sit near each other, therefore cache performance will be low and algorithm performance will suffer.&lt;/p&gt;

&lt;p&gt;To adress those two issues, I suggest to divide input image into &amp;ldquo;Tiles&amp;rdquo; - regions of the original image of equal size, let&amp;rsquo;s say 64x64. The processing function remains the same, but we reuse all temporary buffers and process only 64x64 pixels at one time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;algorithm.png&#34; alt=&#34;Tile based image processing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say we&amp;rsquo;re processing &lt;code&gt;1280x720&lt;/code&gt; frame, using regular approach, the total amount of
additional memory is &lt;strong&gt;4.6 megabytes&lt;/strong&gt; (&lt;code&gt;4608000&lt;/code&gt; bytes). With tile-based approach, we need only &lt;strong&gt;20 kilobytes&lt;/strong&gt; (&lt;code&gt;20480&lt;/code&gt; bytes). 20K are likely to fit entirely in L2 cache and therefore give a significant performance boost.&lt;/p&gt;

&lt;h2 id=&#34;tile-based-implementation&#34;&gt;Tile-based implementation&lt;/h2&gt;

&lt;p&gt;To implement tile-based implementation, we iterate over the image, copy tiles from source image to our local source tile, process it and write to corresponding area in the
destination image.&lt;/p&gt;

&lt;p&gt;A pseudo-code for this routine is follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;int TileSize, typename Algorithm&amp;gt;
void process(const cv::Mat&amp;amp; sourceImage, cv::Mat&amp;amp; resultImage, Algorithm algorithm) const
{
    assert(!resultImage.empty());
    assert(sourceImage.rows == resultImage.rows);
    assert(sourceImage.cols == resultImage.cols);

    const int rows = (sourceImage.rows / TileSize) + (sourceImage.rows % TileSize ? 1 : 0);
    const int cols = (sourceImage.cols / TileSize) + (sourceImage.cols % TileSize ? 1 : 0);

    cv::Mat tileInput, tileOutput;

    for (int rowTile = 0; rowTile &amp;lt; rows; rowTile++)
    {
        for (int colTile = 0; colTile &amp;lt; cols; colTile++)
        {
            copyTileFromSource(sourceImage, tileInput, rowTile, colTile);
            algorithm(tileInput, tileOutput);
            copyTileToResultImage(tileOutput, resultImage, rowTile, colTile);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I hope it&amp;rsquo;s clear to understand what is happening in code above. The &lt;code&gt;Algorithm&lt;/code&gt; here represents some algorithm that we want to run on our tiles. There are two functions &lt;code&gt;copyTileFromSource&lt;/code&gt; and &lt;code&gt;copyTileToResultImage&lt;/code&gt; that will be covered a bit later.&lt;/p&gt;

&lt;h2 id=&#34;dealing-with-out-of-tile-reads&#34;&gt;Dealing with out-of-tile reads&lt;/h2&gt;

&lt;p&gt;You may ask yourself - what should we do with border pixels? Sobel operator use neighbor pixels around each pixel. When we construct a tile shouldn&amp;rsquo;t we take this into account? Sure we are. So that&amp;rsquo;s why there is a padding parameter that controls amount of additional pixels that are added to top, left, bottom and right of the tile in order to make functions that require additional pixels work correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;tile_with_paddings.png&#34; alt=&#34;Tile with paddings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Padding makes tile overlap each other, but we pay this price for good cache locality.&lt;/p&gt;

&lt;p&gt;I will use a slightly modified version of code from above:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;struct TiledAlgorithm
{
    TiledAlgorithm(int tileSize, int padding, int borderType)
        : mTileSize(tileSize)
        , mPadding(padding)
        , mBorderType(borderType)
    {
    }

    void process(const cv::Mat&amp;amp; sourceImage, cv::Mat&amp;amp; resultImage) const
    {
        assert(!resultImage.empty());
        assert(sourceImage.rows == resultImage.rows);
        assert(sourceImage.cols == resultImage.cols);

        int rows = (sourceImage.rows / mTileSize) + (sourceImage.rows % mTileSize ? 1 : 0);
        int cols = (sourceImage.cols / mTileSize) + (sourceImage.cols % mTileSize ? 1 : 0);

        cv::Mat tileInput, tileOutput;

        for (int rowTile = 0; rowTile &amp;lt; rows; rowTile++)
        {
            for (int colTile = 0; colTile &amp;lt; cols; colTile++)
            {
                cv::Rect srcTile(colTile * mTileSize - mPadding, 
                                 rowTile * mTileSize - mPadding, 
                                 mTileSize + 2 * mPadding, 
                                 mTileSize + 2 * mPadding);

                cv::Rect dstTile(colTile * mTileSize,            
                                 rowTile * mTileSize, 
                                 mTileSize, 
                                 mTileSize);

                copySourceTile(sourceImage, tileInput, srcTile);
                processTileImpl(tileInput, tileOutput);
                copyTileToResultImage(tileOutput, resultImage, dstTile);
            }
        }
    }

protected:
    virtual void processTileImpl(const cv::Mat&amp;amp; srcTile, cv::Mat&amp;amp; dstTile) const = 0;
    
    void copySourceTile(const cv::Mat&amp;amp; src, cv::Mat&amp;amp; srcTile, cv::Rect &amp;amp;tile) const;
    void copyTileToResultImage(const cv::Mat&amp;amp; tileImage, cv::Mat&amp;amp; resultImage, cv::Rect resultRoi);

};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;processing_with_paddings.png&#34; alt=&#34;Processing with paddings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To fill a tile with source image we should check whether tile is close to image border. In this case OpenCV will come to help with cv::copyMakeBorder function that helps us to fill the missing pixels with given border fill method. If tile including paddings are entirely in the image boundary, it&amp;rsquo;s enough to just copy image region to a tile:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void copySourceTile(const cv::Mat&amp;amp; src, cv::Mat&amp;amp; srcTile, cv::Rect &amp;amp;tile)
{
    auto tl = tile.tl();
    auto br = tile.br();

    cv::Point tloffset, broffset;

    //Take care of border cases
    if (tile.x &amp;lt; 0)
    {
        tloffset.x = -tile.x;
        tile.x = 0;
    }

    if (tile.y &amp;lt; 0)
    {
        tloffset.y = -tile.y;
        tile.y = 0;
    }

    if (br.x &amp;gt;= src.cols)
    {
        broffset.x = br.x - src.cols + 1;
        tile.width -= broffset.x;
    }

    if (br.y &amp;gt;= src.rows)
    {
        broffset.y = br.y - src.rows + 1;
        tile.height -= broffset.y;
    }

    // If any of the tile sides exceed source image boundary we must use copyMakeBorder to make proper paddings for this side
    if (tloffset.x &amp;gt; 0 || tloffset.y &amp;gt; 0 || broffset.x &amp;gt; 0 || broffset.y &amp;gt; 0)
    {
        cv::Rect paddedTile(tile.tl(), tile.br());
        assert(paddedTile.x &amp;gt;= 0);
        assert(paddedTile.y &amp;gt;= 0);
        assert(paddedTile.br().x &amp;lt; src.cols);
        assert(paddedTile.br().y &amp;lt; src.rows);

        cv::copyMakeBorder(src(paddedTile), srcTile, tloffset.y, broffset.y, tloffset.x, broffset.x, mBorderType);
    }
    else
    {
        // Entire tile (with paddings lies inside image and it&#39;s safe to just take a region:
        src(tile).copyTo(srcTile);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For non-zero padding case we add additional pixels to source tile, therefore it has effective width and height of &lt;code&gt;TileSize + Padding + Padding&lt;/code&gt;, but after processing we write only central segment of the tile of size &lt;code&gt;TileSize x TileSize&lt;/code&gt; to destination image. In case of Sobel, we need a padding of &lt;code&gt;1&lt;/code&gt;, because Sobel uses 3x3 kernel by default.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void copyTileToResultImage(const cv::Mat&amp;amp; tileImage, cv::Mat&amp;amp; resultImage, cv::Rect resultRoi)
{
    cv::Rect srcTile(mPadding, mPadding, mTileSize, mTileSize);

    auto br = resultRoi.br();

    if (br.x &amp;gt;= resultImage.cols)
    {
        resultRoi.width -= br.x - resultImage.cols;
        srcTile.width -= br.x - resultImage.cols;
    }

    if (br.y &amp;gt;= resultImage.rows)
    {
        resultRoi.height -= br.y - resultImage.rows;
        srcTile.height -= br.y - resultImage.rows;
    }

    cv::Mat tileView = tileImage(srcTile);
    cv::Mat dstView = resultImage(resultRoi);

    assert(tileView.rows == dstView.rows);
    assert(tileView.cols == dstView.cols);

    tileView.copyTo(dstView);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;application&#34;&gt;Application&lt;/h2&gt;

&lt;p&gt;This approach can be used when you need to guarantee low-memory footprint of your algorithm or you want to use data locality without changing a lot in your code. In this
case I suggest to pre-allocate data buffers as a continuous block of memory:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// Our algorithm need three intermediate buffers: a,b,c that
// we want to store close to each other
class MyAlgorithm : public TiledAlgorithm
{
public:
    MyAlgorithm(int tileSize, int padding)
    {
        int size = tileSize + padding * 2;

        // Allocate all buffer as continuous array
        mBuffer.create(size * 3, size, CV_8UC1);
            
        // Create views to sub-regions of mBuffer
        a = mBuffer.rowRange(0,      size);
        b = mBuffer.rowRange(size,   2*size);
        c = mBuffer.rowRange(2*size, 3*size);
    }

private:
    cv::Mat mBuffer;

    cv::Mat a, b c;
}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Did you know, that JPEG-2000 coded use tile-based encoding and it allows this codec to retrieve (decode) an arbitrary region of the image? Also, tiles are widely used in aerial photography to stich images.&lt;/p&gt;

&lt;p&gt;I hope you find this post interesting. Pleas let me know on which topics you would like to see in my blog. Feel free to drop a ping on &lt;a href=&#34;https://twitter.com/cvtalks&#34;&gt;@cvtalks&lt;/a&gt; or leave a comment. Thanks!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image processing in your browser - Unit Test automation</title>
      <link>https://example.com/image-processing-in-your-browser-unit-test-automation/</link>
      <pubDate>Fri, 31 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/image-processing-in-your-browser-unit-test-automation/</guid>
      <description>&lt;p&gt;JavaScript. Do you like debug JavaScript code? I hate it. Literally.
What what if you have to? In this post I&amp;rsquo;m going to show you how to
simplify your life by automating unit testing of the JavaScript code
for the browser.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;monkeys.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To get things more interesting - let&amp;rsquo;s automate unit-testing of the
image processing library called &lt;a href=&#34;http://inspirit.github.io/jsfeat/&#34;&gt;JSFeat&lt;/a&gt;. JSFeat provides a
JavaScript implementation of the basic image processing operations
that let you to process images in your browser and build sophisticated
algorithms. &lt;strong&gt;It&amp;rsquo;s like OpenCV for web-browser&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/BloodAxe/jsfeat&#34;&gt;source code&lt;/a&gt; for this tutorial is available on my Github page: &lt;a href=&#34;https://github.com/BloodAxe/jsfeat&#34;&gt;https://github.com/BloodAxe/jsfeat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Typically, when we test C++ software, we end up with test framework of your
choise and test runner. It can be continuous integration server like Jenkins
in your company or public Travic-CI service for open-source project.&lt;/p&gt;

&lt;p&gt;With browser JavaScript things gets more complicated. I&amp;rsquo;m not talking about
JS unit-test frameworks - &lt;a href=&#34;https://github.com/mochajs/mocha&#34;&gt;mocha&lt;/a&gt; is more than enough. I&amp;rsquo;m talking about
browser testing itself. Basically you have to open a webpage in your browser to
invoke a test cases. Manually. Ew!&lt;/p&gt;

&lt;p&gt;Moreover, due to browser sandbox, you are not allowed to access canvas data for
local files. In practice it means that code like showed below &lt;strong&gt;won&amp;rsquo;t work if you
open a HTML page as a local file&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;var img = new Image();
img.src = &#39;dummy.jpg&#39;;
img.onload = function() {
    // This will never happen
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will have to setup a local HTTP server to serve these tests pages to get this
works. One simple way to do it by using python: &lt;code&gt;python -m SimpleHTTPServer 8000&lt;/code&gt; will
do the job. However it&amp;rsquo;s only a partial solution.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s imagine that you have a lot of these tests. Would you open each page
individually, watch how they run and wait to analyze their results? No doubts,
you can do it, but this is not automated testing in any way.&lt;/p&gt;

&lt;p&gt;To recap, here are a list of problems that exists in browser JS testing:
 - A test framework
 - Have to open test pages in a browser
 - Hard to automate and collect results
 - Need local webserver&lt;/p&gt;

&lt;p&gt;Since I&amp;rsquo;m used to Mocha, i will use it. However it&amp;rsquo;s not obligatory and you can chose any other test framework you like. But as you will see later, with Mocha it&amp;rsquo;s really simple. ith mocha you can write your scripts like showed below. This is a real test I wrote as an example for JSFeat:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// Source test/test_grayscale.js
&#39;use strict&#39;;

describe(&#39;jsfeat&#39;, function(){

  describe(&#39;imgproc&#39;, function(){

    it(&#39;grayscale&#39;, function(done) {

      var img = new Image();
      img.src = &#39;lena.png&#39;;
      img.onload = function() {

        var width = img.width;
        var height = img.height;

        var canvas = document.createElement(&#39;canvas&#39;);
        var context2d = canvas.getContext(&#39;2d&#39;);

        context2d.drawImage(img, 0, 0, width, height);
        var image_data = context2d.getImageData(0, 0, width, height);
 
        var gray_img = new jsfeat.matrix_t(width, height, jsfeat.U8_t | jsfeat.C1_t);
        var code = jsfeat.COLOR_RGBA2GRAY;

        jsfeat.imgproc.grayscale(image_data.data, width, height, gray_img, code);
        done();
      };

    });

  });
   
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is a really powerful tool in Nodejs world called [Grunt][grunt] that we will use
to automate tasks like JavaScript static code checking, minification and testing.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install -g grunt-cli
npm install grunt --save-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A headless browser is a full-featured browser engine without graphical interface. It was designed to simulate a real browser including DOM and JavaScript. The most important headless browser is &lt;a href=&#34;http://phantomjs.org/&#34;&gt;PhantomJS&lt;/a&gt;. I found that it works like a charm for this task. With phantomjs we can run arbitrary HTML page inside and execute JavaScript code. This tool let&amp;rsquo;s us to get rid of the manual tabs openning.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// script.js:
// Simple Javascript example

console.log(&#39;Loading a web page&#39;);
var page = require(&#39;webpage&#39;).create();
var url = &#39;http://www.phantomjs.org/&#39;;
page.open(url, function (status) {
  //Page is loaded!
  phantom.exit();
});

phantomjs script.js
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PhantomJS and Mocha are already connected together in a single grunt task called &lt;a href=&#34;https://github.com/jdcataldo/grunt-mocha-phantomjs&#34;&gt;grunt-mocha-phantomjs&lt;/a&gt;. And we use &lt;a href=&#34;https://github.com/gruntjs/grunt-contrib-connect&#34;&gt;grunt-contrib-connect&lt;/a&gt; to host a local webserver.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install grunt-mocha-phantomjs --save-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This extension does exactly what we need: it starts a local webserver, open page in phantomjs and run JS test cases.
With help of it, we are able to run all our tests using simple command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;grunt test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s an example output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Running &amp;quot;concat:jsfeat&amp;quot; (concat) task
File build/jsfeat.js created.

Running &amp;quot;uglify:build&amp;quot; (uglify) task
&amp;gt;&amp;gt; 1 file created.

Running &amp;quot;connect:server&amp;quot; (connect) task
Started connect web server on http://0.0.0.0:8000

Running &amp;quot;mocha_phantomjs:all&amp;quot; (mocha_phantomjs) task


  jsfeat
    imgproc
      ✓ grayscale 


  1 passing (41ms)


Done, without errors.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Within this setup you are now able to automate testing of JavaScript code that require interaction with HTML5 Canvas features. This way I test the code that I write for browser image processing. I hope you enjoyed this post and I&amp;rsquo;m looking forward to see your questions and mentions in a comments!&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/BloodAxe/jsfeat&#34;&gt;source code&lt;/a&gt; for this tutorial is available on my Github page: &lt;a href=&#34;https://github.com/BloodAxe/jsfeat&#34;&gt;https://github.com/BloodAxe/jsfeat&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Argument checking for native addons for Node.js. Do it right!</title>
      <link>https://example.com/how-to-convert-args-from-js-to-cpp/</link>
      <pubDate>Thu, 11 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/how-to-convert-args-from-js-to-cpp/</guid>
      <description>

&lt;div class=&#34;featured-image&#34;&gt;
![NanCheck](logo.jpg)
&lt;/div&gt;

&lt;p&gt;During development of &lt;a href=&#34;https://cloudcv.io&#34;&gt;CloudCV&lt;/a&gt; I came to the problem on converting &lt;code&gt;v8::Arguments&lt;/code&gt; to
native C++ data types in my Node.js native module. If you are new to C++ and Node.js, I suggest you to read how to write C++ modules for Node.js and connecting OpenCV and Node.js first.&lt;/p&gt;

&lt;p&gt;Mapping V8 data types to native C++ equivalents is trivial, but somewhat wordy. One should take the
argument at given index, check whether it is defined, then check it&amp;rsquo;s type and finally cast to C++ type.
This works fine while you have function that receive two or three arguments of trivial type (That can be mapped directly to built-in C++ types). What about strings? Arrays? Complex types like objects or function callback?
You code will grow like and became hard-to-maintain pasta-code some day.&lt;/p&gt;

&lt;p&gt;In this post I present my approach on solving this problem with a laconic way on describing what do you expect as input arguments.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;
&lt;div class=&#34;clearfix&#34;&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;To illustrate the difference between imperative approach I included source code for calibrationPatternDetect method that expose function to detect calibration pattern on a single image to Node.js code. As you may see below, there are a lot of &lt;em&gt;if&lt;/em&gt; conditions, magic numbers and no type checking for a half of arguments. But even without it, this function occupy 50 lines of code.
What even worse, 90% of this code is going to be the same for other functions. The main purpose of code of any &lt;code&gt;NAN_METHOD&lt;/code&gt; implementation - to marshal data in such a way it can be used by C++ code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;NAN_METHOD(calibrationPatternDetect)
{
    NanScope();

    if (args.Length() != 5)
    {
        return NanThrowError(&amp;quot;Invalid number of arguments&amp;quot;);  
    }

    if (!args[0]-&amp;gt;IsObject())
    {
        return NanThrowTypeError(&amp;quot;First argument should be a Buffer&amp;quot;);      
    }

    // 0 - image
    // 1 - width
    // 2 - height
    // 3 - pattern
    // 4 - callback

    int w  = args[1]-&amp;gt;Uint32Value();
    int h  = args[2]-&amp;gt;Uint32Value();
    int pt = args[3]-&amp;gt;Uint32Value();
    PatternType pattern;

    switch (pt)
    {
        case 0:
            pattern = CHESSBOARD;
            break;

        case 1:
            pattern = CIRCLES_GRID;
            break;

        case 2:
            pattern = ASYMMETRIC_CIRCLES_GRID;
            break;    

        default:
            return NanThrowError(&amp;quot;Unsupported pattern type. Only 0 (CHESSBOARD), 1 (CIRCLES_GRID) or 2 (ASYMMETRIC_CIRCLES_GRID) are supported.&amp;quot;);
    };

    if (!args[4]-&amp;gt;IsFunction())
    {
        return NanThrowTypeError(&amp;quot;Last argument must be a function.&amp;quot;);
    }

    // The task holds our custom status information for this asynchronous call,
    // like the callback function we want to call when returning to the main
    // thread and the status information.

    NanCallback *callback = new NanCallback(args[4].As&amp;lt;Function&amp;gt;());
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So the goal is to add more syntax sugar for argument checking.
Basically, it should provide a convenient way to check number and type of arguments passed.
For &lt;a href=&#34;https://cloudcv.io&#34;&gt;CloudCV&lt;/a&gt; project I&amp;rsquo;ve ended with a declarative approach because I found it fit my needs very much
and makes argument checking self-explanatory. Here is how new implementation of &lt;code&gt;calibrationPatternDetect&lt;/code&gt; looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;NAN_METHOD(calibrationPatternDetect)
{
    NanScope();

    Local&amp;lt;Object&amp;gt;   imageBuffer;
    Local&amp;lt;Function&amp;gt; callback;
    cv::Size        patternSize;
    PatternType     pattern;

    try
    {
        if (NanCheck(args).ArgumentsCount(5)
            .Argument(0).IsBuffer().Bind(imageBuffer)
            .Argument(1).Bind(patternSize.width)
            .Argument(2).Bind(patternSize.height)
            .Argument(3).StringEnum&amp;lt;PatternType&amp;gt;({ 
                { &amp;quot;CHESSBOARD&amp;quot;,     PatternType::CHESSBOARD }, 
                { &amp;quot;CIRCLES_GRID&amp;quot;,   PatternType::CIRCLES_GRID }, 
                { &amp;quot;ACIRCLES_GRID&amp;quot;,  PatternType::ACIRCLES_GRID } }).Bind(pattern)
            .Argument(4).IsFunction().Bind(callback))
        {
            NanCallback *nanCallback = new NanCallback(callback);
            NanAsyncQueueWorker(new DetectPatternTask(imageBuffer, patternSize, pattern, nanCallback));
            NanReturnValue(NanTrue());
        }

        NanReturnValue(NanFalse());
    }
    catch (ArgumentMismatchException exc)
    {
        return NanThrowTypeError(exc.what());
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I hope you agree that second version is much more easy to read. Fluent architecture allows to write predicates in a chain, which actually is very similar to the way we thing. All predicate has self-telling names made from verb and a noun. So let me give you a brief overview what &lt;code&gt;NanCheck&lt;/code&gt; is capable of.&lt;/p&gt;

&lt;h2 id=&#34;fluent-api&#34;&gt;Fluent API&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Method_chaining&#34;&gt;Method chaining&lt;/a&gt; (aka Fluent API) makes it very easy to build final predicate for argument checking via consecutive checks.
Each next step will be made &lt;strong&gt;if and only if&lt;/strong&gt; all previous predicates were successful.
In case of error, predicate will throw an &lt;code&gt;ArgumentMismatchException&lt;/code&gt; exception that will terminate all further checks. &lt;code&gt;NanCheck(args)&lt;/code&gt; can be evaluated to &lt;code&gt;bool&lt;/code&gt; which makes it possible to use NanCheck in a condition statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;if (NanCheck(args). ...) {
    // This code will be executed if argument parsing
    // will be successful        
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;type-checking&#34;&gt;Type checking&lt;/h2&gt;

&lt;p&gt;To check particular argument at given index, &lt;code&gt;NanCheckArguments&lt;/code&gt; provide a &lt;code&gt;Argument(index)&lt;/code&gt; function. This function lets you to build a sub-predicate for given argument and bind it&amp;rsquo;s value with particular local variable:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    if (NanCheck(args).Argument(0).IsBuffer()) {
        // This code will be executed if argument parsing
        // will be successful        
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Currently, &lt;code&gt;NanCheck&lt;/code&gt; support type checking of the following built-in V8 types:
1. &lt;code&gt;v8::Function&lt;/code&gt;
2. &lt;code&gt;v8::Object&lt;/code&gt;
3. &lt;code&gt;v8::String&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In addition, it offers &lt;code&gt;NotNull&lt;/code&gt; predicate to ensure argument is not null or empty.
The list of predicates will grow for sure. New functions to check whether argument is &lt;code&gt;v8::Array&lt;/code&gt;, &lt;code&gt;v8::Number&lt;/code&gt;, &lt;code&gt;v8::Integer&lt;/code&gt;, &lt;code&gt;v8::Boolean&lt;/code&gt; will be added in a next updates.&lt;/p&gt;

&lt;h2 id=&#34;binding&#34;&gt;Binding&lt;/h2&gt;

&lt;p&gt;After type checking, it&amp;rsquo;s necessary to complete sub-predicate construction by &lt;em&gt;binding&lt;/em&gt; argument to a local variable. Binding is a assignment of the argument (with data marshaling, if it&amp;rsquo;s necessary) to a variable that will be used later;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    Local&amp;lt;Object&amp;gt;   imageBuffer;
    if (NanCheck(args).Argument(0).IsBuffer().Bind(imageBuffer)) {
        // This code will be executed if argument parsing
        // will be successful        
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NanCheck support transparent binding to all v8 types (Number, String, Function, Object, Array, etc.), native C++ and OpenCV types (via &lt;a href=&#34;https://github.com/BloodAxe/CloudCVBackend/blob/master/src/framework/marshal/opencv.cpp&#34;&gt;marshaling system&lt;/a&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    cv::Size        patternSize;
    if (NanCheck(args).Argument(1).IsObject().Bind(patternSize)) {
        // This code will be executed if argument parsing
        // will be successful        
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is a special case of string arguments called &lt;code&gt;StringEnum&lt;/code&gt; - that is, a string argument, which can be one of a priory defined values. It introduced to support *&lt;em&gt;C++ enum&lt;/em&gt; types and pass them
as string constants. &lt;code&gt;StringEnum&lt;/code&gt; predicate allow to parse string value and map to C++ enum type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    PatternType     pattern;
    if (NanCheck(args)
        .Argument(3).StringEnum&amp;lt;PatternType&amp;gt;({ 
            { &amp;quot;CHESSBOARD&amp;quot;,     PatternType::CHESSBOARD }, 
            { &amp;quot;CIRCLES_GRID&amp;quot;,   PatternType::CIRCLES_GRID }, 
            { &amp;quot;ACIRCLES_GRID&amp;quot;,  PatternType::ACIRCLES_GRID } }).Bind(pattern)) {
        // This code will be executed if argument parsing
        // will be successful        
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;implementation-highlights&#34;&gt;Implementation highlights&lt;/h2&gt;

&lt;p&gt;Thanks to C++11, it&amp;rsquo;s really easy to construct predicate chain using lambda functions. Basically predicate chain is nothing but a recursive anonymous function of the following form:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    auto initFn = [innerPredicate, outerPredicate](const v8::Arguments&amp;amp; args) {
        return innerPredicate(args) &amp;amp;&amp;amp; outerPredicate(args);
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To illustrate an idea of building predicate chain, let&amp;rsquo;s take a look on ArgumentsCount implementation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    NanCheckArguments&amp;amp; NanCheckArguments::ArgumentsCount(int count)
    {
        return AddAndClause([count](const v8::Arguments&amp;amp; args) 
        { 
            if (args.Length() != count)
                throw ArgumentMismatchException(args.Length(), count); 

            return true;
        });
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we construct outer predicate which compare number of arguments to expected value
and throw an exception if it does not match.&lt;/p&gt;

&lt;p&gt;With a help of &lt;code&gt;std::initializer_list&lt;/code&gt; it became really simple to declare string enum with minimal syntax overhead:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    class NanMethodArgBinding
    {
    public:
    ...
        template &amp;lt;typename T&amp;gt;
        NanArgStringEnum&amp;lt;T&amp;gt; 
        StringEnum(std::initializer_list&amp;lt; std::pair&amp;lt;const char*, T&amp;gt; &amp;gt; possibleValues);
    ...
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;re able to call this function with arbitrary number of elements for this enum using
&lt;code&gt;std::initializer_list&lt;/code&gt; syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    { 
        { &amp;quot;CHESSBOARD&amp;quot;,     PatternType::CHESSBOARD }, 
        { &amp;quot;CIRCLES_GRID&amp;quot;,   PatternType::CIRCLES_GRID }, 
        { &amp;quot;ACIRCLES_GRID&amp;quot;,  PatternType::ACIRCLES_GRID }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/BloodAxe/CloudCVBackend/blob/master/src/framework/NanCheck.hpp&#34;&gt;&lt;strong&gt;NanCheck&lt;/strong&gt;&lt;/a&gt; helped me to reduce amount of code required to check arguments passed to &lt;a href=&#34;https://cloudcv.io&#34;&gt;CloudCV&lt;/a&gt; back-end. There are many cool ideas that I&amp;rsquo;ll probably add as soon as there will be necessity to have them in my library:
- Strongly typed objects (Objects with required fields)
- Optional parameters with default values
- Automatic type inference based on &lt;code&gt;Bind&amp;lt;T&amp;gt;(...)&lt;/code&gt; type.
- Support of multiple types per argument (Parameter can be either of type A or B)&lt;/p&gt;

&lt;p&gt;Please leave your comments on this post. I&amp;rsquo;ve spent many hours on figuring out how to implement data marshaling and type checking in V8 and Node.js, so please help information to
spread out - share and re-tweet this post. Cheers!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mapping data from Eigen to OpenCV and back</title>
      <link>https://example.com/mapping-eigen-to-opencv/</link>
      <pubDate>Sat, 16 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/mapping-eigen-to-opencv/</guid>
      <description>

&lt;div class=&#34;featured-image&#34;&gt;
![Eigen2CV](eigen2cv.png)
&lt;/div&gt;

&lt;p&gt;Eigen is a C++ template library for matrix and vector operations.
It is highly optimized for numeric operations and support vectorization and
use aligned memory allocators.&lt;/p&gt;

&lt;p&gt;When it comes to matrix operations, Eigen is much faster than OpenCV.
However, it can be situations when it is necessary to pass Eigen data
to OpenCV functions.&lt;/p&gt;

&lt;p&gt;In this post I will show how to map Eigen data to OpenCV with easy and efficient
way. No copy, minimal overhead and maximum syntax sugar:
&lt;div class=&#34;clearfix&#34;&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simple case&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;Eigen::ArrayXXd img(480, 640);
...
cv::imshow(&amp;quot;test&amp;quot;, eigen2cv(img));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Proposed approach does not limited to continuous memory layout - it support expression and blocks
as well. If given expression has to be evaluated - it will be evaluated into temporary dense storage
and then mapped to OpenCV structure:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expressions&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// Unsharp mask
Eigen::ArrayXXd img, blur;    
cv::GaussianBlur(eigen2cv(img), eigen2cv(blur));

cv::imshow(&amp;quot;sharpened&amp;quot;, eigen2cv(1.5 * img - 0.5 * blur));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;idea&#34;&gt;Idea&lt;/h2&gt;

&lt;p&gt;In fact, Eigen heavily use &lt;a href=&#34;#1&#34;&gt;C++ templates magic&lt;/a&gt; to create expression structures with delayed evaluation and &lt;a href=&#34;#2&#34;&gt;type traits&lt;/a&gt; to detect type of derived objects in compile time.
This approach gives compiler a lot of hints on actual data layout which helps to generate more efficient code.
The drawback of this - if you want to deep dive in Eigen internals be prepared to hardcore.&lt;/p&gt;

&lt;p&gt;I will use templates as well. We will have template class &lt;code&gt;Eigen2CV&lt;/code&gt; and several specializations of this class - for planar types, for blocks, for expression and so on&amp;hellip;
In addition we will specialize this class with mutable specification which will
let us to define &lt;u&gt;at compile time&lt;/u&gt; whether mapped object is allowed for writing or not. Awesome.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;
    typename Derived, 
    typename Base, 
    typename ConstPolicy, 
    typename StorageKind = typename Eigen::internal::traits&amp;lt;Derived&amp;gt;::StorageKind 
    &amp;gt;
class Eigen2CV;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To interact with OpenCV, we can declare implicit conversion operators for &lt;code&gt;cv::Mat&lt;/code&gt;, &lt;code&gt;cv::InputArray&lt;/code&gt; and &lt;code&gt;cv::OutputArray&lt;/code&gt;.
Some of the mapped objects can have read/write access while the rest  - read-only.
Therefore we will introduce the base class &lt;code&gt;Eigen2CVBase&lt;/code&gt; to provide a &amp;ldquo;read-only&amp;rdquo; access for all derived objects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;class Eigen2CVBase
{
public:
    
    operator cv::Mat() const
    {
        return mBody;
    }
    
    operator cv::_InputArray() const
    {
        return cv::_InputArray(mBody);
    }
    
protected:
    
    template&amp;lt;typename Derived&amp;gt;
    void mapPlaneMemory(const Derived&amp;amp; src)
    {
        const bool isRowMajor = int(Derived::Flags) &amp;amp; Eigen::RowMajorBit;
        const int stride = src.outerStride() * sizeof(typename Derived::Scalar);
        
        if (isRowMajor)
            this-&amp;gt;mapPlaneMemoryRowMajor(src.data(),
                                         src.rows(),
                                         src.cols(),
                                         stride);
        else
            this-&amp;gt;mapPlaneMemoryColMajor(src.data(),
                                         src.rows(),
                                         src.cols(),
                                         stride);
    }

    template &amp;lt;typename Scalar&amp;gt;
    void mapPlaneMemoryRowMajor(const Scalar* planeData, int rows, int cols, int stride)
    {
        this-&amp;gt;mBody = cv::Mat(rows, 
                              cols, 
                              opencv_matrix&amp;lt;Scalar&amp;gt;::type, 
                              const_cast&amp;lt;Scalar*&amp;gt;(planeData), 
                              stride);
    }
    
    template &amp;lt;typename Scalar&amp;gt;
    void mapPlaneMemoryColMajor(const Scalar* planeData, int rows, int cols, int stride)
    {
        this-&amp;gt;mBody = cv::Mat(cols, 
                              rows, 
                              opencv_matrix&amp;lt;Scalar&amp;gt;::type, 
                              const_cast&amp;lt;Scalar*&amp;gt;(planeData), 
                              stride);
    }

    template &amp;lt;typename Derived, typename T&amp;gt;
    void assignMatrix(Eigen::DenseBase&amp;lt;Derived&amp;gt;&amp;amp; dst, const cv::Mat_&amp;lt;T&amp;gt;&amp;amp; src)
    {
        typedef typename Derived::Scalar Scalar;
        typedef Eigen::Matrix&amp;lt;T, 
                              Eigen::Dynamic, 
                              Eigen::Dynamic, 
                              Eigen::RowMajor&amp;gt; PlainMatrixType;
        
        dst = Eigen::Map&amp;lt;PlainMatrixType&amp;gt;((T*)src.data, 
                                          src.rows, 
                                          src.cols).
                                          template cast&amp;lt;Scalar&amp;gt;();
    }
    
    cv::Mat mBody;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For those Eigen types that allows read/write access, we will have additional conversion operator to cv::OutputArray. By default all derived types will have read-only access.&lt;/p&gt;

&lt;h2 id=&#34;mapping-eigen-plain-objects&#34;&gt;Mapping Eigen plain objects&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start from mapping continuous block of memory represented by &lt;code&gt;Eigen::Matrix&lt;/code&gt; of &lt;code&gt;Eigen::Array&lt;/code&gt;.
These two classes derives from &lt;code&gt;Eigen::PlainObjectBase&lt;/code&gt; class which provides methods to access internal storage
buffer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename Derived&amp;gt;
class Eigen2CV&amp;lt;
    Derived, 
    Eigen::PlainObjectBase&amp;lt;Derived&amp;gt;, 
    details::Const&amp;gt; : public Eigen2CVBase
{
public:
    
    typedef typename Derived::Scalar Scalar;
    typedef Eigen2CV&amp;lt;Derived, Eigen::PlainObjectBase&amp;lt;Derived&amp;gt;, details::Mutable&amp;gt; Self;
    
    Eigen2CV(const Eigen::PlainObjectBase&amp;lt;Derived&amp;gt;&amp;amp; src)
    : mMappedView(src)
    {
        this-&amp;gt;mapPlaneMemory(mMappedView);
    }
           
private:
    const Eigen::PlainObjectBase&amp;lt;Derived&amp;gt;&amp;amp; mMappedView;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is &lt;code&gt;Eigen2CV&lt;/code&gt; specialization for constant &lt;code&gt;Eigen::PlainObjectBase&lt;/code&gt; object. This specialization of &lt;code&gt;Eigen2CV&lt;/code&gt; can return constant reference to &lt;code&gt;cv::Mat&lt;/code&gt; and &lt;code&gt;cv::InputArray&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now we can write two overloads of &lt;code&gt;eigen2cv&lt;/code&gt; function for &lt;code&gt;Eigen::Matrix&lt;/code&gt; and &lt;code&gt;Eigen::Array&lt;/code&gt;. The goal of &lt;code&gt;eigen2cv&lt;/code&gt; is simple - take an argument and create &amp;lsquo;right&amp;rsquo; Eigen2CV&amp;lt;&amp;hellip;&amp;gt; mapper.
Here is how it looks like for planar data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename E&amp;gt;
Eigen2CV&amp;lt;E, Eigen::PlainObjectBase&amp;lt;E&amp;gt;, details::Mutable&amp;gt; 
eigen2cv(Eigen::PlainObjectBase&amp;lt;E&amp;gt;&amp;amp; src) 
{
    return Eigen2CV&amp;lt;E, 
                    Eigen::PlainObjectBase&amp;lt;E&amp;gt;, 
                    details::Mutable
                    &amp;gt;(src));
}

template&amp;lt;typename E&amp;gt;
Eigen2CV&amp;lt;E, Eigen::PlainObjectBase&amp;lt;E&amp;gt;, details::Const&amp;gt;
eigen2cv(const Eigen::PlainObjectBase&amp;lt;E&amp;gt;&amp;amp; src) 
{
    return Eigen2CV&amp;lt;E, 
                    Eigen::PlainObjectBase&amp;lt;E&amp;gt;, 
                    details::Const
                    &amp;gt;(src);
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I want to draw your attention to how elegant C++ allows us to distinct mutable and constant objects.
Compiler will choose right function depending on the context of &lt;code&gt;src&lt;/code&gt;.
In case of access right conflicts you will get compile-time error.&lt;/p&gt;

&lt;h2 id=&#34;assigning-opencv-matrix-to-eigen-object&#34;&gt;Assigning OpenCV matrix to Eigen object&lt;/h2&gt;

&lt;p&gt;What if someone write:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;eigen2cv(a) = cv::imread(&amp;quot;lena.jpg&amp;quot;, cv::IMREAD_GRAYSCALE);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Well, it&amp;rsquo;s legal and I see no problems with this code while we follow few restrictions:
1. &lt;code&gt;data&lt;/code&gt; has dynamic size or fixed one which match cv::Mat size.
2. Image is single channel - there is no way to map multi-channel images to Eigen now.&lt;/p&gt;

&lt;p&gt;Assignment operator is also quite simple:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template &amp;lt;typename T&amp;gt;
Self&amp;amp; operator=(const cv::Mat_&amp;lt;T&amp;gt;&amp;amp; src)
{
    assignMatrix&amp;lt;Derived, T&amp;gt;(mMappedView, src);
    return *this;
}

/**
 * @brief Assignment operator to copy OpenCV Mat data to mapped Eigen object.
 */
Self&amp;amp; operator= (const cv::Mat&amp;amp; m)
{
    switch (m.type())
    {
        case CV_8U:  return *this = (cv::Mat_&amp;lt;uint8_t&amp;gt;)m;
        case CV_16U: return *this = (cv::Mat_&amp;lt;uint16_t&amp;gt;)m;
        case CV_16S: return *this = (cv::Mat_&amp;lt;int16_t&amp;gt;)m;
        case CV_32S: return *this = (cv::Mat_&amp;lt;int32_t&amp;gt;)m;
        case CV_32F: return *this = (cv::Mat_&amp;lt;float&amp;gt;)m;
        case CV_64F: return *this = (cv::Mat_&amp;lt;double&amp;gt;)m;
        default:
            throw std::runtime_error(&amp;quot;Unsupported OpenCV matrix type&amp;quot;);
    };
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mapping-eigen-expressions&#34;&gt;Mapping Eigen expressions&lt;/h2&gt;

&lt;p&gt;Dealing with expressions is not much harder.
Depending on the expression type, we must either evaluate it into dense storage (when it&amp;rsquo;s real expression like &lt;code&gt;AX + B&lt;/code&gt;, or &lt;code&gt;cast&amp;lt;float&amp;gt;()&lt;/code&gt;)
or use underlying storage with regards to expression operator (&lt;code&gt;block()&lt;/code&gt;, &lt;code&gt;transpose()&lt;/code&gt;, &lt;code&gt;array()&lt;/code&gt;, &lt;code&gt;matrix()&lt;/code&gt;).
We will get to mapping blocks in a next section.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s map expression that require evaluation first. For the sake of simplicity,
I will not implement write expressions, e.g expressions that require eval/update/write-back.
Eigen2CV will be able map Eigen expressions in read-only mode.
And here&amp;rsquo;s how:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename Derived&amp;gt;
class Eigen2CV&amp;lt;Derived, Eigen::EigenBase&amp;lt;Derived&amp;gt;, details::Const&amp;gt; : public Eigen2CVBase
{
public:
    typedef typename Derived::Scalar Scalar;
    typedef typename Eigen::internal::plain_matrix_type&amp;lt;Derived&amp;gt;::type StorageType;

    Eigen2CV(const Eigen::EigenBase&amp;lt;Derived&amp;gt;&amp;amp; src)
    {
        mStorage = src; // All magic happens here
        this-&amp;gt;mapPlaneMemory(mStorage);
    }

protected:

    void mapPlaneMemory(StorageType&amp;amp; src)
    {
        if ( ( StorageType::Options &amp;amp; Eigen::RowMajor) == Eigen::RowMajor)
            this-&amp;gt;mapPlaneMemoryRowMajor(src.data(), 
                                         src.rows(), 
                                         src.cols(), 
                                         src.outerStride() * sizeof(Scalar));
        else
            this-&amp;gt;mapPlaneMemoryColMajor(src.data(), 
                                         src.rows(), 
                                         src.cols(), 
                                         src.outerStride() * sizeof(Scalar));
    }

private:
    StorageType mStorage;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the first look, it is almost the same as specialization for planar data types. A few differences make this specialization very other one.
First, &lt;code&gt;Eigen::internal::plain_matrix_type&amp;lt;Derived&amp;gt;::type&lt;/code&gt; type trait helps us to infer type of dense storage for given expression.
Second, line &lt;code&gt;mStorage = src&lt;/code&gt; looks really simple right? But hold on, &lt;code&gt;src&lt;/code&gt; is an expression, and &lt;code&gt;mStorage&lt;/code&gt; is dense matrix.
Assignment operator makes our like much easier by performing evaluation step inside this assignment.&lt;/p&gt;

&lt;p&gt;And here is &lt;code&gt;eigen2cv&lt;/code&gt; overload for Eigen expressions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename E&amp;gt;
Eigen2CV&amp;lt;E, Eigen::EigenBase&amp;lt;E&amp;gt;, details::Const&amp;gt;
eigen2cv(const Eigen::EigenBase&amp;lt;E&amp;gt;&amp;amp; src) 
{
    return Eigen2CV&amp;lt;E, 
                    Eigen::EigenBase&amp;lt;E&amp;gt;, 
                    details::Const
                    &amp;gt;(src);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mapping-eigen-views&#34;&gt;Mapping Eigen views&lt;/h2&gt;

&lt;p&gt;User can create sub-view for the Eigen storage using &lt;code&gt;block()&lt;/code&gt;.
Eigen block create view that points to the same memory region, but has different size and stride.
Blocks can be read and written.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;typename Derived&amp;gt;
class Eigen2CV&amp;lt;Derived, 
               Eigen::Block&amp;lt;Derived&amp;gt;, 
               details::Mutable, 
               Eigen::Dense&amp;gt; : public Eigen2CVBase
{
public:
    typename Derived::Scalar Scalar;
    typedef Eigen2CV&amp;lt;Derived, Eigen::Block&amp;lt;Derived&amp;gt;, details::Mutable&amp;gt; Self;

    Eigen2CV(const Eigen::Block&amp;lt;Derived&amp;gt;&amp;amp; src)
        : mMappedView(src)
    {
        this-&amp;gt;mapPlaneMemory(mMappedView);
    }
    
    operator cv::_OutputArray()
    {
        return cv::_OutputArray(this-&amp;gt;mBody);
    }
    
    template &amp;lt;typename T&amp;gt;
    Self&amp;amp; operator=(const cv::Mat_&amp;lt;T&amp;gt;&amp;amp; src)
    {
        MatrixAssign&amp;lt;Derived, T&amp;gt;(mMappedView, src);
        return *this;
    }
    
    /**
     * @brief Assignment operator to copy OpenCV Mat data to mapped Eigen object.
     */
    Self&amp;amp; operator= (const cv::Mat&amp;amp; m) throw ()
    {
        switch (m.type())
        {
            case CV_8U:  return *this = (cv::Mat_&amp;lt;uint8_t&amp;gt;)m;
            case CV_16U: return *this = (cv::Mat_&amp;lt;uint16_t&amp;gt;)m;
            case CV_16S: return *this = (cv::Mat_&amp;lt;int16_t&amp;gt;)m;
            case CV_32S: return *this = (cv::Mat_&amp;lt;int32_t&amp;gt;)m;
            case CV_32F: return *this = (cv::Mat_&amp;lt;float&amp;gt;)m;
            case CV_64F: return *this = (cv::Mat_&amp;lt;double&amp;gt;)m;
            default:
                throw std::runtime_error(&amp;quot;Unsupported OpenCV matrix type&amp;quot;);
        };
    }
    
private:
    const Eigen::Block&amp;lt;Derived&amp;gt;&amp;amp; mMappedView;
};

template&amp;lt;typename E&amp;gt;
Eigen2CV&amp;lt;E, Eigen::Block&amp;lt;E&amp;gt;, details::Mutable&amp;gt;
eigen2cv(const Eigen::Block&amp;lt;E&amp;gt;&amp;amp; src)
{
    return Eigen2CV&amp;lt;E, 
                    Eigen::Block&amp;lt;E&amp;gt;, 
                    details::Mutable
                    &amp;gt;(src);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;eigen-memory-organization&#34;&gt;Eigen memory organization&lt;/h2&gt;

&lt;p&gt;Eigen can use column-major or row-major ordering of internal data storage.
By default it&amp;rsquo;s column-major, but OpenCV use row-major ordering.&lt;/p&gt;

&lt;p class=&#34;info info-warning&#34;&gt;
&lt;span class=&#34;label label-info&#34;&gt;Notice&lt;/span&gt;
This mapping implementation will NOT convert underlying Eigen memory to meet OpenCV convention. 
For column-major order of Eigen data type this will lead to transposed matrices in OpenCV. 
&lt;/p&gt;

&lt;h2 id=&#34;demonstration&#34;&gt;Demonstration&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;Image8u_t a(512, 512); // Eigen::Matrix&amp;lt;uint8_t, Dynamic, Dynamic&amp;gt;

for (size_t i = 0; i &amp;lt; 512; i++)
{
    for (size_t j = 0; j &amp;lt; 512; j++)
    {
        a(i,j) = 255.0f * (sin(0.04f * i) * sin(0.04f * i) + 
                           cos(0.04f * j) * cos(0.04f * j));
    }
}

cv::GaussianBlur(eigen2cv(a.block(128, 128, 256, 256)),
                 eigen2cv(a.block(128, 128, 256, 256)), cv::Size(25,25), 0);
cv::imshow(&amp;quot;Blur image region&amp;quot;, eigen2cv(a));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;blur_roi.png&#34; alt=&#34;Blur image region&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;source-code&#34;&gt;Source code&lt;/h2&gt;

&lt;p&gt;Source code for this post can be found on GitHub: &lt;a href=&#34;https://gist.github.com/BloodAxe/c94d65d5977fb1d3e53f&#34;&gt;Eigen2CV.h&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;License: &lt;a href=&#34;https://tldrlegal.com/license/bsd-3-clause-license-(revised)#summary&#34;&gt;BSD-3&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a name=&#34;#1&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.amazon.com/gp/product/0201704315/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0201704315&amp;linkCode=as2&amp;tag=compvisitalk-20&amp;linkId=2ZA2JDQNEDOQJZFL&#34;&gt;Modern C++ Design: Generic Programming and Design Patterns Applied&lt;/a&gt;&lt;img src=&#34;http://ir-na.amazon-adsystem.com/e/ir?t=compvisitalk-20&amp;l=as2&amp;o=1&amp;a=0201704315&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a name=&#34;#2&#34; /&gt;
2. &lt;a href=&#34;http://www.drdobbs.com/cpp/c-type-traits/184404270&#34;&gt;http://www.drdobbs.com/cpp/c-type-traits/184404270&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A good resource on image processing using Python</title>
      <link>https://example.com/pyimagesearch.com/</link>
      <pubDate>Thu, 07 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/pyimagesearch.com/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.pyimagesearch.com/&#34;&gt;&lt;img src=&#34;logo.png&#34; alt=&#34;www.pyimagesearch.com&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let me introduce you &lt;a href=&#34;http://www.pyimagesearch.com/about/&#34;&gt;Adrian Rosebrock&lt;/a&gt; and his &lt;a href=&#34;http://www.pyimagesearch.com/&#34;&gt;http://www.pyimagesearch.com/&lt;/a&gt; website.
It&amp;rsquo;s about computer vision and image processing using Python and OpenCV.
Looks like there are more than one person that like to share programming experience via blogging :)&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how &lt;a href=&#34;http://www.pyimagesearch.com/&#34;&gt;Adrian&lt;/a&gt; position himself:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This blog is dedicated to helping other programmers understand how image search engines work.
While a lot of computer vision concepts are theoretical in nature,
I’m a big fan of “learning by example”. My goal is to distill my life experiences in building image search engines into concise, easy to understand examples.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I hope you will enjoy reading Adrian&amp;rsquo;s posts on &lt;a href=&#34;http://www.pyimagesearch.com/2014/07/28/a-slic-superpixel-tutorial-using-python/&#34;&gt;superpixels&lt;/a&gt;, &lt;a href=&#34;http://www.pyimagesearch.com/2014/07/14/3-ways-compare-histograms-using-opencv-python/&#34;&gt;histogram matching&lt;/a&gt; and &lt;a href=&#34;http://www.pyimagesearch.com/2014/05/26/opencv-python-k-means-color-clustering/&#34;&gt;color clustering&lt;/a&gt;.
In addition, he wrote a book on using OpenCV in Python.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.pyimagesearch.com/practical-python-opencv/&#34;&gt;&lt;img src=&#34;practical_python_and_opencv_cover_green.png&#34; alt=&#34;Practical Python and OpenCV eBook&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Happy reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Training Haar cascade in the cloud</title>
      <link>https://example.com/cloud-haartaining/</link>
      <pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/cloud-haartaining/</guid>
      <description>

&lt;p&gt;In this post I&amp;rsquo;ll show you how you can train cascade classifier with OpenCV very quickly even if you have low-end hardware using virtual machine in the cloud.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;why-clouds&#34;&gt;Why Clouds?&lt;/h2&gt;

&lt;p&gt;Well, training a descriptor takes a lot of time. Depending on template size, number of samples and stages, it can take from several ours to a couple of days to train such cascade! Be prepared that during training stage your PC will likely be unusuable due to high RAM usage and CPU load. If you&amp;rsquo;re on laptop - it will become hot really quick. So, what if you have to train your cascade, but you don&amp;rsquo;t have either time or spare machine to do this?&lt;/p&gt;

&lt;p&gt;Recently I&amp;rsquo;ve faced this problem in one of my personal projects. What even more funny, a 10 hours flight was approaching, but I didn&amp;rsquo;t wanted to waste this time for nothing. I only had a laptop, but this task will drain my battery for sure. So I&amp;rsquo;ve decided to use virtual server to do this.&lt;/p&gt;

&lt;h2 id=&#34;step-1-environment-setup&#34;&gt;Step 1 - Environment setup&lt;/h2&gt;

&lt;p&gt;First, I&amp;rsquo;ve created a basic droplet in &lt;a href=&#34;https://www.digitalocean.com/?refcode=b93faa829f80&#34;&gt;DigitalOcean&lt;/a&gt;.
Yep, for 5$/month you can have your droplet that can do much!
It takes only 55 seconds to deploy a new instance (I assume you&amp;rsquo;re familiar with SSH keys, terminal, Git and so on.) and we&amp;rsquo;re ready to rock!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;image1.png&#34; alt=&#34;Create DigitalOcean droplet&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;step-2-install-latest-opencv-release&#34;&gt;Step 2 - Install latest OpenCV release&lt;/h2&gt;

&lt;p&gt;There are two ways to do this: either using package managers (homebrew, yum or apt-get) or builiding it from scratch.
Personally I prefer second option since you can configure OpenCV. Usually I build static libs whith apps but without tests, java, cuda, python, OpenEXR, Jasper and Tiff. Regardless of the way you choose to install OpenCV, ensure that opencv apps (opencv_createsamples, opencv_traincascade) are also installed!.&lt;/p&gt;

&lt;h2 id=&#34;step-3-prepare-your-train-data&#34;&gt;Step 3 - Prepare your train data&lt;/h2&gt;

&lt;p&gt;There are a lot of tutorials &lt;a href=&#34;http://note.sonots.com/SciSoftware/haartraining.html&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;http://answers.opencv.org/question/7141/about-traincascade-paremeters-samples-and-other/&#34;&gt;3&lt;/a&gt; on how to train cascade with OpenCV: which images are good for positive and negative samples and which settings should be used for cascade training. Let&amp;rsquo;s assume you have everything in a single folder on your load machine and there is a script called &amp;ldquo;train.sh&amp;rdquo; that starts training stage:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;opencv_traincascade -data classifier \
                    -vec &amp;lt;positive samples file&amp;gt; \
                    -bg &amp;lt;negative samples file&amp;gt; \
                    -numStages 12 \
                    -minHitRate 0.999 \
                    -maxFalseAlarmRate 0.5 \
                    -numPos 15000 -numNeg 17000 \
                    -w 24 \
                    -h 24 \
                    -mode ALL \
                    -nonsym 1 \
                    -featureType LBP
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-4-deploy-train-data-to-cloud&#34;&gt;Step 4 - Deploy train data to cloud&lt;/h2&gt;

&lt;p&gt;The easiest way to upload this folder to your virtual droplet is to use the &lt;a href=&#34;http://en.wikipedia.org/wiki/Rsync&#34;&gt;rsync&lt;/a&gt; tool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rsync -avz &amp;lt;source&amp;gt; &amp;lt;destination&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For instance, the following command will upload the traindata/ folder with it&amp;rsquo;s content to ~haartraining.example.com~ webserver to /traindata directory. This example assumes that your public key has been added to haartraining.example.com during droplet creation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rsync -avz ~/Develop/traindata root@haartraining.example.com:/traindata
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-5-start-training&#34;&gt;Step 5 - Start training&lt;/h2&gt;

&lt;p&gt;The easiest way to execute training is to login to remote maching using ssh and execute the train script with a simple command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh root@haartraining.example.com
sh /traindata/train.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, this will require you to keep SSH-session open all the time. If you log-out, the process will terminate and training will be terminated as well.
To prevent this we can use &lt;a href=&#34;http://en.wikipedia.org/wiki/Nohup&#34;&gt;nohup&lt;/a&gt; UNIX utility:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh root@haartraining.example.com
nohup sh /traindata/train.sh &amp;gt; /traindata/train.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Training will continue to work regardless of the user&amp;rsquo;s connection to the terminal, and log results to the file train.log.&lt;/p&gt;

&lt;h2 id=&#34;step-6-getting-the-results&#34;&gt;Step 6 - Getting the results&lt;/h2&gt;

&lt;p&gt;After trainign is done (you can check this by top command output or looking at the train.log), we can download trainresults back with rsync command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rsync -avz root@haartraining.example.com:/traindata ~/Develop/traindata 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-7-speeding-up-training&#34;&gt;Step 7 - Speeding up training&lt;/h2&gt;

&lt;p&gt;To speed-up training stage I recommend to pass additional options to opencv_traincascade tool:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;precalcValBufSize=2048&lt;/li&gt;
&lt;li&gt;precalcIdxBufSize=2048&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ideally you want to use all available memory of your instance for these buffers, so if you have 4Gb of RAM installed, pass at least a 1Gb to each of these buffers.&lt;/p&gt;

&lt;p&gt;It also may be a good idea to &amp;ldquo;shrink&amp;rdquo; the DigitalOcean&amp;rsquo;s droplet to more powerful configuration which gives you 16Gb of RAM and 8 CPU&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;resize.png&#34; alt=&#34;Resize droplet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This task can be done on few clicks:&lt;/p&gt;

&lt;h3 id=&#34;stop-your-droplet&#34;&gt;Stop your droplet&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;poweroff.png&#34; alt=&#34;Poweroff droplet&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;choose-resize-and-pick-a-necessary-configuration&#34;&gt;Choose &amp;ldquo;Resize&amp;rdquo; and pick a necessary configuration&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;resize2.png&#34; alt=&#34;Resize droplet&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;power-up-your-resized-droplet&#34;&gt;Power-up your resized droplet&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Please be advised, that DigitalOcean will charge you regardless whether your droplet is powered on or off. So if you&amp;rsquo;re not using it - make a snapshot of it and delete unused droplet to save money&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all. I hope you enjoyed reading this post. Please, leave your comments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Travis-CI for continuous testing your projects</title>
      <link>https://example.com/2014-02-23-using-travis-ci/</link>
      <pubDate>Sun, 23 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2014-02-23-using-travis-ci/</guid>
      <description>&lt;p&gt;img.img-thumbnail.pull-left(src=&amp;ldquo;travis-logo.png&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;p
    | In this post i will show you how i implemented continuous integration and testing in my&lt;br /&gt;
    a(href=&amp;ldquo;&lt;a href=&#34;http://cloudcv.io&amp;quot;&#34;&gt;http://cloudcv.io&amp;quot;&lt;/a&gt;) CloudCV
    |  project. Healthy unit tests and easy and continuous integration workflow is a must in any project goes beyound &amp;ldquo;Hello, world&amp;rdquo; application.
    | Today software is a mixture of technologies of all kind. Therefore it can break literally everywhere. Each integration point is a place of risk.
    | The CloudCV has a C++ backend that is using OpenCV library, it&amp;rsquo;s front-end is written in Node.js, they both using V8 JavaScript engine and libuv
    | library to execute jobs asynchronously. Below you&amp;rsquo;ll find a solution how i implemented CI and unit testing for this project.&lt;/p&gt;

&lt;p&gt;span.more&lt;/p&gt;

&lt;p&gt;p
    a(href=&amp;ldquo;&lt;a href=&#34;https://travis-ci.org&amp;quot;&#34;&gt;https://travis-ci.org&amp;quot;&lt;/a&gt;) Travis-CI
    |  is a continuous integration platform for almost any project. It supports C, C++, Closure, Erlang, Haskell, Scala, PHP, Ruby, Python, Java and JavaScript and many other languages.
    | You probably saw any of these images on the landing page on some GitHub repositories:
    img(src=&amp;ldquo;buildpassing.png&amp;rdquo;)
    img(src=&amp;ldquo;buildfailing.png&amp;rdquo;)
    | These are build status indicators that display the last build results of a repository.
    | Travis-CI can run tests after each commit so you can know where exactly the problem is.&lt;/p&gt;

&lt;p&gt;div.alert.alert-success
    p And by the way - it&amp;rsquo;s free for open-source projects!&lt;/p&gt;

&lt;p&gt;h2 Adding Travis-CI to your project&lt;/p&gt;

&lt;p&gt;p
    | Technically, Travis-CI runs build job in a temporary virtual instance that is created for your project and deleted after it finish CI tasks.
    | There is no persistentcy between consecutive builds.
    | To add Travis-CI to your project you may want to do two steps:
    ul
        li Add a &amp;ldquo;.travis.yml&amp;rdquo; manifest file to the root of your repository
        li Register your repository in Travis-CI website.&lt;/p&gt;

&lt;p&gt;h3 Writing your first Travis-CI manifest&lt;/p&gt;

&lt;p&gt;p
    | Travis manifest is a YAML file that define how to build the code in your repository, what is the programming language, which dependencies your project has.
    | For example, you may want to install third-party dependencies, or create dummy database for unit tests. That all you can do with manifest file.&lt;/p&gt;

&lt;p&gt;p
    | Let&amp;rsquo;s take a look on Travis-CI manifest for CloudCVBacked. This project is written in C++, it has two library dependencies: OpenCV and Node.js.
    | Here i will show a full listing of my manifest and explain it line by line:&lt;/p&gt;

&lt;p&gt;pre.
    language: node_js
    before_install:
     - sudo apt-get install build-essential
     - curl -sL &lt;a href=&#34;https://github.com/Itseez/opencv/archive/2.4.6.1.zip&#34;&gt;https://github.com/Itseez/opencv/archive/2.4.6.1.zip&lt;/a&gt; &amp;gt; opencv.zip
     - unzip opencv.zip
     - rm opencv.zip
     - mkdir opencv-build
     - cd opencv-build/
     - cmake -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_DOCS=OFF -DBUILD_EXAMPLES=OFF -DBUILD_opencv_java=OFF -DBUILD_JASPER=ON -DWITH_JASPER=ON -DBUILD_ZLIB=ON -DBUILD_SHARED_LIBS=OFF -DBUILD_TESTS=OFF -DBUILD_PERF_TESTS=OFF -DWITH_OPENEXR=OFF -DBUILD_PNG=ON -DWITH_PNG=ON -DWITH_TIFF=ON -DBUILD_TIFF=ON -DWITH_WEBP=OFF -DWITH_JPEG=ON -DBUILD_JPEG=ON ../opencv-2.4.6.1/
     - sudo make install
     - cd ..&lt;/p&gt;

&lt;p&gt;p
    | The first line define the programming language we will use. It is the most important instruction since it affects the build environment.
    | For example, &amp;ldquo;node_js&amp;rdquo; option will create a virtual machine with a pre-installed
    | Node.js. In addition, it will tell Travis-CI to use package.json file for further build instruction.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pre language: node_js
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;p.info
    | You can specify a set of Node.js versions you want to test. This is called a
    a(href=&amp;ldquo;&lt;a href=&#34;http://docs.travis-ci.com/user/build-configuration/#The-Build-Matrix&amp;quot;&#34;&gt;http://docs.travis-ci.com/user/build-configuration/#The-Build-Matrix&amp;quot;&lt;/a&gt;) Build Matrix
    |. For example, in the example below we ask build server to build project on four versions of node:
    pre language: node_js
        | node_js:
        |   - &amp;ldquo;0.11&amp;rdquo;
        |   - &amp;ldquo;0.10&amp;rdquo;
        |   - &amp;ldquo;0.8&amp;rdquo;
        |   - &amp;ldquo;0.6&amp;rdquo;&lt;/p&gt;

&lt;p&gt;h4 Installing additional dependencies&lt;/p&gt;

&lt;p&gt;p
    | By default, build server uses &amp;ldquo;npm install&amp;rdquo; command to install node.js-based projects.
    | The &amp;ldquo;npm install&amp;rdquo; itself read the package.json file and install every dependency from it using Node Pacakage Manager (npm) tool.
    | To install non-node.js dependencies we can use &amp;ldquo;before_install&amp;rdquo; instruction to install additional software into build environment.
    | Here we download and instal OpenCV 2.4.6.1:
    ul
        li First, we install build essential software (gcc, cmake, etc..)
        ll Download stable opencv package from Github.
        li Unzip it and go into build directory
        li Configure OpenCV using cmake
        li Build and install OpenCV
pre.
    before_install:
     - sudo apt-get install build-essential
     - curl -sL &lt;a href=&#34;https://github.com/Itseez/opencv/archive/2.4.6.1.zip&#34;&gt;https://github.com/Itseez/opencv/archive/2.4.6.1.zip&lt;/a&gt; &amp;gt; opencv.zip
     - unzip opencv.zip
     - rm opencv.zip
     - mkdir opencv-build
     - cd opencv-build/
     - cmake -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_DOCS=OFF -DBUILD_EXAMPLES=OFF -DBUILD_opencv_java=OFF -DBUILD_JASPER=ON -DWITH_JASPER=ON -DBUILD_ZLIB=ON -DBUILD_SHARED_LIBS=OFF -DBUILD_TESTS=OFF -DBUILD_PERF_TESTS=OFF -DWITH_OPENEXR=OFF -DBUILD_PNG=ON -DWITH_PNG=ON -DWITH_TIFF=ON -DBUILD_TIFF=ON -DWITH_WEBP=OFF -DWITH_JPEG=ON -DBUILD_JPEG=ON ../opencv-2.4.6.1/
     - sudo make install
     - cd ..&lt;/p&gt;

&lt;p&gt;h3 Building Node.js C++ module with Travis-CI
p
    | When the build environment is ready (If before_install target succeeded), Travis-CI performs install task (by default it&amp;rsquo;s &amp;ldquo;npm install&amp;rdquo;). Now, it&amp;rsquo;s time for NPM to act.
    | Each module in Node.js usually has &amp;ldquo;package.json&amp;rdquo; file.
    | Usually it contains name, version of repository url of the module, it&amp;rsquo;s description, list of dependencies, pre/post install and test scripts.
    | The pre-install target in CloudCVBacked builds the dynamically-linked C++ library that exports Node.js functions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pre 
    | {
    | &amp;quot;name&amp;quot;: &amp;quot;cloudcv&amp;quot;,        
    | ...
    | &amp;quot;scripts&amp;quot;: {
    |     &amp;quot;preinstall&amp;quot;: &amp;quot;node-gyp clean rebuild&amp;quot;,
    |     &amp;quot;test&amp;quot;: &amp;quot;expresso test/*&amp;quot;
    | },
    | 
    | &amp;quot;devDependencies&amp;quot;: {
    |     &amp;quot;expresso&amp;quot;: &amp;quot;*&amp;quot;
    | },
    | 
    | ...
    | }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;div.alert.alert-info
    p To read more about using C++ and Node.js together please follow related posts:
        ul
            li&lt;br /&gt;
                a(href=&amp;ldquo;&lt;a href=&#34;http://computer-vision-talks.com/articles/2013-08-27-connecting-opencv-and-node-js-inside-cloud9-ide/&amp;quot;):&#34;&gt;http://computer-vision-talks.com/articles/2013-08-27-connecting-opencv-and-node-js-inside-cloud9-ide/&amp;quot;):&lt;/a&gt; strong Connecting OpenCV and Node.js inside Cloud9 IDE
            li
                a(href=&amp;ldquo;&lt;a href=&#34;http://computer-vision-talks.com/articles/2013-08-19-cloud-image-processing-using-opencv-and-node-js/&amp;quot;):&#34;&gt;http://computer-vision-talks.com/articles/2013-08-19-cloud-image-processing-using-opencv-and-node-js/&amp;quot;):&lt;/a&gt; strong Cloud image processing using OpenCV and Node.js&lt;/p&gt;

&lt;p&gt;h4 Node.js modules testing with expresso&lt;/p&gt;

&lt;p&gt;p
    | For testing of CloudCV I&amp;rsquo;m using
    a(href=&amp;ldquo;&lt;a href=&#34;http://visionmedia.github.io/expresso/&amp;quot;&#34;&gt;http://visionmedia.github.io/expresso/&amp;quot;&lt;/a&gt;) Expresso
    |  test framework. It fits my needs and doesn&amp;rsquo;t bother me with complex configuration. So i&amp;rsquo;m using it, but you&amp;rsquo;re free to use any other test framework.
    | The Travis-CI runs &amp;ldquo;npm test&amp;rdquo; command, which in fact execute.
    pre &amp;ldquo;expresso test/*&amp;rdquo;&lt;/p&gt;

&lt;p&gt;p
    | Expresso runs all JavaScript tests in the specified folder and returns the status code to Travis-CI.
    | If any test fails the Travis-CI job will be marked as failed and you will get a &amp;ldquo;Build Failing&amp;rdquo; notification.&lt;/p&gt;

&lt;p&gt;h2 Enabling Travis-CI for your repository&lt;/p&gt;

&lt;p&gt;p
    | When you&amp;rsquo;ve added manifest file to your repository it&amp;rsquo;s time to register on travis-ci.org website and add your repository there.
    | I&amp;rsquo;m not going to describe the registration process since it&amp;rsquo;s trivial.
    | But i want to point you to manifest validatio tool that can save a lof of your time:&lt;br /&gt;
    a(href=&amp;ldquo;&lt;a href=&#34;http://lint.travis-ci.org/&amp;quot;&#34;&gt;http://lint.travis-ci.org/&amp;quot;&lt;/a&gt;) Travis manifest validation tool
    |. This page helps you to check whether your manifest is correct.&lt;/p&gt;

&lt;p&gt;p
    | After you&amp;rsquo;ve added your project to Travis-CI, it will do the rest.
    | Each time you push changes to repository, Travis-CI will build your project and send you notification if the error occurs.&lt;/p&gt;

&lt;p&gt;h2 Adding Build Status to the README&lt;/p&gt;

&lt;p&gt;p
    | One of the cool features of travis is that you can view the build status from your github repository.
    | Open the README.md file and add the following lines.
    | Replace GITHUB_USERNAME with your github username and PROJECT_NAME with the name of your github project.
pre
   | &lt;a href=&#34;https://travis-ci.org/[GITHUB_USERNAME]/[PROJECT_NAME]&#34;&gt;&lt;img src=&#34;https://travis-ci.org/[GITHUB_USERNAME]/[PROJECT_NAME].png&#34; alt=&#34;Build Status&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;p
    | This will add the build status icon image to your readme page, so you will always know how healthy your project is.&lt;/p&gt;

&lt;p&gt;img.image-thumbnail(src=&amp;ldquo;build-status.png&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;h2 Conclusion&lt;/p&gt;

&lt;p&gt;p
    | As a developer and computer vision consultant, I work with many heterogenous software/hardware platforms. I&amp;rsquo;ve been using TeamCity, Jenkins and Team Foundation as a CI and build servers.
    | Although they provide much more features than Travis-CI does, no one can compete with Travis-CI if you want to have free, easy-to-use, no-headache continuous-integration service for your projects.
    | It will fits well for both web-based (php, ruby, python) application, tools and libraries in C++, Python, Scala and Java.&lt;/p&gt;

&lt;p&gt;h2 References
p
    ul
        li: a(href=&amp;ldquo;&lt;a href=&#34;https://travis-ci.org&amp;quot;&#34;&gt;https://travis-ci.org&amp;quot;&lt;/a&gt;) Travis-CI
        li: a(href=&amp;ldquo;&lt;a href=&#34;http://lint.travis-ci.org/&amp;quot;&#34;&gt;http://lint.travis-ci.org/&amp;quot;&lt;/a&gt;) Travis manifest validation tool
        li: a(href=&amp;ldquo;&lt;a href=&#34;http://visionmedia.github.io/expresso/&amp;quot;&#34;&gt;http://visionmedia.github.io/expresso/&amp;quot;&lt;/a&gt;) Expresso TDD for JavaScript
        li: a(href=&amp;ldquo;&lt;a href=&#34;http://computer-vision-talks.com/articles/2013-08-27-connecting-opencv-and-node-js-inside-cloud9-ide/&amp;quot;&#34;&gt;http://computer-vision-talks.com/articles/2013-08-27-connecting-opencv-and-node-js-inside-cloud9-ide/&amp;quot;&lt;/a&gt;) Connecting OpenCV and Node.js inside Cloud9 IDE
        li: a(href=&amp;ldquo;&lt;a href=&#34;http://computer-vision-talks.com/articles/2013-08-19-cloud-image-processing-using-opencv-and-node-js/&amp;quot;&#34;&gt;http://computer-vision-talks.com/articles/2013-08-19-cloud-image-processing-using-opencv-and-node-js/&amp;quot;&lt;/a&gt;) Cloud image processing using OpenCV and Node.js
br
p.lead
    | I hope you enjoyed reading this post. Please, describe your experience using Travis-CI in the comments. Thank you!.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenCV Tutorial Part 7</title>
      <link>https://example.com/2012-10-22-opencv-tutorial-part-7/</link>
      <pubDate>Mon, 22 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2012-10-22-opencv-tutorial-part-7/</guid>
      <description>

&lt;p&gt;After a long delay i&amp;rsquo;m happy to resume posting OpenCV tutorials in my blog. In Part 7 i will present you a new way of generation of icons for samples. Also i&amp;rsquo;ll show how to use NEON and assembly language to speed-up cv::transform function twice! Also there are three new samples i have to say few words about each.&lt;/p&gt;

&lt;h2 id=&#34;interface-improvements&#34;&gt;Interface improvements&lt;/h2&gt;

&lt;h3 id=&#34;default-sample-icons&#34;&gt;Default sample icons&lt;/h3&gt;

&lt;p&gt;I think each sample has to have it&amp;rsquo;s own unique icon image. Unfortunately, i&amp;rsquo;m not a cool graphic designer and i&amp;rsquo;m lazy. And i found brillant solution how to create unique icon for almost any sample with minimal efforts. Our sample will generate it for us! It&amp;rsquo;s so easy, right? We write a new sample that implements some cool effect. So it would be great if it&amp;rsquo;s icon will inform user about it. To do this we take a default image we use for icons and pass it through our sample. Result image is the best visual demonstration we can ever imagine. Let&amp;rsquo;s take a look on result of processing default icon with Cartoon Filter Sample: &lt;strong&gt;Default Image&lt;/strong&gt;: &lt;img src=&#34;IMG_0044.png&#34; alt=&#34;&#34; title=&#34;Mandrill (Original)&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;generated-cartoon-filter-icon&#34;&gt;&lt;strong&gt;Generated Cartoon Filter Icon&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;IMG_0045.jpg&#34; alt=&#34;&#34; title=&#34;Mandrill (Cartoon)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Also, these icons are now used in the rest of UI. The final picture looks much more user-friendly:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Screen-Shot-2012-08-19-at-9.05.33-PM-1024x813.png&#34; alt=&#34;&#34; title=&#34;OpenCV Tutorial Part 7 Sample Icons&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;in-case-you-looking-for-a-code&#34;&gt;In case you looking for a code&lt;/h3&gt;

&lt;p&gt;While working on this chapter i noticed that UI-realted code has a lot of duplicates connected with converting of std::string to NSString, loading UIImage objects from bundle. To remove such duplicates i added a special facade class which performs all conversion and implements sample icon generation as described above.&lt;/p&gt;

&lt;h4 id=&#34;samplefacade-interface&#34;&gt;&lt;strong&gt;SampleFacade interface&lt;/strong&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;@interface SampleFacade : NSObject

- (id) initWithSample:(SampleBase*) sample;

@property (readonly) SampleBase * sample;

- (NSString *) title;
- (NSString *) description;
- (NSString *) friendlyName;

- (UIImage*)   smallIcon;
- (UIImage*)   largeIcon;

- (bool) processFrame:(const cv::Mat&amp;amp;) inputFrame into:(cv::Mat&amp;amp;) outputFrame;
- (UIImage*) processFrame:(UIImage*) source;

@end
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;icons-generation&#34;&gt;Icons generation&lt;/h4&gt;

&lt;p&gt;When the data table is populated with a sample list it creates a cell for each sample. A cell contains a sample thumbnail image queried from [SampleFacade smallIcon].&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (UIImage*)   smallIcon
{
    if (!m_smallIcon)
    {
        if (self.sample-&amp;gt;;hasIcon())
        {
            NSString * iconStr = [NSString stringWithStdString:self.sample-&amp;gt;;getSampleIcon()];
            m_smallIcon = [[UIImage imageNamed:iconStr] thumbnailWithSize:80];
        }
        else
        {
            UIImage * srcImage = [UIImage imageNamed:@&amp;quot;DefaultSampleIcon.png&amp;quot;];
            m_smallIcon = [self processFrame:[srcImage thumbnailWithSize:80]];
        }
    }

    return m_smallIcon;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, this method of generating sample icon image cannot be applied to any sample. It&amp;rsquo;s applicable only to samples that can process single image without any additional data input. For example, we can&amp;rsquo;t use this method to generate icon for video tracking, because it will give us exactly the same picture. But for samples that does manipulate with pixels it&amp;rsquo;s the ideal solution. ;&lt;/p&gt;

&lt;h3 id=&#34;more-friendly-ipad-interface&#34;&gt;More friendly iPad interface&lt;/h3&gt;

&lt;p&gt;When in landscape mode iPad&amp;rsquo;s application interface shows list of samples and result of sample processing simultaneously. In the previous versions of the app, selecting new sample while previous was running had no effect. You had go back to sample information window and then click again &amp;ldquo;run&amp;rdquo; to start using new sample. Now it&amp;rsquo;s fixed - you can switch to any sample any time you want. Watch this great demonstration video:  Implementation was very trivial - when user taps &amp;ldquo;Run on Video&amp;rdquo; or &amp;ldquo;Run on Image&amp;rdquo; button we save new view controller in DetailViewController&amp;rsquo;s private property:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (void)prepareForSegue:(UIStoryboardSegue *)segue sender:(id)sender
{
    if ([[segue identifier] isEqualToString:@&amp;quot;processVideo&amp;quot;])
    {
        VideoViewController * sampleController = [segue destinationViewController];
        [sampleController setSample:currentSample];
        self.activeVideoController = sampleController;
    }
    else if ([[segue identifier] isEqualToString:@&amp;quot;processImage&amp;quot;])
    {
        ImageViewController * sampleController = [segue destinationViewController];
        [sampleController setSample:currentSample];
        self.activeImageController = sampleController;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And modified configureView function now updates active image or video view if it&amp;rsquo;s not null:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (void)configureView
{
    // Update the user interface for the detail item.

    if (currentSample)
    {
        self.sampleDescriptionTextView.text = [currentSample description];
        self.title = [currentSample title];
        self.sampleIconView.image = [currentSample largeIcon];

        if (self.activeImageController)
            [self.activeImageController setSample:currentSample];

        if (self.activeVideoController)
            [self.activeVideoController setSample:currentSample];
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;performance-optimization&#34;&gt;Performance optimization&lt;/h2&gt;

&lt;h3 id=&#34;optimizing-cv-transform-with-arm-neon&#34;&gt;Optimizing cv::transform with ARM NEON&lt;/h3&gt;

&lt;p&gt;Assembly language and architecture-specific code was always a subject of my special attention. After i got my first iPhone i started learning ARM Assembly and it&amp;rsquo;s SIMD engine called NEON. As a result of my first attempts to write something useful i wrote &lt;a href=&#34;https://example.com/articles/2011-02-08-a-very-fast-bgra-to-grayscale-conversion-on-iphone/&#34; title=&#34;A very fast BGRA to Grayscale conversion on Iphone&#34;&gt;fast BGRA to Grayscale color conversion&lt;/a&gt; function. It was a long time ago, but this function is still actual. NEON-accelerated BGRA to Grayscale conversion is being used in this project too. In this section i will show you how to improve performance of the cv::transform function. Linear transform is useful function. In our samples we use it for Sepia effect for example. Also it can perform BGRA to Gray conversion without reducing number of image channels, adjust contrast and swap channels. A brief theory if you forgot what this function does. A cv::transform function multiplies each image pixel on 4x4 matrix and puts resulting vector to destination image. Input pixel is vector of 4 elements (unsigned bytes), each element contains channel intensity in following order: B, G, R, A. The matrix is represented by a 4x4 floating point (single precision) array. Our goal is to rewrite multiplication of 4x4 matrix on 4-element Vector. First, a wrapping function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;namespace cv 
{
  void neon_transform_bgra(const cv::Mat&amp;amp; input, cv::Mat&amp;amp; result, const cv::Mat_&amp;amp; m)
  {
    assert(input.type() == CV_8UC4);
    
    if (result.rows != input.rows || result.cols != input.cols || result.type() != CV_8UC4)
    {
      result.create(input.rows, input.cols, CV_8UC4);
    }
    //result = input.clone();
    //initSameSizeAlignedIfNecessary(m, result);
    
    //cv::Mat trans;
    //cv::transpose(m, trans);
    
    float * matrix = reinterpret_cast&amp;lt;float*&amp;gt;(m.data);
    
    int v[4];
    int out[4];
    
    for (int row = 0; row &amp;lt; input.rows; row++)
    {
      cv::Vec4b * srcRow = reinterpret_cast&amp;lt;cv::Vec4b*&amp;gt;(input.row(row).data);
      cv::Vec4b * dstRow = reinterpret_cast&amp;lt;cv::Vec4b*&amp;gt;(result.row(row).data);
      
      for (int col = 0; col &amp;lt; input.cols; col++)
      {
        const cv::Vec4b&amp;amp; src = srcRow[col];
        cv:Vec4b&amp;amp; dst        = dstRow[col];
        
        v[0] =  src[0];
        v[1] =  src[1];
        v[2] =  src[2];
        v[3] =  src[3];
        
        neon_asm_mat4_vec4_mul(matrix, v, out);
        
        dst[0] = cv::saturate_cast&amp;lt;unsigned char&amp;gt;(out[0]);
        dst[1] = cv::saturate_cast&amp;lt;unsigned char&amp;gt;(out[1]);
        dst[2] = cv::saturate_cast&amp;lt;unsigned char&amp;gt;(out[2]);
        dst[3] = cv::saturate_cast&amp;lt;unsigned char&amp;gt;(out[3]);
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A battle of three descriptors: SURF, FREAK and BRISK</title>
      <link>https://example.com/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/</link>
      <pubDate>Sat, 18 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/</guid>
      <description>

&lt;p&gt;I think developers and research guys who works with object recognition, image registration and other areas that uses keypoint extraction can find this post useful. Recently (from 2.4.2) a new feature descriptor algorithm was added to OpenCV library. FREAK descriptor is claimed to be superior to ORB and SURF descriptors, yet it&amp;rsquo;s very fast (comparable to ORB). Also people in comments on my blog mentioned BRISK descriptor which is also new and more efficient than SURF. Well, finally i find a time to compare them and publish my research results.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This post will be very similar to &lt;a href=&#34;http://computer-vision-talks.com/2011/08/feature-descriptor-comparison-report/&#34; title=&#34;Feature descriptor comparison report&#34;&gt;OpenCV comparison reports&lt;/a&gt; i made in past. Although those reports were published years ago, they are still somewhat actual. For this test i decided to rewrite the whole testing framework from scratch. The source code will be available soon. But for now, let me explain what i did to find a best of three algorithms. What is main goal of converting image to descriptors? Move from pixel domain to more compact form of representation the same data. In addition we would like our representation be rotation and scale invariant (e.g representation remains the same or changes slightly when source image rotated or scaled). SURF, FREAK and BRISK descriptors claims they are rotation and scale invariant.&lt;/p&gt;

&lt;h2 id=&#34;transformations&#34;&gt;Transformations&lt;/h2&gt;

&lt;p&gt;Like in the OpenCV comparison report, test application works with test pattern image. And we have four basic transformations: rotation, scale, blur and brightness adjustment. Here how the rotation transformation class looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;class ImageRotationTransformation : public ImageTransformation
{
public:
    ImageRotationTransformation(float startAngleInDeg, float endAngleInDeg, float step, cv::Point2f rotationCenterInUnitSpace)
    : ImageTransformation(&amp;quot;Rotation&amp;quot;)
    , m_startAngleInDeg(startAngleInDeg)
    , m_endAngleInDeg(endAngleInDeg)
    , m_step(step)
    , m_rotationCenterInUnitSpace(rotationCenterInUnitSpace)
    {
        // Fill the arguments
        for (float arg = startAngleInDeg; arg &amp;lt; = endAngleInDeg; arg += step)
            m_args.push_back(arg);
    }

    virtual std::vector getX() const
    {
        return m_args;
    }

    virtual void transform(float t, const cv::Mat&amp;amp; source, cv::Mat&amp;amp; result) const
    {
        cv::Point2f center(source.cols * m_rotationCenterInUnitSpace.x, source.cols * m_rotationCenterInUnitSpace.y);
        cv::Mat rotationMat = cv::getRotationMatrix2D(center, t, 1);
        cv::warpAffine(source, result, rotationMat, source.size());
    }

private:
    float m_startAngleInDeg;
    float m_endAngleInDeg;
    float m_step;

    cv::Point2f m_rotationCenterInUnitSpace;

    std::vector m_args;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Other types of transformations looks similar. But it shows the idea.&lt;/p&gt;

&lt;h2 id=&#34;featurealgorithm&#34;&gt;FeatureAlgorithm&lt;/h2&gt;

&lt;p&gt;As you may know we need three components when dealing with descriptors:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Feature detector - class derived from cv::FeatureDetector class that implements particular detection algorithm. For example, cv::SurfFeatureDetector implements detection algorithm described in SURF paper.&lt;/li&gt;
&lt;li&gt;Descriptor extractor - class derived from cv::DescriptorExtractor. It computes descriptors from passed keypoints. cv::SurfDescriptorExtractor will computer SURF descriptors.&lt;/li&gt;
&lt;li&gt;Descriptor matcher - An instance of cv::BFMatcher of cv::FlannBasedMatcher classes is used to match two sets of descriptors.
We store these three objects in FeatureAlgorithm class:
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;class FeatureAlgorithm
{
public:
    FeatureAlgorithm(std::string name, cv::FeatureDetector* d, cv::DescriptorExtractor* e, cv::DescriptorMatcher* m);

    std::string name;

    bool knMatchSupported;

    bool extractFeatures(const cv::Mat&amp;amp; image, Keypoints&amp;amp; kp, Descriptors&amp;amp; desc) const;

    void matchFeatures(const Descriptors&amp;amp; train, const Descriptors&amp;amp; query, Matches&amp;amp; matches) const;
    void matchFeatures(const Descriptors&amp;amp; train, const Descriptors&amp;amp; query, int k, std::vector&amp;amp; matches) const;

private:
    cv::FeatureDetector*     detector;
    cv::DescriptorExtractor* extractor;
    cv::DescriptorMatcher*   matcher;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;test-routine&#34;&gt;Test routine&lt;/h2&gt;

&lt;p&gt;The main test function takes FeatureAlgorithm, Transformation and test image. As output we return list of matching statistics for each run. Here is a brief sequence:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Convert input image to grayscale&lt;/li&gt;
&lt;li&gt;Detect keypoints and extract descriptors from input grayscale image&lt;/li&gt;
&lt;li&gt;Generate all transformed images using passed transformation algorithm&lt;/li&gt;
&lt;li&gt;For each of the transformed image:

&lt;ul&gt;
&lt;li&gt;Detect keypoints and extract descriptors&lt;/li&gt;
&lt;li&gt;Match train descriptors and query&lt;/li&gt;
&lt;li&gt;Split matches to inliers and outliers using homography estimation&lt;/li&gt;
&lt;li&gt;Compute statistics (consumed time, total percent of matches, percent of correct matches, etc)
The main cycle is paralleled using OpenMP, on my Quad Core i5 it loads all cores for 100% while doing test. Feature Algoritms:
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;algorithms.push_back(FeatureAlgorithm(&amp;quot;SURF/BRISK/BF&amp;quot;,
        new cv::SurfFeatureDetector(),
        new cv::BriskDescriptorExtractor(),
        new cv::BFMatcher(cv::NORM_HAMMING, true)));

algorithms.push_back(FeatureAlgorithm(&amp;quot;SURF/FREAK/BF&amp;quot;,
        new cv::SurfFeatureDetector(),
        new cv::FREAK(),
        new cv::BFMatcher(cv::NORM_HAMMING, true)));

algorithms.push_back(FeatureAlgorithm(&amp;quot;SURF/SURF/BF&amp;quot;,
        new cv::SurfFeatureDetector(),
        new cv::SurfDescriptorExtractor(),
        new cv::BFMatcher(cv::NORM_L2, true)));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Image transformations:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;transformations.push_back(new GaussianBlurTransform(9));
transformations.push_back(new BrightnessImageTransform(-127, +127, 10));
transformations.push_back(new ImageRotationTransformation(0, 360, 10, cv::Point2f(0.5f,0.5f)));
transformations.push_back(new ImageScalingTransformation(0.25f, 2.0f, 0.1f));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&#34;metrics&#34;&gt;Metrics&lt;/h2&gt;

&lt;p&gt;The following metrics are calculated:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Percent of matches&lt;/strong&gt; - quotient of dividing matches count on the minimum of keypoints count on two frames in percents.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Percent of correct matches&lt;/strong&gt; - quotient of dividing correct matches count on total matches count in percents.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Matching ratio&lt;/strong&gt; - percent of matches * percent of correct matches.
In all charts i will use &amp;ldquo;Matching ratio&amp;rdquo; ( in percents) value for Y-axis.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;statistics&#34;&gt;Statistics&lt;/h2&gt;

&lt;p&gt;After running all tests we collect statistics for each transformation and algorithm. The report table for particular transformation algorithm looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Argument SURF/BRISK/BF    SURF/FREAK/BF    SURF/SURF/BF
       1           100          88.5965         82.6752
       2           100          86.9608         79.1689
       3           100          85.6069         70.6731
       4           100          85.0897         64.9057
       5           100          83.1528         59.4776
       6           100          85.1648         58.9763
       7           100          88.6447         59.3066
       8           100          94.9109         64.8019
       9           100          95.9707         69.1154
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To make a graphic representation i used Google Spreadsheets to import CSV tables and generate charts. You can find this spreadsheet here: &lt;a href=&#34;https://docs.google.com/spreadsheet/ccc?key=0AuBBvmQlA4pfdGtPRHM5alBkQUowZEVBNlFrZ1dIa0E#gid=2&#34;&gt;OpenCV 2.4.9 Features Comparison Report&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;chart_6.png&#34; alt=&#34;&#34; title=&#34;SURF vs FREAK vs BRISK descriptors comparison (rotation)&#34; /&gt; &lt;img src=&#34;chart_6-1.png&#34; alt=&#34;&#34; title=&#34;SURF vs FREAK vs BRISK descriptors comparison (scale)&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenCV Tutorial - Part 6</title>
      <link>https://example.com/2012-07-22-opencv-tutorial-part-6/</link>
      <pubDate>Sun, 22 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2012-07-22-opencv-tutorial-part-6/</guid>
      <description>

&lt;p&gt;[toc] Hi folks! I’m glad to publish a sixth part of the OpenCV Tutorial cycle. In this post I will describe how to implement interesting non-photorealistic effect that makes image looks like a cartoon. It has numerous names: cartoon filter or simply “toon” also it known as rotoscoping. In addition we will refactor application interface and add tweeting feature to share your results across the web. According to the roadmap I promised to put the video recording module too, but due to lack of free time I decided to put it on hold for now. To compensate this in this part I will demonstrate how to get Sepia effect using simple matrix multiplication. Don’t afraid, video recording will be added, but later. I think after part 9, when most of the samples will be optimized using ARM NEON I will add this feature.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&#34;interface-improvements&#34;&gt;Interface improvements&lt;/h2&gt;

&lt;p&gt;I will never get tired to repeat that user experience is a top 1 priority for mobile apps. As a developer you have to think about users in the first order. Will the user be satisfied with your app or deletes it after using for 30 seconds – this depends only from you. I was unsatisfied with previous interface. Especially with those unintuitive icons and text labels. Many thanks for free icon packs where I got new toolbar icons for my app. With new interface I tried to make image and video views looks similar. The left toolbar button for image view responsible for selecting a photo; in the video mode I put a button that switches between front and back cameras to this position. The central place on a toolbar is taken by “options” button that shows the options for selected sample view (iPhone interface). The right button is a special action button that presents a list of action you can do: save image to album, tweet it or do anything else. Let’s take a look how our interface did evolved:&lt;/p&gt;

&lt;h3 id=&#34;iphone-interface-improvements&#34;&gt;iPhone interface improvements&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;Screen-Shot-2012-07-23-at-3.12.49-PM.png&#34; alt=&#34;&#34; title=&#34;Cartoon filter using Machine Eye&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;ipad-interface-improvements&#34;&gt;iPad interface improvements&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;Screen-Shot-2012-07-23-at-3.14.26-PM-1024x815.png&#34; alt=&#34;&#34; title=&#34;Sepia effect with Machine Eye&#34; /&gt; To eliminate duplicate code for two processing modes (image and video) we introduce a base class to store common data for derived views:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;typedef void (^TweetImageCompletionHandler)(); 
typedef void (^SaveImageCompletionHandler)(); 

#define kSaveImageActionTitle  @&amp;quot;Save image&amp;quot;
#define kComposeTweetWithImage @&amp;quot;Tweet image&amp;quot;

@interface BaseSampleViewController : UIViewController

@property (readonly) SampleBase * currentSample;

- (void) configureView;
- (void) setSample:(SampleBase*) sample;
- (void) tweetImage:(UIImage*) image withCompletionHandler: (TweetImageCompletionHandler) handler;
- (void) saveImage:(UIImage*) image  withCompletionHandler: (SaveImageCompletionHandler) handler;

@end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will consider implementation of the tweetImage and saveImage function a little bit later. &lt;strong&gt;BaseSampleViewController&lt;/strong&gt; has a public readonly property to current sample, and a special &lt;strong&gt;setSample&lt;/strong&gt; function to change it. When a new sample is assigned it calls configureView to update the view title and perform other actions.&lt;/p&gt;

&lt;h3 id=&#34;action-sheet-easy-way-to-perform-typical-actions&#34;&gt;Action sheet – easy way to perform typical actions&lt;/h3&gt;

&lt;p&gt;You could notice that I’ve removed “Save” button from the toolbar. The main reason for this – I wanted to avoid adding “Tweet”, “Share”, “Save and email” buttons. All these actions should be grouped somewhere else. The ideal solution is to use &lt;strong&gt;UIActionSheet&lt;/strong&gt; control that will present available actions. Using this class is very easy: First, you need to instantiate action sheet with a list of available actions. For our case it will looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;self.actionSheet = [[UIActionSheet alloc] initWithTitle:@&amp;quot;Actions&amp;quot; 
                                               delegate:self 
                                      cancelButtonTitle:@&amp;quot;Cancel&amp;quot; 
                                 destructiveButtonTitle:nil 
                                      otherButtonTitles:kSaveImageActionTitle, 
                                      kComposeTweetWithImage, nil];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;String constants kSaveImageActionTitle and kComposeTweetWithImage has corresponding values &amp;ldquo;Save image&amp;rdquo; and &amp;ldquo;Tweet image&amp;rdquo;. The key step when we instantiate action sheet - to specify a delegate that will handle interaction with this action sheet. To implement &lt;strong&gt;UIActionSheetDelegate&lt;/strong&gt; protocol you must provide implementation at least for &lt;strong&gt;actionSheet:clickedButtonAtIndex:&lt;/strong&gt; method:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (void)actionSheet:(UIActionSheet *)senderSheet clickedButtonAtIndex:(NSInteger)buttonIndex
{
  NSString * title = [senderSheet buttonTitleAtIndex:buttonIndex];

  if (title == kSaveImageActionTitle)
  {
    [self saveImage:self.imageView.image withCompletionHandler:nil];
  }
  else if (title == kComposeTweetWithImage)
  {
    [self tweetImage:self.imageView.image withCompletionHandler:nil];
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s imagine we are running a video processing and decide to save image. We press actions button on a toolbar then tap on &amp;ldquo;Save image&amp;rdquo; button, but the video source continue generating new frames. Expected behavior in this case is to suspend processing. To do this we pause video source when action sheet is shown and resume when it dismissed. &lt;strong&gt;UIActionSheetDelegate&lt;/strong&gt; provides a method &lt;strong&gt;willPresentActionSheet&lt;/strong&gt; called before action sheet will appear:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (void)willPresentActionSheet:(UIActionSheet *)actionSheet;  // before animation and showing view
{
  [videoSource stopRunning];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we pause video source when action sheet appears. To resume processing frames we will use &lt;strong&gt;actionSheet:clickedButtonAtIndex&lt;/strong&gt; method. If clicked button index does not corresponds to known action we simply resume video source because action sheet will be dismissed soon. But if user ask to tweet or save image, we invoke corresponding method and resume video source only after this action is complete. Since tweeting involves presenting tweet compose view it&amp;rsquo;s reasonable to pause all processing and continue when the all work is done.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (void)actionSheet:(UIActionSheet *)senderSheet clickedButtonAtIndex:(NSInteger)buttonIndex
{
  NSString * title = [senderSheet buttonTitleAtIndex:buttonIndex];

  if (title == kSaveImageActionTitle)
  {
    UIImage * image = [UIImage imageWithMat:outputFrame.clone() andDeviceOrientation:[[UIDevice currentDevice] orientation]];
    [self saveImage:image withCompletionHandler: ^{ [videoSource startRunning]; }];
  }
  else if (title == kComposeTweetWithImage)
  {
    UIImage * image = [UIImage imageWithMat:outputFrame.clone() andDeviceOrientation:[[UIDevice currentDevice] orientation]];
    [self tweetImage:image withCompletionHandler:^{ [videoSource startRunning]; }];
  }
  else
  {
    [videoSource startRunning];
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sepia-negative-and-other-simple-effects&#34;&gt;Sepia, negative and other simple effects&lt;/h2&gt;

&lt;h3 id=&#34;brightness-and-contrast-adjustments&#34;&gt;Brightness and contrast adjustments&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>OpenCV Tutorial - Part 5</title>
      <link>https://example.com/2012-07-14-opencv-tutorial-part-5/</link>
      <pubDate>Sat, 14 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2012-07-14-opencv-tutorial-part-5/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;Screen-Shot-2012-07-11-at-12.12.42-AM.png&#34; alt=&#34;&#34; title=&#34;OpenCV Tutorial Options&#34; /&gt; Hello readers! The fifth part of the OpenCV Tutorial is here! In this post we will add options pane for our samples. In the end of this chapter our application will receive options interface as shown on screenshot. But first, let me remind you (if you came here for the first time) what is happening here. The &amp;ldquo;OpenCV Tutorial&amp;rdquo; is a open-source project maintained by me (Eugene Khvedchenya). My goal - create a  iPhone/iPad application to demonstrate various image processing algorithms of OpenCV library and how to use them in iOS applications. Please check previous parts if you haven&amp;rsquo;t done this yet:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://computer-vision-talks.com/2012/06/opencv-tutorial-a-collection-of-opencv-samples-for-iphoneipad-part-1/&#34; title=&#34;OpenCV Tutorial – a collection of OpenCV samples for iPhone/iPad – Part 1&#34;&gt;Part 1 - Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://computer-vision-talks.com/2012/06/opencv-tutorial-part-2/&#34; title=&#34;OpenCV Tutorial – Part 2&#34;&gt;Part 2 - Writing a base UI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://computer-vision-talks.com/2012/06/opencv-tutorial-part-3/&#34; title=&#34;OpenCV Tutorial – Part 3&#34;&gt;Part 3 - Video and image processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://computer-vision-talks.com/2012/07/opencv-tutorial-part-4/&#34; title=&#34;OpenCV Tutorial – Part 4&#34;&gt;Part 4 - Correction of mistakes&lt;/a&gt;
Also, there is great &lt;a href=&#34;http://computer-vision-talks.com/opencv-tutorial-roadmap/&#34; title=&#34;OpenCV Tutorial roadmap&#34;&gt;OpenCV Tutorial roadmap&lt;/a&gt;. I&amp;rsquo;m trying to follow it. As usually, all source code can be found on a GitHub: &lt;a href=&#34;https://github.com/BloodAxe/OpenCV-Tutorial&#34;&gt;OpenCV Tutorial&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&#34;sample-options&#34;&gt;Sample options&lt;/h2&gt;

&lt;p&gt;To expose algorithm options to user code we add a bunch of &amp;ldquo;registerOption&amp;rdquo; functions that allows to register properties. Registered properties can be changed in future using user interface. Our application will support four property types: boolean, integer, float and string enum types.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Boolean&lt;/strong&gt; - this option can be used as a switch to enable or disable particular sample features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integer&lt;/strong&gt; - this option allows adjust any kind of thresholds of integer type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Float&lt;/strong&gt; - this option allows adjust any kind of thresholds of real type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;String enum&lt;/strong&gt; - this option is ideal choice if you want to select one of available options.
Let&amp;rsquo;s take a look to SampleBase::registerOption functions. Each overload allows to register option of each supported property type. Overloaded functions of Integer and Float types also allows you to specify minimum and maximum allowed values. Each option has it&amp;rsquo;s unique name, section that allows to group options to logical sections and pointer to property value.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;//! Base class for all samples
class SampleBase
{
public:
  ...

  const OptionsMap&amp;amp; getOptions() const;

protected:
  void registerOption(std::string name, std::string section, bool  * value);
  void registerOption(std::string name, std::string section, int   *  value, int min, int max);
  void registerOption(std::string name, std::string section, float *  value, float min, float max);
  void registerOption(std::string name, std::string section, std::string* value, std::vector stringEnums, int defaultValue = 0);

private:
  OptionsMap m_optionsWithSections;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Typical use of these functions is shown in EdgeDetectionSample constructor. This this method we register available edge detection algorithm (String Enum), algorithm parameters (Integer and Real types), a switch to control how the output image looks - edges only or blended one (Boolean).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;EdgeDetectionSample::EdgeDetectionSample()
: m_showOnlyEdges(true)
, m_algorithmName(&amp;quot;Canny&amp;quot;)
, m_cannyLoThreshold(50)
, m_cannyHiThreshold(150)
, m_harrisBlockSize(2)
, m_harrisapertureSize(3)
, m_harrisK(0.04f)
, m_harrisThreshold(200)
{
  std::vector algos;
  algos.push_back(&amp;quot;Canny&amp;quot;);
  algos.push_back(&amp;quot;Sobel&amp;quot;);
  algos.push_back(&amp;quot;Schaar&amp;quot;);

  registerOption(&amp;quot;Algorithm&amp;quot;,       &amp;quot;&amp;quot;, &amp;amp;m;_algorithmName, algos);  
  registerOption(&amp;quot;Show only edges&amp;quot;, &amp;quot;&amp;quot;, &amp;amp;m;_showOnlyEdges);

  // Canny detector options
  registerOption(&amp;quot;Threshold 1&amp;quot;, &amp;quot;Canny&amp;quot;, &amp;amp;m;_cannyLoThreshold, 0, 100); 
  registerOption(&amp;quot;Threshold 2&amp;quot;, &amp;quot;Canny&amp;quot;, &amp;amp;m;_cannyHiThreshold, 0, 200); 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All properties are stored in the m_optionsWithSections class field. There is a potential memory leak, because options are not deleted, but since each sample is created only once and deleted only when application exits we leave this as is. To access registered properties we expose public function getOptions().&lt;/p&gt;

&lt;h2 id=&#34;presenting-options-in-ui&#34;&gt;Presenting options in UI&lt;/h2&gt;

&lt;p&gt;To present registered options we will use UITableView class, where each table row corresponds to single option. Filling table view with dynamic data a little bit tricky. We have to complete few steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Implement UITableViewDelegate and UITableViewDataSource interfaces and connect them to our table. The first protocol provides a data four out table from the registered options. The second protocol modifies default table behavior as we wants to.&lt;/li&gt;
&lt;li&gt;For each property type we have to create a custom cell view that allows modifying underlying value. During data binding we will be responsible to create a view that corresponds to property type.&lt;/li&gt;
&lt;li&gt;When the property is changed from the user interface we have to inform our view controller about this via callback.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s start from creating a custom cells.&lt;/p&gt;

&lt;h3 id=&#34;optioncell&#34;&gt;OptionCell&lt;/h3&gt;

&lt;p&gt;Each cell has a notification delegate used to inform a user code when algorithm property has been changed. Also there will be a readonly property cellHeight that define a designed cell height:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;@protocol OptionCellDelegate 

- (void) optionDidChanged:(SampleOption*) option;

@end 

@interface OptionCell : UITableViewCell

@property (readonly) float cellHeight;
@property id delegate;

@end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create a custom cell we subclass from OptionCell type and add corresponding XIB file that contains it&amp;rsquo;s interface. We also assigne the reusable identifier for each cell type. Quote from &lt;a href=&#34;http://developer.apple.com/library/ios/#DOCUMENTATION/UIKit/Reference/UITableView_Class/Reference/Reference.html#//apple_ref/occ/instm/UITableView/dequeueReusableCellWithIdentifier&#34; title=&#34;UITableView Class Reference&#34;&gt;UITableView Class Reference&lt;/a&gt;:&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenCV Tutorial - Part 4</title>
      <link>https://example.com/2012-07-07-opencv-tutorial-part-4/</link>
      <pubDate>Sat, 07 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2012-07-07-opencv-tutorial-part-4/</guid>
      <description>

&lt;p&gt;This is the fourth part of the OpenCV Tutorial. In this part the solution of the annoying iOS video capture orientation bug will be described. Of course that&amp;rsquo;s not all. There are some new features - we will add processing of saved photos from your photo album. Also to introduce minor interface improvements and I&amp;rsquo;ll show you how to disable unsupported API like video capture in your app and run in on iOS Simulator.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&#34;startup-images&#34;&gt;Startup images&lt;/h2&gt;

&lt;p&gt;  First of all, i would like to say a great thanks for my friend who made nice startup images for this project. I&amp;rsquo;m really loving them, good graphics make application looks like a pro. Thanks Eugene. By the way, if you looking for free-lance graphic designer - feel free to contact him anytime. So i got a bunch of startup images. With regards to iOS guidelines, to fulfill all possible cases you need 6 different images - two for iPhone (for old one and for retina display) and four for iPad family (in portrait and landscape orientation for both retina and non-retina displays). Assigning them to XCode project was a trivial task - just drag and drop them to project options:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Screen-Shot-2012-07-07-at-10.56.47-AM.png&#34; alt=&#34;&#34; title=&#34;OpenCV Tutorial icons&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;device-interface-and-video-orientation&#34;&gt;Device, Interface and Video orientation&lt;/h2&gt;

&lt;p&gt;In the part 3 we created video capture class that use AVFoundation to get raw data from camera capture input. To present our images we introduced GLESImageView class. The motivation was to have fast rendering of bitmaps. But we experienced a problem with video orientation. If we were using AVVideoCapturePreviewLayer then iOS API take care about video/device/interface orientation by itself. But since our choice to use low-level capture API it&amp;rsquo;s our headache. Fortunately, we have to do only one thing - apply the correct rotation for our texture with image with regards to the interface orientation. There is a slight difference between device orientation and interface orientation. The device orientation refers to physical orientation of your device in the world, while interface orientation refers to orientation of UI controls on the screen. To get the current interface orientation you will need an instance of UIViewController object that holds our GLESImageView. We can access interface orientation like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;UIInterfaceOrientation uiOrientation = [viewController interfaceOrientation];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To follow SRP we put orientation handling code inside to GLESImageView class. To access the parent view controller from any view you can use following snippet code that i found somewhere on stackoverflow.com: &lt;strong&gt;GLESImageView.mm&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (UIViewController *)viewController
{
  UIResponder *responder = self;
  while (![responder isKindOfClass:[UIViewController class]])
  {
  responder = [responder nextResponder];
  if (nil == responder)
  {
    break;
  }
  }

  return (UIViewController *)responder;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;camera-frame-orientation&#34;&gt;Camera frame orientation&lt;/h2&gt;

&lt;p&gt;There is another one kind of orientation - AVCaptureVideoOrientation type. This enum defines the physical orientation of the images captured with particular capture device. The video orientation differs for front and rear cameras. Here is a proof link - &lt;a href=&#34;http://developer.apple.com/library/ios/#qa/qa1744/_index.html#//apple_ref/doc/uid/DTS40011134&#34;&gt;Technical Q&amp;amp;A QA1744&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The iPod touch, iPhone 4 and iPad 2 front facing camera is mounted AVCaptureVideoOrientationLandscapeLeft, and the back-facing camera is mounted AVCaptureVideoOrientationLandscapeRight.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Also i draw your attention that AVCaptureVideoDataOutput does not support setting video orientation using API:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Currently, the capture outputs for a movie file (AVCaptureMovieFileOutput) and still image (AVCaptureStillImageOutput) support setting the orientation, but the data output for processing video frames (AVCaptureVideoDataOutput) does not.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Well, it&amp;rsquo;s not so bat, actually. To handle different orientation of rear and front camera we can use simple flip operation. This brings AVCaptureVideoOrientationLandscapeLeft to AVCaptureVideoOrientationLandscapeRight. We&amp;rsquo;ll use this video orientation as a standard orientation: &lt;strong&gt;VideoSource.mm&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (void)captureOutput:(AVCaptureOutput *)captureOutput 
didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer 
       fromConnection:(AVCaptureConnection *)connection 
{ 
  ... 

  cv::Mat frame(height, width, CV_8UC4, (void*)baseAddress, stride);

  if ([self videoOrientation] == AVCaptureVideoOrientationLandscapeLeft)
  {
    cv::flip(frame, frame, 0);
  }

  ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is four possible interface orientations - Portrait, PortraitUpsideDown, LandscapeLeft, LandscapeRight. So we can define four sets of texture coordinates that does correct visualization of our bitmap. At every frame we choose right set of texture coordinates with regards to interface orientation. Here is a code that i use: &lt;strong&gt;GLESImageView.mm&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- (void)drawFrame:(const cv::Mat&amp;amp;) bgraFrame
{
  ...

  GLfloat * textureVertices;
  static GLfloat textureVerticesPortrait[] =
  {
    1, 1,   1, 0,
    0, 1,   0, 0
  };  

  static GLfloat textureVerticesPortraitUpsideDown[] =
  {
    0, 0,   0, 1,
    1, 0,   1, 1
  };

  static GLfloat textureVerticesLandscapeLeft[] =
  {
    1, 0,   0, 0,
    1, 1,   0, 1
  };  

  static GLfloat textureVerticesLandscapeRight[] =
  {
    0, 1,   1, 1,
    0, 0,   1, 0
  }; 

  switch (uiOrientation)
  {
    case UIInterfaceOrientationPortrait:
      textureVertices = textureVerticesPortrait;
      break;

    case UIInterfaceOrientationPortraitUpsideDown:
      textureVertices = textureVerticesPortraitUpsideDown;
      break;

    case UIInterfaceOrientationLandscapeLeft:
      textureVertices = textureVerticesLandscapeLeft;
      break;

    case UIInterfaceOrientationLandscapeRight:
    default:
      textureVertices = textureVerticesLandscapeRight;
      break;
  };
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using this we select correct texture coordinates. The rest of drawing code left without changes. Great, now it seems that video looks correct at every orientation. Cool! Let&amp;rsquo;s move on.&lt;/p&gt;

&lt;h2 id=&#34;processing-saved-photos&#34;&gt;Processing saved photos&lt;/h2&gt;

&lt;p&gt;Although processing video frames in real-time is awesome by itself, i though that adding a possibility to process a saved picture is also worth to be implemented. All the more so it&amp;rsquo;s easier to implement than video processing. As usual, we start from creating a new ImageViewController class. Our view will contains UIImageView control to present processed result on the View. Like the VideoViewController, our class also requires a SampleBase object and input image to process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Screen-Shot-2012-07-07-at-11.14.25-AM.png&#34; alt=&#34;&#34; title=&#34;Edge detection of saved photo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is how it looks. Two buttons in the top right corner is action buttons - the &amp;ldquo;Save&amp;rdquo; button puts a processed image to a saved photos album, the second button with camera picture - allows you to select another one image for processing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ImageViewController.h&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;@interface ImageViewController : UIViewController 

- (void) setSample:(SampleBase*) sample;
- (void) setImage:(UIImage*) image;

@property (weak, nonatomic) IBOutlet UIImageView *imageView;

@end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ImageViewController.mm&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (void)viewDidLoad
{
  [super viewDidLoad];
  // Do any additional setup after loading the view.
  self.navigationItem.rightBarButtonItems = [NSArray arrayWithObjects:
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>OpenCV Tutorial - Part 3</title>
      <link>https://example.com/2012-06-27-opencv-tutorial-part-3/</link>
      <pubDate>Wed, 27 Jun 2012 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2012-06-27-opencv-tutorial-part-3/</guid>
      <description>

&lt;p&gt;In &lt;a href=&#34;http://computer-vision-talks.com/2012/06/opencv-tutorial-a-collection-of-opencv-samples-for-iphoneipad-part-1/&#34; title=&#34;OpenCV Tutorial – a collection of OpenCV samples for iPhone/iPad – Part 1&#34;&gt;Part 1&lt;/a&gt; and &lt;a href=&#34;http://computer-vision-talks.com/2012/06/opencv-tutorial-part-2/&#34; title=&#34;OpenCV Tutorial – Part 2&#34;&gt;Part 2&lt;/a&gt; we created base application for our &amp;ldquo;OpenCV Tutorial&amp;rdquo; application. In this part we add video source to process frames using our samples and present the result to user. As usual, you can find source code for this application at &lt;a href=&#34;https://github.com/BloodAxe/OpenCV-Tutorial&#34; title=&#34;OpenCV Tutorial Repository&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;video-capture-in-ios&#34;&gt;Video capture in iOS&lt;/h2&gt;

&lt;p&gt;At this moment (as far as i know) there OpenCV&amp;rsquo;s cv::VideoCapture does not support iOS platform. Therefore we have to use iOS AVFoundation API to setup video capture. This is more complicated that write cv::VideoCapture(&amp;ldquo;YourVideoFileName.avi&amp;rdquo;) but it&amp;rsquo;s not a rocket science.  There is a great Apple documentation article &lt;a href=&#34;http://developer.apple.com/library/ios/#DOCUMENTATION/AudioVideo/Conceptual/AVFoundationPG/Articles/00_Introduction.html&#34; title=&#34;AV Foundation Programming Guide&#34;&gt;AV Foundation Programming Guide&lt;/a&gt; that i strongly advice you to read. &lt;strong&gt;By the way, before I forget - video capture is not supported on iOS simulator. You&amp;rsquo;ll need real device to test your app!&lt;/strong&gt; To manage the capture from a device such as a camera or microphone, you assemble objects to represent inputs and outputs, and use an instance of &lt;code&gt;AVCaptureSession&lt;/code&gt; to coordinate the data flow between them. Minimally you need:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;An instance of &lt;code&gt;AVCaptureDevice&lt;/code&gt; to represent the input device, such as a camera or microphone&lt;/li&gt;
&lt;li&gt;An instance of a concrete subclass of &lt;code&gt;AVCaptureInput&lt;/code&gt; to configure the ports from the input device&lt;/li&gt;
&lt;li&gt;An instance of a concrete subclass of &lt;code&gt;AVCaptureOutput&lt;/code&gt; to manage the output to a movie file or still image&lt;/li&gt;
&lt;li&gt;An instance of &lt;code&gt;AVCaptureSession&lt;/code&gt; to coordinate the data flow from the input to the output
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To put all things together we introduce VideoSource class which incapsulate initialization and video capture routine. Let&amp;rsquo;s take a look on it&amp;rsquo;s interface:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;@protocol VideoSourceDelegate 

- (void) frameCaptured:(cv::Mat) frame;

@end

@interface VideoSource : NSObject

@property id delegate;

- (bool) hasMultipleCameras;
- (void) toggleCamera;
- (void) startRunning;
- (void) stopRunning;

@end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The VideoSourceDelegate protocol defines a callback procedure that user code can handle. The frameCaptured method is called when the frame is received from a camera device. We wrap it in cv::Mat structure for latter use. Initialization of VideoCamera is also not complicated: we create capture session, add video input and video output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (id) init
{
  if (self = [super init])
  {
    currentCameraIndex = 0;
    session = [[AVCaptureSession alloc] init];
    [session setSessionPreset:AVCaptureSessionPreset640x480];
    captureDevices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo];

    AVCaptureDevice *videoDevice = [captureDevices objectAtIndex:0];

    NSError * error;
    captureInput = [AVCaptureDeviceInput deviceInputWithDevice:videoDevice error:&amp;amp;error];

    if (error)
    {
      NSLog(@&amp;quot;Couldn&#39;t create video input&amp;quot;);
    }

    captureOutput = [[AVCaptureVideoDataOutput alloc] init];
    captureOutput.alwaysDiscardsLateVideoFrames = YES; 

    // Set the video output to store frame in BGRA (It is supposed to be faster)
    NSString* key = (NSString*)kCVPixelBufferPixelFormatTypeKey; 
    NSNumber* value = [NSNumber numberWithUnsignedInt:kCVPixelFormatType_32BGRA]; 
    NSDictionary* videoSettings = [NSDictionary dictionaryWithObject:value forKey:key]; 
    [captureOutput setVideoSettings:videoSettings];    

    /*We create a serial queue to handle the processing of our frames*/
    dispatch_queue_t queue;
    queue = dispatch_queue_create(&amp;quot;com.computer-vision-talks.cameraQueue&amp;quot;, NULL);
    [captureOutput setSampleBufferDelegate:self queue:queue];
    dispatch_release(queue);

    [session addInput:captureInput];
    [session addOutput:captureOutput];
  }

  return self;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Please not that we query all video devices available using the [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo] call to get them all. This feature allows us to toggle between front and rear cameras in runtime. Also we set capture session preset to 640x480 because the larger the image the more it it need to be processed by our algorithms. 640x480 is a good choice. As our last step we configure capture output to pass our frames in BGRA format. For our image processing it&amp;rsquo;s the ideal case. In case when our video source has several video inputs we can toggle between them using toggleCamera method:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (void) toggleCamera
{
  currentCameraIndex++;
  int camerasCount = [captureDevices count];
  currentCameraIndex = currentCameraIndex % camerasCount;

  AVCaptureDevice *videoDevice = [captureDevices objectAtIndex:currentCameraIndex];

  [session beginConfiguration];

  if (captureInput)
  {
    [session removeInput:captureInput];
  }

  NSError * error;
  captureInput = [AVCaptureDeviceInput deviceInputWithDevice:videoDevice error:&amp;amp;error];

  if (error)
  {
    NSLog(@&amp;quot;Couldn&#39;t create video input&amp;quot;);
  }

  [session addInput:captureInput];
  [session setSessionPreset:AVCaptureSessionPreset640x480];
  [session commitConfiguration];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this function we cycle through available inputs and add them to capture session (do not forget to remove previous input) and set the capture preset again (just in case). Switching between from and rear camera is initiated by the user by tapping on toggle button (we will talk about it a bit later). Just one thing left - we should implement AVCaptureVideoDataOutputSampleBufferDelegate in our VideoSource class to receive frames from AVFoundation API like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;- (void)captureOutput:(AVCaptureOutput *)captureOutput 
didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer 
       fromConnection:(AVCaptureConnection *)connection 
{ 
  if (!delegate)
    return;

  CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer); 

  /*Lock the image buffer*/
  CVPixelBufferLockBaseAddress(imageBuffer,0); 

  /*Get information about the image*/
  uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(imageBuffer); 
  size_t width = CVPixelBufferGetWidth(imageBuffer); 
  size_t height = CVPixelBufferGetHeight(imageBuffer);  
  size_t stride = CVPixelBufferGetBytesPerRow(imageBuffer);
  //NSLog(@&amp;quot;Frame captured: %lu x %lu&amp;quot;, width,height);

  cv::Mat frame(height, width, CV_8UC4, (void*)baseAddress);

  [delegate frameCaptured:frame];

  /*We unlock the  image buffer*/
  CVPixelBufferUnlockBaseAddress(imageBuffer,0);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this method we obtain raw data in BGRA format from CoreMedia and put them input cv::Mat (no data is copied here). And then we call [delegate frameCaptured:] to inform user code that new frame is available.&lt;/p&gt;

&lt;h2 id=&#34;displaying-processed-frames&#34;&gt;Displaying processed frames&lt;/h2&gt;

&lt;p&gt;Since we working with video processing it&amp;rsquo;s obviously to present the result of video processing in real-time on the screen. To secure the appropriate FPS we have to redraw our screen fast. The existing UIImageView control cannot be used here because we&amp;rsquo;ll spent to much time on unnecessary image conversion (cv::Mat to UIImage) and drawing UIImage on the ImageView. For our goal the fastest way to draw the picture is to use OpenGL ES and GLView to draw our bitmap using GPU. In this section i&amp;rsquo;ll show you how to create simple view that able to draw cv::Mat in real-time. To secure this&amp;rsquo;ll UIView and to EAGLContext using it. This allows us to use OpenGL API to draw textured rectangle on top of our view as fast as possible. For user code we expose only single function that is can use:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-objectivec&#34;&gt;@interface GLESImageView : UIView

- (void)drawFrame:(const cv::Mat&amp;amp;) bgraFrame;

@end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>