<!doctype html><html dir=ltr lang=en data-theme class=html><head><title>Eugene Khvedchenya
|
Training Haar cascade in the cloud</title><meta charset=utf-8><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Eugene Khvedchenya"><meta name=description content="A personal blog about Computer Vision, AI, Kaggle, Open-Source and research. You will never read this in books."><link rel=stylesheet href=/scss/main.min.b2e0cb07595e3519ab1193bb421914e06c0e26b0cc561fef23b3c6131d4d2ffa.css integrity="sha256-suDLB1leNRmrEZO7QhkU4GwOJrDMVh/vI7PGEx1NL/o=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.31b0a1f317f55c529a460897848c97436bb138b19c399b37de70d463a8bf6ed5.css integrity="sha256-MbCh8xf1XFKaRgiXhIyXQ2uxOLGcOZs33nDUY6i/btU=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=canonical href=https://computer-vision-talks.com/post/cloud-haartaining/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.560a26330d27ff44a44e83b53cd07a95d4230a65930d31c5c76a8d481e5b35bf.js integrity="sha256-VgomMw0n/0SkToO1PNB6ldQjCmWTDTHFx2qNSB5bNb8=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Training Haar cascade in the cloud"><meta name=twitter:description content="In this post I&rsquo;ll show you how you can train cascade classifier with OpenCV very quickly even if you have low-end hardware using virtual machine in the cloud.
Why Clouds? Well, training a descriptor takes a lot of time. Depending on template size, number of samples and stages, it can take from several ours to a couple of days to train such cascade! Be prepared that during training stage your PC will likely be unusuable due to high RAM usage and CPU load."><meta property="og:title" content="Training Haar cascade in the cloud"><meta property="og:description" content="In this post I&rsquo;ll show you how you can train cascade classifier with OpenCV very quickly even if you have low-end hardware using virtual machine in the cloud.
Why Clouds? Well, training a descriptor takes a lot of time. Depending on template size, number of samples and stages, it can take from several ours to a couple of days to train such cascade! Be prepared that during training stage your PC will likely be unusuable due to high RAM usage and CPU load."><meta property="og:type" content="article"><meta property="og:url" content="https://computer-vision-talks.com/post/cloud-haartaining/"><meta property="article:section" content="post"><meta property="article:published_time" content="2014-05-20T00:00:00+00:00"><meta property="article:modified_time" content="2014-05-20T00:00:00+00:00"><meta property="og:site_name" content="Computer Vision Talks"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"Training Haar cascade in the cloud","headline":"Training Haar cascade in the cloud","alternativeHeadline":"","description":"
      
        In this post I\u0026rsquo;ll show you how you can train cascade classifier with OpenCV very quickly even if you have low-end hardware using virtual machine in the cloud.\nWhy Clouds? Well, training a descriptor takes a lot of time. Depending on template size, number of samples and stages, it can take from several ours to a couple of days to train such cascade! Be prepared that during training stage your PC will likely be unusuable due to high RAM usage and CPU load.


      


    ","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/computer-vision-talks.com\/post\/cloud-haartaining\/"},"author":{"@type":"Person","name":"Eugene Khvedchenya"},"creator":{"@type":"Person","name":"Eugene Khvedchenya"},"accountablePerson":{"@type":"Person","name":"Eugene Khvedchenya"},"copyrightHolder":{"@type":"Person","name":"Eugene Khvedchenya"},"copyrightYear":"2014","dateCreated":"2014-05-20T00:00:00.00Z","datePublished":"2014-05-20T00:00:00.00Z","dateModified":"2014-05-20T00:00:00.00Z","publisher":{"@type":"Organization","name":"Eugene Khvedchenya","url":"https://computer-vision-talks.com/","logo":{"@type":"ImageObject","url":"https:\/\/computer-vision-talks.com\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/computer-vision-talks.com\/post\/cloud-haartaining\/","wordCount":"746","genre":[],"keywords":["opencv","cloud-computing","tutorials"]}</script></head><body class="body theme--light"><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/images/4ECfkGJr_400x400.jpg alt="profile picture"><div class=sidebar__introduction-title><a href=/>Computer Vision Talks</a></div><div class=sidebar__introduction-description><p>A personal blog about Computer Vision, AI, Kaggle, Open-Source and research. You will never read this in books.</p></div></div><ul class=sidebar__list><li class=sidebar__list-item><a href=https://www.linkedin.com/in/cvtalks/ target=_blank rel=noopener aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://twitter.com/cvtalks target=_blank rel=noopener aria-label=Twitter title=Twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://github.com/BloodAxe target=_blank rel=noopener aria-label=GitHub title=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=mailto:ekhvedchenya@computer-vision-talks.com target=_blank rel=noopener aria-label=E-Mail title=E-Mail><i class="fas fa-envelope fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
Eugene Khvedchenya
2022</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.9c062c557275acbaba71f0c7cd4024da3e3cc825d248bc4b2130811b0965330b.js integrity="sha256-nAYsVXJ1rLq6cfDHzUAk2j48yCXSSLxLITCBGwllMws=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-10687369-5","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/ title>Home</a></li><li class=nav__list-item><a href=/post/ title>Posts</a></li><li class=nav__list-item><a href=/open-source title>Open-Source</a></li><li class=nav__list-item><a href=/challenges title>Wins</a></li><li class=nav__list-item><a href=/publications title>Publications</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>Training Haar Cascade in the Cloud</h1><ul class=post__meta><li class=post__meta-item><em class="fas fa-calendar-day post__meta-icon"></em>
<span class=post__meta-text>Tue, May 20, 2014</span></li><li class=post__meta-item><em class="fas fa-stopwatch post__meta-icon"></em>
<span class=post__meta-text>4-minute read</span></li></ul><p>In this post I&rsquo;ll show you how you can train cascade classifier with OpenCV very quickly even if you have low-end hardware using virtual machine in the cloud.</p><h2 id=why-clouds>Why Clouds?</h2><p>Well, training a descriptor takes a lot of time. Depending on template size, number of samples and stages, it can take from several ours to a couple of days to train such cascade! Be prepared that during training stage your PC will likely be unusuable due to high RAM usage and CPU load. If you&rsquo;re on laptop - it will become hot really quick. So, what if you have to train your cascade, but you don&rsquo;t have either time or spare machine to do this?</p><p>Recently I&rsquo;ve faced this problem in one of my personal projects. What even more funny, a 10 hours flight was approaching, but I didn&rsquo;t wanted to waste this time for nothing. I only had a laptop, but this task will drain my battery for sure. So I&rsquo;ve decided to use virtual server to do this.</p><h2 id=step-1---environment-setup>Step 1 - Environment setup</h2><p>First, I&rsquo;ve created a basic droplet in <a href="https://www.digitalocean.com/?refcode=b93faa829f80">DigitalOcean</a>.
Yep, for 5$/month you can have your droplet that can do much!
It takes only 55 seconds to deploy a new instance (I assume you&rsquo;re familiar with SSH keys, terminal, Git and so on.) and we&rsquo;re ready to rock!</p><p><img src=image1.png alt="Create DigitalOcean droplet"></p><h2 id=step-2---install-latest-opencv-release>Step 2 - Install latest OpenCV release</h2><p>There are two ways to do this: either using package managers (homebrew, yum or apt-get) or builiding it from scratch.
Personally I prefer second option since you can configure OpenCV. Usually I build static libs whith apps but without tests, java, cuda, python, OpenEXR, Jasper and Tiff. Regardless of the way you choose to install OpenCV, ensure that opencv apps (opencv_createsamples, opencv_traincascade) are also installed!.</p><h2 id=step-3---prepare-your-train-data>Step 3 - Prepare your train data</h2><p>There are a lot of tutorials <a href=http://note.sonots.com/SciSoftware/haartraining.html>1</a>, <a href=http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html>2</a>, <a href=http://answers.opencv.org/question/7141/about-traincascade-paremeters-samples-and-other/>3</a> on how to train cascade with OpenCV: which images are good for positive and negative samples and which settings should be used for cascade training. Let&rsquo;s assume you have everything in a single folder on your load machine and there is a script called &ldquo;train.sh&rdquo; that starts training stage:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>opencv_traincascade -data classifier <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -vec &lt;positive samples file&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -bg &lt;negative samples file&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -numStages <span class=m>12</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -minHitRate 0.999 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -maxFalseAlarmRate 0.5 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -numPos <span class=m>15000</span> -numNeg <span class=m>17000</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -w <span class=m>24</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -h <span class=m>24</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -mode ALL <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -nonsym <span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                    -featureType LBP
</span></span></code></pre></div><h2 id=step-4---deploy-train-data-to-cloud>Step 4 - Deploy train data to cloud</h2><p>The easiest way to upload this folder to your virtual droplet is to use the <a href=http://en.wikipedia.org/wiki/Rsync>rsync</a> tool.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>rsync -avz &lt;source&gt; &lt;destination&gt;
</span></span></code></pre></div><p>For instance, the following command will upload the traindata/ folder with it&rsquo;s content to ~haartraining.example.com~ webserver to /traindata directory. This example assumes that your public key has been added to haartraining.example.com during droplet creation.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>rsync -avz ~/Develop/traindata root@haartraining.example.com:/traindata
</span></span></code></pre></div><h2 id=step-5---start-training>Step 5 - Start training</h2><p>The easiest way to execute training is to login to remote maching using ssh and execute the train script with a simple command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh root@haartraining.example.com
</span></span><span class=line><span class=cl>sh /traindata/train.sh
</span></span></code></pre></div><p>However, this will require you to keep SSH-session open all the time. If you log-out, the process will terminate and training will be terminated as well.
To prevent this we can use <a href=http://en.wikipedia.org/wiki/Nohup>nohup</a> UNIX utility:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh root@haartraining.example.com
</span></span><span class=line><span class=cl>nohup sh /traindata/train.sh &gt; /traindata/train.log
</span></span></code></pre></div><p>Training will continue to work regardless of the user&rsquo;s connection to the terminal, and log results to the file train.log.</p><h2 id=step-6---getting-the-results>Step 6 - Getting the results</h2><p>After trainign is done (you can check this by top command output or looking at the train.log), we can download trainresults back with rsync command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>rsync -avz root@haartraining.example.com:/traindata ~/Develop/traindata 
</span></span></code></pre></div><h2 id=step-7---speeding-up-training>Step 7 - Speeding up training</h2><p>To speed-up training stage I recommend to pass additional options to opencv_traincascade tool:</p><ul><li>precalcValBufSize=2048</li><li>precalcIdxBufSize=2048</li></ul><p>Ideally you want to use all available memory of your instance for these buffers, so if you have 4Gb of RAM installed, pass at least a 1Gb to each of these buffers.</p><p>It also may be a good idea to &ldquo;shrink&rdquo; the DigitalOcean&rsquo;s droplet to more powerful configuration which gives you 16Gb of RAM and 8 CPU&rsquo;s.</p><p><img src=resize.png alt="Resize droplet"></p><p>This task can be done on few clicks:</p><h3 id=stop-your-droplet>Stop your droplet</h3><p><img src=poweroff.png alt="Poweroff droplet"></p><h3 id=choose-resize-and-pick-a-necessary-configuration>Choose &ldquo;Resize&rdquo; and pick a necessary configuration</h3><p><img src=resize2.png alt="Resize droplet"></p><h3 id=power-up-your-resized-droplet>Power-up your resized droplet</h3><p><strong>Please be advised, that DigitalOcean will charge you regardless whether your droplet is powered on or off. So if you&rsquo;re not using it - make a snapshot of it and delete unused droplet to save money</strong>.</p><p>That&rsquo;s all. I hope you enjoyed reading this post. Please, leave your comments.</p></div><div class=post__footer><span><a class=tag href=/tags/opencv/>opencv</a><a class=tag href=/tags/cloud-computing/>cloud-computing</a><a class=tag href=/tags/tutorials/>tutorials</a></span></div><div id=comment><h2>comments</h2><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//cvtalks.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
Eugene Khvedchenya
2022</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.9c062c557275acbaba71f0c7cd4024da3e3cc825d248bc4b2130811b0965330b.js integrity="sha256-nAYsVXJ1rLq6cfDHzUAk2j48yCXSSLxLITCBGwllMws=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-10687369-5","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>