<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ghostwriter example</title>
    <link>https://example.com/tags/algorithms/index.xml</link>
    <description>Recent content on Ghostwriter example</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>My Name</copyright>
    <atom:link href="https://example.com/tags/algorithms/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Tile-based image processing</title>
      <link>https://example.com/tile-based-image-processing/</link>
      <pubDate>Thu, 04 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/tile-based-image-processing/</guid>
      <description>

&lt;p&gt;How would you design an algorithm to process 40Mpx image? 100Mpx? What about gigapixel-sized panorams? Obviously, it should differs from those that are intended for 640x480 images. Here I want to present you implementation of the very simple but powerful approach called &amp;ldquo;Tile-based image processing&amp;rdquo;. I will show you how to make this using OpenCV.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;tiles.png&#34; alt=&#34;Tile based image processing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s define a few restrictions in order to simplify our implementation. In this tutorial I will consider a &amp;lsquo;pass-through&amp;rsquo; pipeline - when we apply some function to input image and give an output image of the same size as an output.&lt;/p&gt;

&lt;p&gt;It is possible to extend this approach to work with many input images, but for the sake of simplicity I&amp;rsquo;ll omit this for now.&lt;/p&gt;

&lt;p&gt;Consider a following algorithm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Take a source image for RGB color space.&lt;/li&gt;
&lt;li&gt;Convert in to grayscale color space (unsigned byte).&lt;/li&gt;
&lt;li&gt;Compute Sobel derivatives (signed short).&lt;/li&gt;
&lt;li&gt;Take a Dx, Dy for each pixel and compute it&amp;rsquo;s magnitude and orientation.&lt;/li&gt;
&lt;li&gt;Leave only those, which magnitude is larger than threshold.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using OpenCV it could look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;cv::Mat source = cv::imread(&amp;quot;input.jpg&amp;quot;);
cv::Mat grayscale, dx, dy;
cv::cvtColor(source, grayscale);
cv::Sobel(grayscale, dx, 1, 0);
cv::Sobel(grayscale, dy, 0, 1);
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;problems-with-straighforward-implementation&#34;&gt;Problems with straighforward implementation&lt;/h2&gt;

&lt;p&gt;This routine require &lt;code&gt;N + 2 * N * sizeof(signed short)&lt;/code&gt; bytes of additional memory for straightforward implementation, where N is number of pixels in source image. Large number of intermediate buffers can cause memory issues for memory restricted devices (mobile phones, embedded systems).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;On iOS, in particular, your app might get terminated by iOS watchdog for high peak RAM usage, despite the fact you use this memory only for a temp buffers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Second issue with large amount of buffers is cache-misses. Large buffers are likely to sit near each other, therefore cache performance will be low and algorithm performance will suffer.&lt;/p&gt;

&lt;p&gt;To adress those two issues, I suggest to divide input image into &amp;ldquo;Tiles&amp;rdquo; - regions of the original image of equal size, let&amp;rsquo;s say 64x64. The processing function remains the same, but we reuse all temporary buffers and process only 64x64 pixels at one time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;algorithm.png&#34; alt=&#34;Tile based image processing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say we&amp;rsquo;re processing &lt;code&gt;1280x720&lt;/code&gt; frame, using regular approach, the total amount of
additional memory is &lt;strong&gt;4.6 megabytes&lt;/strong&gt; (&lt;code&gt;4608000&lt;/code&gt; bytes). With tile-based approach, we need only &lt;strong&gt;20 kilobytes&lt;/strong&gt; (&lt;code&gt;20480&lt;/code&gt; bytes). 20K are likely to fit entirely in L2 cache and therefore give a significant performance boost.&lt;/p&gt;

&lt;h2 id=&#34;tile-based-implementation&#34;&gt;Tile-based implementation&lt;/h2&gt;

&lt;p&gt;To implement tile-based implementation, we iterate over the image, copy tiles from source image to our local source tile, process it and write to corresponding area in the
destination image.&lt;/p&gt;

&lt;p&gt;A pseudo-code for this routine is follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;template&amp;lt;int TileSize, typename Algorithm&amp;gt;
void process(const cv::Mat&amp;amp; sourceImage, cv::Mat&amp;amp; resultImage, Algorithm algorithm) const
{
    assert(!resultImage.empty());
    assert(sourceImage.rows == resultImage.rows);
    assert(sourceImage.cols == resultImage.cols);

    const int rows = (sourceImage.rows / TileSize) + (sourceImage.rows % TileSize ? 1 : 0);
    const int cols = (sourceImage.cols / TileSize) + (sourceImage.cols % TileSize ? 1 : 0);

    cv::Mat tileInput, tileOutput;

    for (int rowTile = 0; rowTile &amp;lt; rows; rowTile++)
    {
        for (int colTile = 0; colTile &amp;lt; cols; colTile++)
        {
            copyTileFromSource(sourceImage, tileInput, rowTile, colTile);
            algorithm(tileInput, tileOutput);
            copyTileToResultImage(tileOutput, resultImage, rowTile, colTile);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I hope it&amp;rsquo;s clear to understand what is happening in code above. The &lt;code&gt;Algorithm&lt;/code&gt; here represents some algorithm that we want to run on our tiles. There are two functions &lt;code&gt;copyTileFromSource&lt;/code&gt; and &lt;code&gt;copyTileToResultImage&lt;/code&gt; that will be covered a bit later.&lt;/p&gt;

&lt;h2 id=&#34;dealing-with-out-of-tile-reads&#34;&gt;Dealing with out-of-tile reads&lt;/h2&gt;

&lt;p&gt;You may ask yourself - what should we do with border pixels? Sobel operator use neighbor pixels around each pixel. When we construct a tile shouldn&amp;rsquo;t we take this into account? Sure we are. So that&amp;rsquo;s why there is a padding parameter that controls amount of additional pixels that are added to top, left, bottom and right of the tile in order to make functions that require additional pixels work correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;tile_with_paddings.png&#34; alt=&#34;Tile with paddings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Padding makes tile overlap each other, but we pay this price for good cache locality.&lt;/p&gt;

&lt;p&gt;I will use a slightly modified version of code from above:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;struct TiledAlgorithm
{
    TiledAlgorithm(int tileSize, int padding, int borderType)
        : mTileSize(tileSize)
        , mPadding(padding)
        , mBorderType(borderType)
    {
    }

    void process(const cv::Mat&amp;amp; sourceImage, cv::Mat&amp;amp; resultImage) const
    {
        assert(!resultImage.empty());
        assert(sourceImage.rows == resultImage.rows);
        assert(sourceImage.cols == resultImage.cols);

        int rows = (sourceImage.rows / mTileSize) + (sourceImage.rows % mTileSize ? 1 : 0);
        int cols = (sourceImage.cols / mTileSize) + (sourceImage.cols % mTileSize ? 1 : 0);

        cv::Mat tileInput, tileOutput;

        for (int rowTile = 0; rowTile &amp;lt; rows; rowTile++)
        {
            for (int colTile = 0; colTile &amp;lt; cols; colTile++)
            {
                cv::Rect srcTile(colTile * mTileSize - mPadding, 
                                 rowTile * mTileSize - mPadding, 
                                 mTileSize + 2 * mPadding, 
                                 mTileSize + 2 * mPadding);

                cv::Rect dstTile(colTile * mTileSize,            
                                 rowTile * mTileSize, 
                                 mTileSize, 
                                 mTileSize);

                copySourceTile(sourceImage, tileInput, srcTile);
                processTileImpl(tileInput, tileOutput);
                copyTileToResultImage(tileOutput, resultImage, dstTile);
            }
        }
    }

protected:
    virtual void processTileImpl(const cv::Mat&amp;amp; srcTile, cv::Mat&amp;amp; dstTile) const = 0;
    
    void copySourceTile(const cv::Mat&amp;amp; src, cv::Mat&amp;amp; srcTile, cv::Rect &amp;amp;tile) const;
    void copyTileToResultImage(const cv::Mat&amp;amp; tileImage, cv::Mat&amp;amp; resultImage, cv::Rect resultRoi);

};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;processing_with_paddings.png&#34; alt=&#34;Processing with paddings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To fill a tile with source image we should check whether tile is close to image border. In this case OpenCV will come to help with cv::copyMakeBorder function that helps us to fill the missing pixels with given border fill method. If tile including paddings are entirely in the image boundary, it&amp;rsquo;s enough to just copy image region to a tile:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void copySourceTile(const cv::Mat&amp;amp; src, cv::Mat&amp;amp; srcTile, cv::Rect &amp;amp;tile)
{
    auto tl = tile.tl();
    auto br = tile.br();

    cv::Point tloffset, broffset;

    //Take care of border cases
    if (tile.x &amp;lt; 0)
    {
        tloffset.x = -tile.x;
        tile.x = 0;
    }

    if (tile.y &amp;lt; 0)
    {
        tloffset.y = -tile.y;
        tile.y = 0;
    }

    if (br.x &amp;gt;= src.cols)
    {
        broffset.x = br.x - src.cols + 1;
        tile.width -= broffset.x;
    }

    if (br.y &amp;gt;= src.rows)
    {
        broffset.y = br.y - src.rows + 1;
        tile.height -= broffset.y;
    }

    // If any of the tile sides exceed source image boundary we must use copyMakeBorder to make proper paddings for this side
    if (tloffset.x &amp;gt; 0 || tloffset.y &amp;gt; 0 || broffset.x &amp;gt; 0 || broffset.y &amp;gt; 0)
    {
        cv::Rect paddedTile(tile.tl(), tile.br());
        assert(paddedTile.x &amp;gt;= 0);
        assert(paddedTile.y &amp;gt;= 0);
        assert(paddedTile.br().x &amp;lt; src.cols);
        assert(paddedTile.br().y &amp;lt; src.rows);

        cv::copyMakeBorder(src(paddedTile), srcTile, tloffset.y, broffset.y, tloffset.x, broffset.x, mBorderType);
    }
    else
    {
        // Entire tile (with paddings lies inside image and it&#39;s safe to just take a region:
        src(tile).copyTo(srcTile);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For non-zero padding case we add additional pixels to source tile, therefore it has effective width and height of &lt;code&gt;TileSize + Padding + Padding&lt;/code&gt;, but after processing we write only central segment of the tile of size &lt;code&gt;TileSize x TileSize&lt;/code&gt; to destination image. In case of Sobel, we need a padding of &lt;code&gt;1&lt;/code&gt;, because Sobel uses 3x3 kernel by default.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void copyTileToResultImage(const cv::Mat&amp;amp; tileImage, cv::Mat&amp;amp; resultImage, cv::Rect resultRoi)
{
    cv::Rect srcTile(mPadding, mPadding, mTileSize, mTileSize);

    auto br = resultRoi.br();

    if (br.x &amp;gt;= resultImage.cols)
    {
        resultRoi.width -= br.x - resultImage.cols;
        srcTile.width -= br.x - resultImage.cols;
    }

    if (br.y &amp;gt;= resultImage.rows)
    {
        resultRoi.height -= br.y - resultImage.rows;
        srcTile.height -= br.y - resultImage.rows;
    }

    cv::Mat tileView = tileImage(srcTile);
    cv::Mat dstView = resultImage(resultRoi);

    assert(tileView.rows == dstView.rows);
    assert(tileView.cols == dstView.cols);

    tileView.copyTo(dstView);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;application&#34;&gt;Application&lt;/h2&gt;

&lt;p&gt;This approach can be used when you need to guarantee low-memory footprint of your algorithm or you want to use data locality without changing a lot in your code. In this
case I suggest to pre-allocate data buffers as a continuous block of memory:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// Our algorithm need three intermediate buffers: a,b,c that
// we want to store close to each other
class MyAlgorithm : public TiledAlgorithm
{
public:
    MyAlgorithm(int tileSize, int padding)
    {
        int size = tileSize + padding * 2;

        // Allocate all buffer as continuous array
        mBuffer.create(size * 3, size, CV_8UC1);
            
        // Create views to sub-regions of mBuffer
        a = mBuffer.rowRange(0,      size);
        b = mBuffer.rowRange(size,   2*size);
        c = mBuffer.rowRange(2*size, 3*size);
    }

private:
    cv::Mat mBuffer;

    cv::Mat a, b c;
}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Did you know, that JPEG-2000 coded use tile-based encoding and it allows this codec to retrieve (decode) an arbitrary region of the image? Also, tiles are widely used in aerial photography to stich images.&lt;/p&gt;

&lt;p&gt;I hope you find this post interesting. Pleas let me know on which topics you would like to see in my blog. Feel free to drop a ping on &lt;a href=&#34;https://twitter.com/cvtalks&#34;&gt;@cvtalks&lt;/a&gt; or leave a comment. Thanks!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to detect circles in noisy images</title>
      <link>https://example.com/how-to-detect-circles-in-noisy-image/</link>
      <pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/how-to-detect-circles-in-noisy-image/</guid>
      <description>&lt;p&gt;p
    | This was a request from
    a(href=&amp;ldquo;&lt;a href=&#34;http://www.reddit.com/r/computervision/comments/2a1lvi/help_how_to_process_this_image_to_find_the_circles/&amp;quot;&#34;&gt;http://www.reddit.com/r/computervision/comments/2a1lvi/help_how_to_process_this_image_to_find_the_circles/&amp;quot;&lt;/a&gt;) /r/computervision.
    | A reddit member was asking on how to count number of eggs on quite
    | noisy image like you may see below.
    | I&amp;rsquo;ve decided to write a simple algorithm that does the job and explain how it works.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;div.beforeafter
    img(src=&amp;quot;source.jpg&amp;quot;,alt=&amp;quot;before&amp;quot;)
    img(src=&amp;quot;display.jpg&amp;quot;,alt=&amp;quot;after&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;span.more&lt;/p&gt;

&lt;p&gt;h2 Step 1 - Filter image&lt;/p&gt;

&lt;p&gt;p
    img(src=&amp;ldquo;source.jpg&amp;rdquo;,alt=&amp;ldquo;Source image&amp;rdquo;)
    | The original image has noticeable color noise and therefore it must be filtered before we pass it to further stages.
    | Ideally you should choose filter algorithm based on your task and noise model. For the sake of simplicity,
    | I will not use Weiner filter or deconvolution to deal with blur and artifacts that are present on source image.&lt;/p&gt;

&lt;p&gt;p
    | Instead, I will use pyramidal mean-shift filter.
    | This algorithm works quite well on this problem and helps to get rid of artifacts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pre.
    cv::Mat inputRgbImage = cv::imread(&amp;quot;input.jpg&amp;quot;);
    cv::Mat filtered;
    cv::pyrMeanShiftFiltering(inputRgbImage, 
                              filtered, 
                              spatialWindowRadius, 
                              colorWindowRadius, 
                              2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;strong Result of filtering
div.beforeafter.twentytwenty-container
    img(src=&amp;ldquo;source.jpg&amp;rdquo;,alt=&amp;ldquo;before&amp;rdquo;)
    img(src=&amp;ldquo;filtered.jpg&amp;rdquo;,alt=&amp;ldquo;after&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;h2 Step 2 - Increase sharpness&lt;/p&gt;

&lt;p&gt;p
    | As you may see - eggs edges are not too sharp. For circle detection we will use Hough transform. OpenCV&amp;rsquo; Hough algorithm implementation use Canny edge detector to
    | detect edges. Unfortunately it&amp;rsquo;s not possible to pass manually computed binary image.
    | Instead we have to pass 8-bit grayscale image for circle detection.
    | Therefore we may want to increase their sharpness to make the image more friendly for Canny detector.&lt;/p&gt;

&lt;p&gt;p
    | Sharpening can be easily done via unsharp mask.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pre.
    // Perform in-place unsharp masking operation
    // http://opencv-code.com/quick-tips/sharpen-image-with-unsharp-mask/
    void unsharpMask(cv::Mat&amp;amp; im) 
    {
        cv::Mat tmp;
        cv::GaussianBlur(im, tmp, cv::Size(5,5), 5);
        cv::addWeighted(im, 1.5, tmp, -0.5, 0, im);
    }

| However, unsharp mask can cause artifacts on edge borders which may lead to double edge.
| I&#39;m dealing with it by adding laplaccian component to final result:

pre.
    cv::cvtColor(filtered, grayImg, cv::COLOR_BGR2GRAY);
    grayImg.convertTo(grayscale, CV_32F);

    cv::GaussianBlur(grayscale, blurred, cv::Size(5,5), 0);

    cv::Laplacian(blurred, laplaccian, CV_32F);

    cv::Mat sharpened = 1.5f * grayscale
                      - 0.5f * blurred
                      - weight * grayscale.mul(scale * laplaccian);

strong Result of sharpening
div.beforeafter.twentytwenty-container
    img(src=&amp;quot;filtered.jpg&amp;quot;,alt=&amp;quot;before&amp;quot;)
    img(src=&amp;quot;filteredGray.jpg&amp;quot;,alt=&amp;quot;after&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;h3 Step 3 - Detect circles&lt;/p&gt;

&lt;p&gt;p
    | After we got nicely filtered and enchanced image we can pass it to cv::HoughCircles to detect circles on the image.
    | According to problem task, we can limit the maximum radius of circles that we are interested in (we need only small circles).
    | In addition eggs can be close to each other, so it make sense to set minimal distance between two circles to minimal
    | allowed diameter of the circle.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pre.
   cv::HoughCircles(filteredGray, 
                    circles, 
                    cv::HOUGH_GRADIENT, 
                    2,   // Accumulator resolution
                    12,  // Minimum distance between the centers of the detected circles.
                    cannyThreshold, 
                    accumulatorThreshold, 
                    5,   // Minimum circle radius
                    20); // Maximum circle radius
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;h4 Step 4 - Validate eggs&lt;/p&gt;

&lt;p&gt;p
    | I&amp;rsquo;ve skipped validation step since I don&amp;rsquo;t know what are the requirements to detected eggs.
    | But I suppose that after detection of possible candidates using Hough the algorithm should validate each contour
    | to verify it is exactly what we were looking for. Here are some hints what we can check:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ul
    li Contour defects - how egg shape is close to ideal circle
    li Contour breaks - is there any breaks in egg boundary or not
    li Egg color - perhaps we need to count objects only of particular color
    li Neighbours - maybe (or maybe not) we need only isolated objects.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;h4 Step 5 - GPU Speed-up&lt;/p&gt;

&lt;p&gt;p
    | The given implementation is very slow since it use CPU and doing a lot of stuff that can be efficiently computed on GPU.
    | Fortunately, OpenCV has GPU implementations for mean-shift segmentation and hough transform and image blending.
    | You can easily speed-up this algorithm by using cv::gpu types and functions instead.&lt;/p&gt;

&lt;p&gt;h4 Bonus content&lt;/p&gt;

&lt;p&gt;p
    | Readed to the end of article? I&amp;rsquo;m impressed :) Here you go, the full source code of the algorithm and small playground to
    | tweak setting in runtime:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;script(src=&amp;quot;https://gist.github.com/BloodAxe/943fb14220021113d405.js&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Integration of KAZE 1.6 in OpenCV</title>
      <link>https://example.com/kaze-1.6-in-opencv/</link>
      <pubDate>Thu, 03 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/kaze-1.6-in-opencv/</guid>
      <description>

&lt;div class=&#34;featured-image&#34;&gt;
![AKAZE logo][akaze-logo]
&lt;/div&gt;

&lt;p&gt;A new version of KAZE and AKAZE features is a good candidate to become a part of OpenCV.
So i decided to update KAZE port i made a while ago with a new version of these features
and finally make a pull request to make it a part of OpenCV.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more clearfix&#34; /&gt;&lt;/p&gt;

&lt;div class=&#34;alert alert-info&#34;&gt;
&lt;p class=&#34;lead&#34;&gt;KAZE are now a part of OpenCV library&lt;/p&gt;

The OpenCV has accepted my pull-request and merged KAZE port into master branch of the OpenCV library. KAZE and AKAZE features will become available in OpenCV 3.0. Of course, you can grab development branch and build it from scratch to access it now.    
&lt;/div&gt;

&lt;p class=lead&gt;
** Looking for source code? It&#39;s all there: [KAZE &amp;amp; AKAZE in OpenCV][kaze-branch]. **
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;#results&#34;&gt;TL;DR; Scroll down to estimation charts&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;h1 id=&#34;integration-roadmap&#34;&gt;Integration roadmap&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m going to keep KAZE sources intact if possible to simplify their further support.
Original KAZE and AKAZE implementations will be placed in &lt;code&gt;kaze/&lt;/code&gt; and &lt;code&gt;akaze/&lt;/code&gt; folders under &lt;code&gt;features2/&lt;/code&gt; module and what we want is to write a facade-wrappers for these algorithms.&lt;/p&gt;

&lt;p&gt;To integrate KAZE featues we need to adopt sources code to OpenCV coding guidelines, make consistent with headers include system,
integrate into build system, implement wrapped from KAZE to Features2D API and add unit tests. This will be split into three steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Adopt KAZE and AKAZE sources (Remove unused functions, fix includes, macros)&lt;/li&gt;
&lt;li&gt;Implement Features2D wrappers and expose properties for runtime configuration of KAZE.&lt;/li&gt;
&lt;li&gt;Add unit tests and remove duplicate functions that are already exists in OpenCV.&lt;/li&gt;
&lt;li&gt;Replace OpenMP with cv::parallel&lt;em&gt;for&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;changes-in-kaze-sources&#34;&gt;Changes in KAZE sources&lt;/h2&gt;

&lt;p&gt;First, we need to adopt existing sources.&lt;/p&gt;

&lt;h3 id=&#34;step-1-update-opencv-includes&#34;&gt;Step 1 -Update OpenCV includes&lt;/h3&gt;

&lt;p&gt;Since we&amp;rsquo;re making KAZE a part of the library, it&amp;rsquo;s impossible to reference OpenCV types using standard.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;quot;opencv2/opencv.h&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead one may want to use precomp.hpp header like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;quot;precomp.hpp&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-2-cleaning-up-the-code&#34;&gt;Step 2 - Cleaning up the code&lt;/h3&gt;

&lt;p&gt;p There is a C-style assert(cond) macro that I will replace with CV_Assert for convinience.&lt;/p&gt;

&lt;h4 id=&#34;dump-of-kaze-internal-structures&#34;&gt;Dump of KAZE internal structures&lt;/h4&gt;

&lt;p&gt;KAZE and AKAZE algorithm can &amp;lsquo;dump&amp;rsquo; internal buffers to disk using imwrite function.
But features2d module can not be available and i assume the goal of this feature was to simplify debugging of KAZE features.
Since we may expect it is mature enough, we will remove these functions (&lt;code&gt;Save_Scale_Space&lt;/code&gt;, &lt;code&gt;Save_Detector_Responses&lt;/code&gt;, &lt;code&gt;Save_Flow_Responses&lt;/code&gt;, &lt;code&gt;Save_Nonlinear_Scale_Space&lt;/code&gt;) from sources.&lt;/p&gt;

&lt;h4 id=&#34;fixing-the-pi-constant&#34;&gt;Fixing the PI constant&lt;/h4&gt;

&lt;p&gt;KAZE uses &lt;code&gt;M_PI&lt;/code&gt; symbol to represent Pi number. We will use &lt;code&gt;CV_PI&lt;/code&gt; replacement instead.&lt;/p&gt;

&lt;h4 id=&#34;cleanup-utils-cpp&#34;&gt;Cleanup utils.cpp&lt;/h4&gt;

&lt;p&gt;Helper file utils.cpp contains auxilar functions that is not used by KAZE directly but rather used for precision esitmation.
We don&amp;rsquo;t need these functions in OpenCV packages. So we say goodbye to following functions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void draw_keypoints(cv::Mat&amp;amp; img, const std::vector&amp;lt;cv::KeyPoint&amp;gt;&amp;amp; kpts);
int save_keypoints(const std::string&amp;amp; outFile,
                   const std::vector&amp;lt;cv::KeyPoint&amp;gt;&amp;amp; kpts,
                   const cv::Mat&amp;amp; desc, bool save_desc);

void matches2points_nndr(const std::vector&amp;lt;cv::KeyPoint&amp;gt;&amp;amp; train,
                         const std::vector&amp;lt;cv::KeyPoint&amp;gt;&amp;amp; query,
                         const std::vector&amp;lt;std::vector&amp;lt;cv::DMatch&amp;gt; &amp;gt;&amp;amp; matches,
                         std::vector&amp;lt;cv::Point2f&amp;gt;&amp;amp; pmatches, float nndr);
void compute_inliers_ransac(const std::vector&amp;lt;cv::Point2f&amp;gt;&amp;amp; matches,
                            std::vector&amp;lt;cv::Point2f&amp;gt;&amp;amp; inliers,
                            float error, bool use_fund);
void compute_inliers_homography(const std::vector&amp;lt;cv::Point2f&amp;gt;&amp;amp; matches,
                                std::vector&amp;lt;cv::Point2f&amp;gt; &amp;amp;inliers,
                                const cv::Mat&amp;amp;H, float min_error);
void draw_inliers(const cv::Mat&amp;amp; img1, const cv::Mat&amp;amp; img2, cv::Mat&amp;amp; img_com,
                  const std::vector&amp;lt;cv::Point2f&amp;gt;&amp;amp; ptpairs);
void draw_inliers(const cv::Mat&amp;amp; img1, const cv::Mat&amp;amp; img2, cv::Mat&amp;amp; img_com,
                  const std::vector&amp;lt;cv::Point2f&amp;gt;&amp;amp; ptpairs, int color);
void read_homography(const std::string&amp;amp; hFile, cv::Mat&amp;amp; H1toN);
void show_input_options_help(int example);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-3-fix-constant-expression-bug-in-compute-derivative-kernels&#34;&gt;Step 3 - Fix constant expression bug in compute_derivative_kernels&lt;/h3&gt;

&lt;p&gt;This is very similar to a bug - a static array get initialized with ksize that is not compile-time defined.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void compute_derivative_kernels(cv::OutputArray _kx, 
                                cv::OutputArray _ky,
                                int dx, int dy, int scale) {

    int ksize = 3 + 2*(scale-1);
    ...
    float kerI[ksize];

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can quickly fix this issue with std::vector:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void compute_derivative_kernels(cv::OutputArray _kx, 
                                cv::OutputArray _ky,
                                int dx, int dy, int scale) {

    int ksize = 3 + 2*(scale-1);
    ...
    std::vector&amp;lt;float&amp;gt; kerI(ksize);

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-5-wrapping-kaze-for-opencv&#34;&gt;Step 5 - Wrapping KAZE for OpenCV&lt;/h3&gt;

&lt;p&gt;OpenCV provides three base types for extending features2d API:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cv::FeaturesDetector&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cv::DescriptorExtractor&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cv::Feature2D&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since KAZE features provides both detector and descriptor extractor features, we will derive
our class from &lt;code&gt;cv::Feature2D&lt;/code&gt;.
First, we should implement helper functions to indicate depth and size of feature descriptor and matcher type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// returns the descriptor size in bytes
int KAZE::descriptorSize() const
{
    return extended ? 128 : 64;
}

// returns the descriptor type
int KAZE::descriptorType() const
{
    return CV_32F;
}

// returns the default norm type
int KAZE::defaultNorm() const
{
    return NORM_L2;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to make OpenCV happy we need to implement three virtual functions from Feature2D: &lt;code&gt;detectImpl&lt;/code&gt;, &lt;code&gt;computeImpl&lt;/code&gt;
and &lt;code&gt;operator()&lt;/code&gt; also known as &lt;strong&gt;detectAndCompute&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;step-6-detection-of-kaze-keypoints&#34;&gt;Step 6 - Detection of KAZE keypoints:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void KAZE::detectImpl(InputArray image, std::vector&amp;lt;KeyPoint&amp;gt;&amp;amp; keypoints, InputArray mask) const
{
    Mat img = image.getMat();
    if (img.type() != CV_8UC1)
        cvtColor(image, img, COLOR_BGR2GRAY);

    Mat img1_32;
    img.convertTo(img1_32, CV_32F, 1.0 / 255.0, 0);

    KAZEOptions options;
    options.img_width = img.cols;
    options.img_height = img.rows;
    options.extended = extended;

    KAZEFeatures impl(options);
    impl.Create_Nonlinear_Scale_Space(img1_32);
    impl.Feature_Detection(keypoints);

    if (!mask.empty())
    {
        cv::KeyPointsFilter::runByPixelsMask(keypoints, mask.getMat());
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Please note that we conver input image to grayscale normalized to [0;1] floating-point image.
This is a requirement of KAZE algorithm.&lt;/p&gt;

&lt;h3 id=&#34;step-7-extraction-of-kaze-descriptors&#34;&gt;Step 7 - Extraction of KAZE descriptors:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void KAZE::computeImpl(InputArray image, 
                       std::vector&amp;lt;KeyPoint&amp;gt;&amp;amp; keypoints, 
                       OutputArray descriptors) const
{
    cv::Mat img = image.getMat();
    if (img.type() != CV_8UC1)
        cvtColor(image, img, COLOR_BGR2GRAY);

    Mat img1_32;
    img.convertTo(img1_32, CV_32F, 1.0 / 255.0, 0);

    cv::Mat&amp;amp; desc = descriptors.getMatRef();

    KAZEOptions options;
    options.img_width = img.cols;
    options.img_height = img.rows;
    options.extended = extended;

    KAZEFeatures impl(options);
    impl.Create_Nonlinear_Scale_Space(img1_32);
    impl.Feature_Description(keypoints, desc);

    CV_Assert(!desc.rows || desc.cols == descriptorSize() &amp;amp;&amp;amp; 
              &amp;quot;Descriptor size does not match expected&amp;quot;);
    CV_Assert(!desc.rows || (desc.type() &amp;amp; descriptorType()) &amp;amp;&amp;amp; 
              &amp;quot;Descriptor type does not match expected&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Two asserts at the end of the function to ensure that KAZE returns consistent with &lt;code&gt;descriptorType()&lt;/code&gt; and &lt;code&gt;descriptorSize()&lt;/code&gt; results.&lt;/p&gt;

&lt;h3 id=&#34;step-8-detection-and-extraction-at-single-call&#34;&gt;Step 8 - Detection and extraction at single call:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;void KAZE::operator()(InputArray image, InputArray mask,
    std::vector&amp;lt;KeyPoint&amp;gt;&amp;amp; keypoints,
    OutputArray descriptors,
    bool useProvidedKeypoints) const
{
    cv::Mat img = image.getMat();
    if (img.type() != CV_8UC1)
        cvtColor(image, img, COLOR_BGR2GRAY);

    Mat img1_32;
    img.convertTo(img1_32, CV_32F, 1.0 / 255.0, 0);

    cv::Mat&amp;amp; desc = descriptors.getMatRef();

    KAZEOptions options;
    options.img_width = img.cols;
    options.img_height = img.rows;
    options.extended = extended;

    KAZEFeatures impl(options);
    impl.Create_Nonlinear_Scale_Space(img1_32);

    if (!useProvidedKeypoints)
    {
        impl.Feature_Detection(keypoints);
    }

    if (!mask.empty())
    {
        cv::KeyPointsFilter::runByPixelsMask(keypoints, mask.getMat());
    }

    impl.Feature_Description(keypoints, desc);

    CV_Assert(!desc.rows || desc.cols == descriptorSize() &amp;amp;&amp;amp; 
              &amp;quot;Descriptor size does not match expected&amp;quot;);
    CV_Assert(!desc.rows || (desc.type() &amp;amp; descriptorType()) &amp;amp;&amp;amp; 
              &amp;quot;Descriptor type does not match expected&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-9-algorithm-configuration&#34;&gt;Step 9 - Algorithm configuration&lt;/h3&gt;

&lt;p&gt;OpenCV provides an option to create and configure algorithm in runtime by it&amp;rsquo;s name. This is done by using special CV_INIT_ALGORITHM marco, that initialize all OpenCV algorithms during startup. Using this macro we register new KAZE and AKAZE algorithms under Feature2D module and expose additional properties that user can change in runtime:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;features2d_init.cpp&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;CV_INIT_ALGORITHM(KAZE, &amp;quot;Feature2D.KAZE&amp;quot;,
                  obj.info()-&amp;gt;addParam(obj, &amp;quot;extended&amp;quot;, obj.extended))
    
CV_INIT_ALGORITHM(AKAZE, &amp;quot;Feature2D.AKAZE&amp;quot;,
                  obj.info()-&amp;gt;addParam(obj, &amp;quot;descriptor_channels&amp;quot;, obj.descriptor_channels);
                  obj.info()-&amp;gt;addParam(obj, &amp;quot;descriptor&amp;quot;, obj.descriptor);
                  obj.info()-&amp;gt;addParam(obj, &amp;quot;descriptor_size&amp;quot;, obj.descriptor_size))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Please note, that KAZE and AKAZE has much more properties. They (and documentation for them) will be added later.&lt;/p&gt;

&lt;h3 id=&#34;step-10-unit-tests&#34;&gt;Step 10 - Unit tests&lt;/h3&gt;

&lt;p&gt;There is a nice feature detectors and descriptors unit testing system in OpenCV. Using it is very simple, but it performs many sanity checks and validates both parths of Features2D API.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;test_keypoints.cpp&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;All we need to do is to add a new unit test suites:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;TEST(Features2d_Detector_Keypoints_KAZE, validation)
{
    CV_FeatureDetectorKeypointsTest test(Algorithm::create&amp;lt;FeatureDetector&amp;gt;(&amp;quot;Feature2D.KAZE&amp;quot;));
    test.safe_run();
}

TEST(Features2d_Detector_Keypoints_AKAZE, validation)
{
    CV_FeatureDetectorKeypointsTest test(Algorithm::create&amp;lt;FeatureDetector&amp;gt;(&amp;quot;Feature2D.AKAZE&amp;quot;));
    test.safe_run();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition to simple checks that our implementation does some job, there are more sophisticaed tests to verify rotation and scale invariance of the computed features.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;test_rotation_and_scale_invariance.cpp&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;TEST(Features2d_ScaleInvariance_Detector_KAZE, regression)
{
    DetectorScaleInvarianceTest test(Algorithm::create&amp;lt;FeatureDetector&amp;gt;(&amp;quot;Feature2D.KAZE&amp;quot;),
        0.08f,
        0.49f);
    test.safe_run();
}

TEST(Features2d_ScaleInvariance_Detector_AKAZE, regression)
{
    DetectorScaleInvarianceTest test(Algorithm::create&amp;lt;FeatureDetector&amp;gt;(&amp;quot;Feature2D.AKAZE&amp;quot;),
        0.08f,
        0.49f);
    test.safe_run();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-11-enabling-multithreading&#34;&gt;Step 11 - Enabling multithreading&lt;/h3&gt;

&lt;p&gt;Both, feature detection and extraction stage can be made faster by using multi-threading.
Fortunately, AKAZE designed very clear and one may are find OpenMP instructions in critical sections:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#pragma omp parallel for
for (int i = 0; i &amp;lt; (int)(kpts.size()); i++) {
    Compute_Main_Orientation(kpts[i]);
    Get_SURF_Descriptor_64(kpts[i], desc.ptr&amp;lt;float&amp;gt;(i));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But OpenCV uses abstraction layer for multithreading called &lt;code&gt;cv::parallel_for_&lt;/code&gt;. Personally I think it&amp;rsquo;s very wise architectural desing decision since it allows to get rid of specific cavetas for particular threading backends (OpenCV, TBB, Concurrency, GCD, etc). You can read more about using cv::parallel&lt;em&gt;for&lt;/em&gt; in one of my &lt;a href=&#34;https://example.com/articles/2012-11-06-maximizing-performance-grayscale-color-conversion-using-neon-and-cvparallel_for/&#34;&gt;previous posts&lt;/a&gt; or visit &lt;a href=&#34;http://answers.opencv.org/question/3730/how-to-use-parallel_for/&#34;&gt;OpenCV documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For instance, here is how to parallelize building of the nonlinear scale space for AKAZE. The old version of OpenMP version of &lt;code&gt;Compute_Multiscale_Derivatives&lt;/code&gt; function looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;/**
 * @brief This method computes the multiscale derivatives for the nonlinear scale space
 */
void AKAZEFeatures::Compute_Multiscale_Derivatives(void) {

    #pragma omp parallel for
    for (int i = 0; i &amp;lt; (int)(evolution_.size()); i++) {

        float ratio = pow(2.f, (float)evolution_[i].octave);
        int sigma_size_ = fRound(evolution_[i].esigma*options_.derivative_factor / ratio);

        compute_scharr_derivatives(evolution_[i].Lsmooth, evolution_[i].Lx, 1, 0, sigma_size_);
        compute_scharr_derivatives(evolution_[i].Lsmooth, evolution_[i].Ly, 0, 1, sigma_size_);
        compute_scharr_derivatives(evolution_[i].Lx, evolution_[i].Lxx, 1, 0, sigma_size_);
        compute_scharr_derivatives(evolution_[i].Ly, evolution_[i].Lyy, 0, 1, sigma_size_);
        compute_scharr_derivatives(evolution_[i].Lx, evolution_[i].Lxy, 0, 1, sigma_size_);

        evolution_[i].Lx = evolution_[i].Lx*((sigma_size_));
        evolution_[i].Ly = evolution_[i].Ly*((sigma_size_));
        evolution_[i].Lxx = evolution_[i].Lxx*((sigma_size_)*(sigma_size_));
        evolution_[i].Lxy = evolution_[i].Lxy*((sigma_size_)*(sigma_size_));
        evolution_[i].Lyy = evolution_[i].Lyy*((sigma_size_)*(sigma_size_));
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using &lt;code&gt;cv::parallel_for_&lt;/code&gt; we introduce an &amp;lsquo;invoker&amp;rsquo; function object that perform a discrete piece of job on small subset of whole data. Threading API does all job on scheduling multithreaded execution among worker threads:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;class MultiscaleDerivativesInvoker : public cv::ParallelLoopBody
{
public:
    explicit MultiscaleDerivativesInvoker(std::vector&amp;lt;TEvolution&amp;gt;&amp;amp; ev, const AKAZEOptions&amp;amp; opt) 
    : evolution_(ev)
    , options_(opt)
    {
    }


    void operator()(const cv::Range&amp;amp; range) const 
    {
        for (int i = range.start; i &amp;lt; range.end; i++)
        {
            float ratio = pow(2.f, (float)evolution_[i].octave);
            int sigma_size_ = fRound(evolution_[i].esigma * options_.derivative_factor / ratio);

            compute_scharr_derivatives(evolution_[i].Lsmooth, evolution_[i].Lx, 1, 0, sigma_size_);
            compute_scharr_derivatives(evolution_[i].Lsmooth, evolution_[i].Ly, 0, 1, sigma_size_);
            compute_scharr_derivatives(evolution_[i].Lx, evolution_[i].Lxx, 1, 0, sigma_size_);
            compute_scharr_derivatives(evolution_[i].Ly, evolution_[i].Lyy, 0, 1, sigma_size_);
            compute_scharr_derivatives(evolution_[i].Lx, evolution_[i].Lxy, 0, 1, sigma_size_);

            evolution_[i].Lx = evolution_[i].Lx*((sigma_size_));
            evolution_[i].Ly = evolution_[i].Ly*((sigma_size_));
            evolution_[i].Lxx = evolution_[i].Lxx*((sigma_size_)*(sigma_size_));
            evolution_[i].Lxy = evolution_[i].Lxy*((sigma_size_)*(sigma_size_));
            evolution_[i].Lyy = evolution_[i].Lyy*((sigma_size_)*(sigma_size_));
        }
    }

private:
    mutable std::vector&amp;lt;TEvolution&amp;gt; &amp;amp; evolution_;
    AKAZEOptions                      options_;
};

/**
 * @brief This method computes the multiscale derivatives for the nonlinear scale space
 */
void AKAZEFeatures::Compute_Multiscale_Derivatives(void) {

    cv::parallel_for_(cv::Range(0, evolution_.size()), 
                      MultiscaleDerivativesInvoker(evolution_, options_));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a name=&#34;results&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kaze-performance&#34;&gt;KAZE performance&lt;/h1&gt;

&lt;p&gt;After integration I ran KAZE and AKAZE using &lt;a href=&#34;https://github.com/BloodAxe/OpenCV-Features-Comparison&#34;&gt;feature descriptor estimation framework&lt;/a&gt; to see how they perform. I was really impressed about matching precision on rotation and scaling tests. Look at these self-explaining charts where &lt;strong&gt;AKAZE beats all other features&lt;/strong&gt;!&lt;/p&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;//ajax.googleapis.com/ajax/static/modules/gviz/1.0/chart.js&#34;&gt;
{&#34;dataSourceUrl&#34;:&#34;//docs.google.com/spreadsheet/tq?key=0AuBBvmQlA4pfdGhzcWVFUWhqZkdzb01HTnIwQllIQlE&amp;transpose=0&amp;headers=1&amp;range=A41%3AG78&amp;gid=0&amp;pub=1&#34;,&#34;options&#34;:{&#34;titleTextStyle&#34;:{&#34;bold&#34;:true,&#34;color&#34;:&#34;#000&#34;,&#34;fontSize&#34;:16},&#34;curveType&#34;:&#34;&#34;,&#34;animation&#34;:{&#34;duration&#34;:0},&#34;width&#34;:900,&#34;lineWidth&#34;:2,&#34;hAxis&#34;:{&#34;useFormatFromData&#34;:true,&#34;title&#34;:&#34;Rotation degree&#34;,&#34;minValue&#34;:null,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;maxValue&#34;:null},&#34;vAxes&#34;:[{&#34;useFormatFromData&#34;:true,&#34;title&#34;:&#34;Percent of correct matches&#34;,&#34;minValue&#34;:null,&#34;logScale&#34;:false,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;maxValue&#34;:null},{&#34;useFormatFromData&#34;:true,&#34;minValue&#34;:null,&#34;logScale&#34;:false,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;maxValue&#34;:null}],&#34;booleanRole&#34;:&#34;certainty&#34;,&#34;title&#34;:&#34;Rotation test&#34;,&#34;height&#34;:600,&#34;interpolateNulls&#34;:false,&#34;domainAxis&#34;:{&#34;direction&#34;:1},&#34;legend&#34;:&#34;right&#34;,&#34;focusTarget&#34;:&#34;series&#34;,&#34;annotations&#34;:{&#34;domain&#34;:{}},&#34;useFirstColumnAsDomain&#34;:true,&#34;tooltip&#34;:{&#34;trigger&#34;:&#34;none&#34;}},&#34;state&#34;:{},&#34;view&#34;:{},&#34;isDefaultVisualization&#34;:false,&#34;chartType&#34;:&#34;LineChart&#34;,&#34;chartName&#34;:&#34;Chart 1&#34;}
&lt;/script&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;//ajax.googleapis.com/ajax/static/modules/gviz/1.0/chart.js&#34;&gt;
{&#34;dataSourceUrl&#34;:&#34;//docs.google.com/spreadsheet/tq?key=0AuBBvmQlA4pfdGhzcWVFUWhqZkdzb01HTnIwQllIQlE&amp;transpose=0&amp;headers=1&amp;range=A80%3AG98&amp;gid=0&amp;pub=1&#34;,&#34;options&#34;:{&#34;titleTextStyle&#34;:{&#34;bold&#34;:true,&#34;color&#34;:&#34;#000&#34;,&#34;fontSize&#34;:16},&#34;curveType&#34;:&#34;&#34;,&#34;animation&#34;:{&#34;duration&#34;:0},&#34;width&#34;:900,&#34;lineWidth&#34;:2,&#34;hAxis&#34;:{&#34;useFormatFromData&#34;:true,&#34;title&#34;:&#34;Scale factor&#34;,&#34;minValue&#34;:null,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;maxValue&#34;:null},&#34;vAxes&#34;:[{&#34;useFormatFromData&#34;:true,&#34;title&#34;:&#34;Percent of correct matches&#34;,&#34;minValue&#34;:null,&#34;logScale&#34;:false,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;maxValue&#34;:null},{&#34;useFormatFromData&#34;:true,&#34;minValue&#34;:null,&#34;logScale&#34;:false,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;maxValue&#34;:null}],&#34;title&#34;:&#34;Scale invariance&#34;,&#34;booleanRole&#34;:&#34;certainty&#34;,&#34;height&#34;:600,&#34;legend&#34;:&#34;right&#34;,&#34;focusTarget&#34;:&#34;series&#34;,&#34;annotations&#34;:{&#34;domain&#34;:{}},&#34;useFirstColumnAsDomain&#34;:true,&#34;tooltip&#34;:{&#34;trigger&#34;:&#34;none&#34;}},&#34;state&#34;:{},&#34;view&#34;:{},&#34;isDefaultVisualization&#34;:false,&#34;chartType&#34;:&#34;LineChart&#34;,&#34;chartName&#34;:&#34;Chart 1&#34;}
&lt;/script&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;//ajax.googleapis.com/ajax/static/modules/gviz/1.0/chart.js&#34;&gt;
{&#34;dataSourceUrl&#34;:&#34;//docs.google.com/spreadsheet/tq?key=0AuBBvmQlA4pfdGhzcWVFUWhqZkdzb01HTnIwQllIQlE&amp;transpose=0&amp;headers=1&amp;range=A30%3AG39&amp;gid=0&amp;pub=1&#34;,&#34;options&#34;:{&#34;titleTextStyle&#34;:{&#34;bold&#34;:true,&#34;color&#34;:&#34;#000&#34;,&#34;fontSize&#34;:16},&#34;series&#34;:{&#34;0&#34;:{&#34;hasAnnotations&#34;:true}},&#34;curveType&#34;:&#34;&#34;,&#34;animation&#34;:{&#34;duration&#34;:0},&#34;width&#34;:900,&#34;lineWidth&#34;:2,&#34;hAxis&#34;:{&#34;title&#34;:&#34;Half of the gaussian kernel size&#34;,&#34;useFormatFromData&#34;:true,&#34;minValue&#34;:null,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;maxValue&#34;:null},&#34;vAxes&#34;:[{&#34;title&#34;:&#34;Percent of correct matches&#34;,&#34;useFormatFromData&#34;:true,&#34;minValue&#34;:null,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;logScale&#34;:false,&#34;maxValue&#34;:null},{&#34;useFormatFromData&#34;:true,&#34;minValue&#34;:null,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;logScale&#34;:false,&#34;maxValue&#34;:null}],&#34;booleanRole&#34;:&#34;certainty&#34;,&#34;title&#34;:&#34;Robustness to blur&#34;,&#34;height&#34;:600,&#34;legend&#34;:&#34;right&#34;,&#34;focusTarget&#34;:&#34;series&#34;,&#34;useFirstColumnAsDomain&#34;:true,&#34;tooltip&#34;:{&#34;trigger&#34;:&#34;none&#34;}},&#34;state&#34;:{},&#34;view&#34;:{},&#34;isDefaultVisualization&#34;:true,&#34;chartType&#34;:&#34;LineChart&#34;,&#34;chartName&#34;:&#34;Chart 3&#34;}
&lt;/script&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;//ajax.googleapis.com/ajax/static/modules/gviz/1.0/chart.js&#34;&gt;
{&#34;dataSourceUrl&#34;:&#34;//docs.google.com/spreadsheet/tq?key=0AuBBvmQlA4pfdGhzcWVFUWhqZkdzb01HTnIwQllIQlE&amp;transpose=0&amp;headers=1&amp;range=A2%3AG28&amp;gid=0&amp;pub=1&#34;,&#34;options&#34;:{&#34;titleTextStyle&#34;:{&#34;bold&#34;:true,&#34;color&#34;:&#34;#000&#34;,&#34;fontSize&#34;:16},&#34;curveType&#34;:&#34;&#34;,&#34;animation&#34;:{&#34;duration&#34;:0},&#34;width&#34;:900,&#34;lineWidth&#34;:2,&#34;hAxis&#34;:{&#34;title&#34;:&#34;Change of brightness&#34;,&#34;useFormatFromData&#34;:true,&#34;minValue&#34;:null,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;maxValue&#34;:null},&#34;vAxes&#34;:[{&#34;title&#34;:&#34;Percent of correct matches&#34;,&#34;useFormatFromData&#34;:false,&#34;formatOptions&#34;:{&#34;source&#34;:&#34;inline&#34;},&#34;minValue&#34;:null,&#34;format&#34;:&#34;0.##&#34;,&#34;logScale&#34;:false,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;maxValue&#34;:null},{&#34;useFormatFromData&#34;:true,&#34;minValue&#34;:null,&#34;viewWindow&#34;:{&#34;max&#34;:null,&#34;min&#34;:null},&#34;maxValue&#34;:null}],&#34;title&#34;:&#34;Brightness invariance&#34;,&#34;booleanRole&#34;:&#34;certainty&#34;,&#34;height&#34;:600,&#34;legend&#34;:&#34;right&#34;,&#34;focusTarget&#34;:&#34;series&#34;,&#34;useFirstColumnAsDomain&#34;:true,&#34;tooltip&#34;:{&#34;trigger&#34;:&#34;none&#34;}},&#34;state&#34;:{},&#34;view&#34;:{},&#34;isDefaultVisualization&#34;:true,&#34;chartType&#34;:&#34;LineChart&#34;,&#34;chartName&#34;:&#34;Chart 4&#34;}
&lt;/script&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/BloodAxe/opencv/tree/kaze&#34;&gt;KAZE Integration branch on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.robesafe.com/personal/pablo.alcantarilla/code/kaze_features_1.6.0.tar.gz&#34;&gt;KAZE 1.6 implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.robesafe.com/personal/pablo.alcantarilla/code/akaze_features_1.1.0.tar.gz&#34;&gt;AKAZE 1.2 implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>KAZE 1.5.1</title>
      <link>https://example.com/2013-06-17-kaze-1-5-1/</link>
      <pubDate>Mon, 17 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2013-06-17-kaze-1-5-1/</guid>
      <description>&lt;div class=&#34;alert alert-danger&#34;&gt;
    &lt;p class=&#34;lead&#34;&gt;This post is outdated.&lt;/p&gt;
    &lt;p&gt;
        Please, visit updated post: 
        &lt;a href=&#34;https://example.com/articles/kaze-1.6-in-opencv/&#34;&gt;Integration of KAZE 1.6 in OpenCV&lt;/a&gt;
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;A new version of KAZE features has been integrated my private fork of OpenCV (You can find it&amp;rsquo;s here: &lt;a href=&#34;https://github.com/BloodAxe/opencv/tree/kaze-features&#34;&gt;https://github.com/BloodAxe/opencv/tree/kaze-features&lt;/a&gt;). We&amp;rsquo;re on the way to make pull-request and integrate KAZE features to official OpenCV repository.&lt;/p&gt;

&lt;p&gt;There only few things are left:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Include KAZE into features2d unit tests.&lt;/li&gt;
&lt;li&gt;Rewrite KAZE to support OpenCV threading API.&lt;/li&gt;
&lt;li&gt;Expose adjustable parameters of KAZE algorithm.&lt;/li&gt;
&lt;li&gt;Do code cleanup and documentation for pull request.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I think we (Pablo, KAZE author) and me complete these steps in a near future. During our hard work you can enjoy these nice charts that shows how awesome KAZE features are:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;chart_1.png&#34; alt=&#34;KAZE 1.5.1 Rotation Test&#34; /&gt; &lt;img src=&#34;chart_1-1.png&#34; alt=&#34;KAZE 1.5.1 Scale Test&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Porting KAZE features to OpenCV</title>
      <link>https://example.com/2013-03-17-porting-kaze-features-to-opencv/</link>
      <pubDate>Sun, 17 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2013-03-17-porting-kaze-features-to-opencv/</guid>
      <description>

&lt;p&gt;Recently i came across the publications to a new features called &lt;a href=&#34;http://www.robesafe.com/personal/pablo.alcantarilla/kaze.html&#34;&gt;KAZE&lt;/a&gt; (Japanesee work meaning &amp;ldquo;Wind&amp;rdquo;). They interested me, because KAZE authors provided very promising evalutaion results and i decided to evaluate them too using my OpenCV features comparison tool. Fortunately KAZE algorithm is based on OpenCV, so it was not too hard to wrap KAZE features implementatino to cv::Feature2D API.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34; /&gt;&lt;/p&gt;

&lt;div class=&#34;alert alert-danger&#34;&gt;
    &lt;p class=&#34;lead&#34;&gt;This post is outdated.&lt;/p&gt;
    &lt;p&gt;
        Please, visit updated post: 
        &lt;a href=&#34;https://example.com/articles/kaze-1.6-in-opencv/&#34;&gt;Integration of KAZE 1.6 in OpenCV&lt;/a&gt;
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Impatient readers:&lt;/em&gt;&lt;/strong&gt; you can grab the most recent version of KAZE port to OpenCV here: &lt;a href=&#34;https://github.com/BloodAxe/opencv/tree/kaze-features&#34;&gt;kaze-features&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Short video demonstration:&lt;/p&gt;

&lt;h2 id=&#34;kaze-opencv&#34;&gt;KAZE &amp;amp; OpenCV&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m not gonna describe the algorithm by itself or implementation details, you may wish to read the &lt;a href=&#34;http://www.robesafe.com/personal/pablo.alcantarilla/papers/Alcantarilla12eccv.pdf&#34;&gt;original paper&lt;/a&gt; or look at the &lt;a href=&#34;http://www.robesafe.com/personal/pablo.alcantarilla/code/kaze_features_1_4.tar&#34;&gt;code&lt;/a&gt; if you have enough mana.&lt;/p&gt;

&lt;p&gt;If not - don&amp;rsquo;t worry, using KAZE features is easy like any other feature algorithm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cv::Mat image = /* ... your image goes here ... */

cv::KAZE kazeFeatures;
kazeFeatures(image, keypoints, descriptors);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;KAZE algorithm is not included in the official OpenCV repository yet&lt;/em&gt;&lt;/strong&gt;. It exists only in my fork of the opencv on github. You can clone it and build OpenCV with KAZE algorithm by yourself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/BloodAxe/opencv kaze-features
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;m going to cooperate with KAZE authors and help them to include KAZE algorithm to OpenCV library. So &lt;a href=&#34;https://github.com/BloodAxe/opencv/tree/kaze-features&#34;&gt;kaze-features&lt;/a&gt; branch in my opencv fork will be updated constantly with more recent versions.&lt;/p&gt;

&lt;h2 id=&#34;kaze-estimation&#34;&gt;KAZE Estimation&lt;/h2&gt;

&lt;p&gt;I did a comparison of KAZE features with other descriptors and here are the results that i got using my &lt;a href=&#34;https://github.com/BloodAxe/OpenCV-Features-Comparison&#34;&gt;OpenCV Comparison Tool&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Rotation-Test.png&#34; alt=&#34;Rotation invariance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Scale-Test.png&#34; alt=&#34;Scale invariance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Brightness-Test.png&#34; alt=&#34;Brightness invariance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Blur-Test.png&#34; alt=&#34;Blur invariance&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;strike&gt;Regarding to the results of estimation the KAZE algorithm is much more robust than state of the art algorithms like SURF and FREAK or BRISK.&lt;/strike&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;It&amp;rsquo;s amazing! In all tests KAZE features shows best results !!!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Currently KAZE features are somewhat slow. I contacted with one of it&amp;rsquo;s authors and he assured me that they are working on this problem so i beleive they will improve their performance in the near future.&lt;/p&gt;

&lt;h2 id=&#34;licensing&#34;&gt;Licensing&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve contacted with one of authors of KAZE features and asked him about code license and usage rights. Here it is:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Pablo Fernandez Alcantarilla&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The code is released under BSD license. So it is completely free. That was my initial goal, I wanted to do something open source better than SIFT, SURF so people don&amp;rsquo;t need to mess up with the patents.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Feature descriptor comparison report</title>
      <link>https://example.com/2011-08-19-feature-descriptor-comparison-report/</link>
      <pubDate>Fri, 19 Aug 2011 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2011-08-19-feature-descriptor-comparison-report/</guid>
      <description>

&lt;p&gt;Sharing my research work of behavior of several types of feature descriptors. This article is an update of old &amp;ldquo;&lt;a href=&#34;http://computer-vision-talks.com/2011/01/comparison-of-feature-descriptors/&#34; title=&#34;Comparison of feature descriptors&#34;&gt;Comparison of feature descriptors&lt;/a&gt;&amp;rdquo; post. I&amp;rsquo;ve added a brand new ORB feature descriptor to the test suite, also SIFT descriptor included as well. And a new version of LAZY descriptor present in this test too.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;For this test i have written special test framework, which allows me to easily add the new kind of descriptors and test cases and generate report data in CSV-like format. Than i upload it in Google docs and create this awesome charts. Five quality and one performance test was done for each kind of descriptor.&lt;/p&gt;

&lt;h1 id=&#34;test-cases&#34;&gt;Test cases&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Rotation test - this test shows how the feature descriptor depends on feature orientation.&lt;/li&gt;
&lt;li&gt;Scaling test - this test shows how the feature descriptor depends on feature size.&lt;/li&gt;
&lt;li&gt;Blur test - this test shows how the feature descriptor is robust against blur.&lt;/li&gt;
&lt;li&gt;Lighting test - this test shows how the feature descriptor is robust against lighting.&lt;/li&gt;
&lt;li&gt;Pattern detection test - this test performs detection of planar object (image) on the real video. In contrast to the synthetic tests, this test gives a real picture of the overall stability of the particular descriptor.&lt;/li&gt;
&lt;li&gt;Performance test is a measurement of description extraction time.
All quality tests works in similar way. Using a given source image we generate a synthetic test data: transformed images corresponding feature points. The transformation algorithm depends on the particular test. For the rotation test case, it&amp;rsquo;s the rotation of the source image around it&amp;rsquo;s center for 360 degrees, for scaling - it&amp;rsquo;s resizing of image from 0.25X to 2x size of original. Blur test uses gaussian blur with several steps and the lighting test changes the overall picture brightness. The pattern detection test deserves a special attention. This test is done on very complex and noisy &lt;a href=&#34;http://www.youtube.com/watch?v=58qTQ_R4IU8&amp;amp;feature=player_embedded&#34;&gt;video sequence&lt;/a&gt;. So it&amp;rsquo;s challenging task for any feature descriptor algorithm to demonstrate a good results in this test. The metric for all quality tests is the percent of correct matches between the source image and the transformed one. Since we use planar object, we can easily select the inliers from all matches using the homography estimation. I use OpenCV&amp;rsquo;s function cvFindHomography for this. This metric gives very good and stable results. I do no outlier detection of matches before homography estimation because this will affect the results in unexpected way. The matching of descriptors is done via brute-force matching from the OpenCV.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;rotation-test&#34;&gt;Rotation test&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;rotation_invariantness_thumb.png&#34; alt=&#34;Descriptor&#39;s invariance to rotation summary report&#34; title=&#34;Descriptor&#39;s invariance to rotation summary report&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this test i obtain pretty expectable results, because all descriptors are rotation invariant expect the BRIEF. Slight changes in stability can be explained by the feature orientation calculation algorithm and descriptor nature. A detailed study of why the descriptor behaves exactly as it is, takes time and effort. It&amp;rsquo;s a topic for another article. Maybe later on&amp;hellip;.&lt;/p&gt;

&lt;h1 id=&#34;scaling-test&#34;&gt;Scaling test&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;scale_invariantness_thumb.png&#34; alt=&#34;Descriptor&#39;s invariance to scaling summary report&#34; title=&#34;Descriptor&#39;s invariance to scaling summary report&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SURF and SIFT descriptors demonstrate us very good stability in this test because they do expensive keypoint size calculation. Other descriptors uses fixed-size descriptor and you can see what it leads to. Currently for LAZY descriptor i do not have separate LAZY feature detector (i use ORB detector for tests) but I&amp;rsquo;m thinking on lightweight feature detector with feature size calculation, because it&amp;rsquo;s a must-have feature. Actually, scale invariance is much more important rather than precise orientation calculation.&lt;/p&gt;

&lt;h1 id=&#34;blur-test&#34;&gt;Blur test&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;blur_invariantness_thumb.png&#34; alt=&#34;Descriptor&#39;s invariance to blur summary report&#34; title=&#34;Descriptor&#39;s invariance to blur summary report&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this test i tried to simulate the motion blur which can occurs if camera moves suddenly. All descriptors demonstrate good results in this test. By good I mean that the more blur size is applied the less percent of correct matches is obtained. Which is expected behavior.&lt;/p&gt;

&lt;h1 id=&#34;lighting-test&#34;&gt;Lighting test&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;lighting_invariantness_thumb.png&#34; alt=&#34;Descriptor&#39;s invariance to lighting summary report&#34; title=&#34;Descriptor&#39;s invariance to lighting summary report&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In lighting test the transformed images differs only in overall image brightness. All kinds of descriptors works well in this case. The major reason is that all descriptors extracted normalized, e.g the norm_2 of the descriptor vector equals 1. This normalization makes descriptor invariant to brightness changes.&lt;/p&gt;

&lt;h1 id=&#34;pattern-detection-on-real-video&#34;&gt;Pattern detection on real video&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;pattern_detection_thumb.png&#34; alt=&#34;Pattern detection test&#34; title=&#34;Pattern detection test&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Detection of the object on real video is the most complex task since ground truth contains rotation, scaling and motion blur. Also other objects are also present. And finally, its not HD quality. These conditions are dictated by the actual conditions of application of computer vision. As you can see on diagram, the SIFT and SURF descriptors gives the best results, nevertheless they are far away from ideal, its quite enough for such challenging video. Unfortunately, scale-covariant descriptors show very bad results in this test because pattern image appears in 1:1 scale only at the beginning of the video (The spike near frame 20). On the rest of the video sequence target object moves from the camera back and scale-covariant descriptors cant handle this situation.&lt;/p&gt;

&lt;h1 id=&#34;performance-summary&#34;&gt;Performance summary&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Comparison of the OpenCVs feature detection algorithms  II</title>
      <link>https://example.com/2011-07-13-comparison-of-the-opencv-feature-detection-algorithms/</link>
      <pubDate>Wed, 13 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2011-07-13-comparison-of-the-opencv-feature-detection-algorithms/</guid>
      <description>

&lt;p&gt;Here is an update of half year-old post about differences between existing feature detection algorithms. Original article can be found here: &lt;a href=&#34;http://computer-vision-talks.com/2011/01/comparison-of-the-opencvs-feature-detection-algorithms-2/&#34;&gt;Comparison of the OpenCV&amp;rsquo;s feature detection algorithms  I&lt;/a&gt;. I decided to update this comparison report since many things happened: OpenCV 2.3.1 has been released and the new type of feature detector (ORB feature detector) has been introduced. ORB is an acronym of Oriented-BRIEF and uses modified to compute orientation FAST detector for detection stage and BRIEF for descriptor extraction. In this article I will test newcomer on the same test cases (the same hardware and input images) using the latest OpenCV build (2.3.1, revision 6016).&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h4 id=&#34;test-images&#34;&gt;Test images&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;mandril_thumb.jpg&#34; alt=&#34;mandril&#34; title=&#34;mandril&#34; /&gt; &lt;img src=&#34;barbara_thumb.jpg&#34; alt=&#34;barbara&#34; title=&#34;barbara&#34; /&gt; &lt;img src=&#34;lena_thumb.jpg&#34; alt=&#34;lena&#34; title=&#34;lena&#34; /&gt; &lt;img src=&#34;peppers_thumb.jpg&#34; alt=&#34;peppers&#34; title=&#34;peppers&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;estimation-criteria&#34;&gt;Estimation criteria&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Speed per frame&lt;/strong&gt;  absolute total time in milliseconds spent to the feature detection of the single frame.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Speed per keypoint&lt;/strong&gt;  detection time for single keypoint. Evaluated as total time divided to number of detected keypoints. Helps us to estimate how cheap the detection actually is.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Percent of tracked features&lt;/strong&gt;  percent of successfully tracked features from original to transformed image. In ideal situation, value of this mark should be near 100%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Average tracking error&lt;/strong&gt;  this is the average distance between position of tracked feature and their calculated position on transformed frame. This mark indicates accuracy of the feature detection. Large values indicates large number of false positive tracking or drift of feature point among frames.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Features count deviation&lt;/strong&gt;  difference between number of keypoints on reference frame and number of detected keypoints on transformed frame divided by number of keypoints on reference frame. Helps estimate how slight exposure changes affects feature detection.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Average detection error&lt;/strong&gt;  average distance between nearest keypoints on original and transformed frame.&lt;/p&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;

&lt;p&gt;For each image I obtain a five measurements per each detection algorithm. Then I calculate average for each kind of measurement and here are the results: &lt;img src=&#34;average-number-of-detected-keypoints_thumb.png&#34; alt=&#34;average-number-of-detected-keypoints&#34; title=&#34;average-number-of-detected-keypoints&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1  Average number of detected keypoints&lt;/p&gt;

&lt;p&gt;As you can see from Figure 1 FAST detector finds a lot of feature points as usual. You can manipulate the numbers of detected points by adjusting detection threshold. Other detectors detects much less feature points but their quality is significantly better. And ORB detector seems to have the fixed maximum number of features detected because if found exactly 702 features on each image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;percent-of-tracked-features_thumb.png&#34; alt=&#34;percent-of-tracked-features&#34; title=&#34;percent-of-tracked-features&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2  Percent of tracked features&lt;/p&gt;

&lt;p&gt;Tracking test looks very very strange  the works result show MSER detector which is expectable, since it returns centers of stable extremum regions. But the GoodFeaturesToTrack detector also shows very bad results in comparison with other types of detectors. Its annoying because I expected this detector to be the best since its name is Good Features To Track. But here is the facts  the best result of tracking you can achieve with SURF, STAR and new ORB feature detector.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;average-detection-time_thumb.png&#34; alt=&#34;average-detection-time&#34; title=&#34;average-detection-time&#34; /&gt;Figure 3  Average detection time&lt;/p&gt;

&lt;p&gt;As usual  SIFT and SURF very slow. Other detectors are relatively fast. Feature detection with ORB detector takes ~25 ms for 512x512 image which is good because it calculates feature orientation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;average-feature-point-drift_thumb.png&#34; alt=&#34;average-feature-point-drift&#34; title=&#34;average-feature-point-drift&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4  Average feature point drift&lt;/p&gt;

&lt;p&gt;At final we estimate a quality of tracking by measuring distance between actual position of tracked points and expected position that was pre-calculated early. Im very surprised of results showed by ORB detector  it shows the smallest drift! This will decrease systematic error while tracking a long image sequence. Very nice!&lt;/p&gt;

&lt;h4 id=&#34;comparison-to-previous-results&#34;&gt;Comparison to previous results&lt;/h4&gt;

&lt;p&gt;When I got the new results I decided to compare them with previous test results just in case. I knew that guys from Willow garage did some changes in the features2d module. And I was curious what actually they did.&lt;/p&gt;

&lt;p&gt;Most of all, I was curious about the performance. As you can see from Figure 5  there are really some changes happened. SURF detector becomes significantly slower in comparison to 2.2 version, but the GoodFeaturesToTrack becomes work faster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;performance-difference-between-2_2-and-2_3_1_thumb.png&#34; alt=&#34;performance-difference-between-2_2-and-2_3_1&#34; title=&#34;performance-difference-between-2_2-and-2_3_1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 5 - Change of detection time&lt;/p&gt;

&lt;p&gt;On the Figure 6 you can see the same performance difference test for one feature point. This shows how expensive detection in terms of one feature.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;performance_improvements_from_2_2_to_2_3_1_thumb.png&#34; alt=&#34;performance_improvements_from_2_2_to_2_3_1&#34; title=&#34;performance_improvements_from_2_2_to_2_3_1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 6 - Change of detection time per one feature point&lt;/p&gt;

&lt;p&gt;Sad but true  SURF detector in OpenCV 2.3.1 became even more slower than in 2.2! I cant figure out what the reason of such performance degradation, because I used the same compiler settings as I used for 2.2 tests. Anyway, the are also a good news  GoodFeaturesToTrack detector becomes a bit faster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;changes-in-tracking-robustness__thumb.png&#34; alt=&#34;changes-in-tracking-robustness_&#34; title=&#34;changes-in-tracking-robustness_&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 6  Feature point drift changes&lt;/p&gt;

&lt;p&gt;It seems that something were changed in the detection routines because average pixel drift differs for every type of detector. Cant figure out what the reason of such results.&lt;/p&gt;

&lt;h4 id=&#34;instead-of-conclusion&#34;&gt;Instead of conclusion&lt;/h4&gt;

&lt;p&gt;I really want to see the automated regression tests for every new OpenCV release done by automated build system. This will provide a great help to developers because they can track how the algorithm change their behavior and why.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to morphology operations on images</title>
      <link>https://example.com/2011-02-16-introduction-to-morphology-operations-on-images/</link>
      <pubDate>Wed, 16 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/2011-02-16-introduction-to-morphology-operations-on-images/</guid>
      <description>

&lt;p&gt;A brief tutorial/intro to the mathematical morphology in image processing.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;more&#34;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&#34;basic-definitions&#34;&gt;Basic Definitions&lt;/h2&gt;

&lt;p&gt;The term morphology refers to the description of the properties of shape and structure of any objects. In the context of computer vision, this term refers to the description of the properties of shapes of areas on the image. Operations of mathematical morphology were originally defined as operations on sets, but it soon became clear that they are also useful in the processing tasks of the set of points in the two-dimensional space. Sets in mathematical morphology represent objects in the image. It is easy to see that the set of all background pixels of binary image is one of the options for a full description. In the first place mathematical morphology is used to extract some properties of the image, useful for its presentation and descriptions. For example, contours, skeletons and convex hulls. Also morphological methods are used in the preliminary and final image processing. For example, morphological filtering, thickening or thinning. The input data for the mathematical morphology are the two images: processed and special, depending on the type of operations and solve problems. Such a special image called primitive or structural element. Typically, a structural element is much smaller than the processed image. Structural element can be regarded as a description of the area with some form. It is clear that the shape can be arbitrary, as long as it can be represented as a binary image of a given size. In many image processing packages the most common structural elements have a special name: BOX [H, W]-rectangle of given size, DISK [R] - drive a given size, RING [R] - the ring of a given size. &lt;img src=&#34;CommonElements_thumb.jpg&#34; alt=&#34;CommonElements&#34; title=&#34;CommonElements&#34; /&gt;Typical form of structure elements The result of morphological operation depends on the size and configuration of the original image and the structural entity. The size of the structural element is usually equal to 3 * 3, 4 * 4 or 5 * 5 pixels. This is due to the main idea of the morphological processing, which are search of the specific image detail. The desired item is described by a primitive, resulting in morphological processing, you can highlight or remove such items on the whole image. One major advantage of the morphological process is its simplicity: both the input and output processing procedure, we obtain the binarized image. Other techniques, usually from the original image first get a grayscale image, which is then reduced to binary using a threshold function.&lt;/p&gt;

&lt;h2 id=&#34;basic-operations&#34;&gt;Basic operations&lt;/h2&gt;

&lt;p&gt;The basic operations of mathematical morphology are the dilation, erosion, closure and disconnection. In these names capture the essence of operations: dilation increases the image, and erosion makes it less, closure operation allows you to close the inner hole region and eliminate the bays along the border area, the operation of disconnection helps get rid of small fragments, protruding regions near its borders. Next will be presented a mathematical definition of morphological operations.&lt;/p&gt;

&lt;h4 id=&#34;the-union-intersection-complement-difference&#34;&gt;The union, intersection, complement, difference&lt;/h4&gt;

&lt;p&gt;Before proceeding to the operations of morphology, it makes sense to consider the set-theoretic operations underlying mathematical morphology. The union of two sets A and B, which is denoted C = A  B, is by definition the set of all elements belonging to either the set A, or set B, or both sets simultaneously. Similarly, the intersection of two sets A and B, denoted C = A  B, is by definition the set of all elements that belong simultaneously to both sets A and B. Complement of A is a set of elements not contained in A: Ac = {w | w  A}. The difference of two sets A and B is denoted A \ B and is defined as follows: A \ B = {w  w  A, w  B} = A  Bc. This set consists of elements A, are not included in set B. Consider all the above operations on a specific example. &lt;img src=&#34;BasicOperationsSourceImages_thumb.png&#34; alt=&#34;BasicOperationsSourceImages&#34; title=&#34;BasicOperationsSourceImages&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;BasicOperationsUnion_thumb.png&#34; alt=&#34;BasicOperationsUnion&#34; title=&#34;BasicOperationsUnion&#34; /&gt; C = A  B Union
&lt;img src=&#34;BasicOperationsIntersection_thumb.png&#34; alt=&#34;BasicOperationsIntersection&#34; title=&#34;BasicOperationsIntersection&#34; /&gt; C = A  B Intersection&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;BasicOperationsComplement_thumb.png&#34; alt=&#34;BasicOperationsComplement&#34; title=&#34;BasicOperationsComplement&#34; /&gt; C = Ac Complement
&lt;img src=&#34;BasicOperationsSubstraction_thumb.png&#34; alt=&#34;BasicOperationsSubstraction&#34; title=&#34;BasicOperationsSubstraction&#34; /&gt; C = A \ B Difference&lt;/p&gt;

&lt;h4 id=&#34;translation&#34;&gt;Translation&lt;/h4&gt;

&lt;p&gt;Translation operation of Xt set of pixels X and the vector t is given in the form Xt = {x + t | x  X}. Consequently, the transfer of many individual pixels on the binary image shifts all the pixels are set at a specified distance. Translation vector t can be specified as an ordered pair (r, c), where r - a component of the transfer vector in the direction of rows, and c - a component of the transfer vector in the direction of the image columns. &lt;img src=&#34;TranslationExample_thumb.jpg&#34; alt=&#34;TranslationExample&#34; title=&#34;TranslationExample&#34; /&gt; Translation example&lt;/p&gt;

&lt;h4 id=&#34;dilation-erosion-contour-closure-opening&#34;&gt;Dilation, erosion, contour closure, opening&lt;/h4&gt;

&lt;p&gt;For the next operations we consider a specific example. Suppose we have the following binary image and a structural element:&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>