<!doctype html><html dir=ltr lang=en data-theme class=html><head><title>Eugene Khvedchenya
|
OpenCV Tutorial - Part 3</title><meta charset=utf-8><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Eugene Khvedchenya"><meta name=description content="A personal blog about Computer Vision, AI, Kaggle, Open-Source and research. You will never read this in books."><link rel=stylesheet href=/scss/main.min.b2e0cb07595e3519ab1193bb421914e06c0e26b0cc561fef23b3c6131d4d2ffa.css integrity="sha256-suDLB1leNRmrEZO7QhkU4GwOJrDMVh/vI7PGEx1NL/o=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.31b0a1f317f55c529a460897848c97436bb138b19c399b37de70d463a8bf6ed5.css integrity="sha256-MbCh8xf1XFKaRgiXhIyXQ2uxOLGcOZs33nDUY6i/btU=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=canonical href=https://computer-vision-talks.com/post/2012-06-27-opencv-tutorial-part-3/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.560a26330d27ff44a44e83b53cd07a95d4230a65930d31c5c76a8d481e5b35bf.js integrity="sha256-VgomMw0n/0SkToO1PNB6ldQjCmWTDTHFx2qNSB5bNb8=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="OpenCV Tutorial - Part 3"><meta name=twitter:description content="In Part 1 and Part 2 we created base application for our &ldquo;OpenCV Tutorial&rdquo; application. In this part we add video source to process frames using our samples and present the result to user. As usual, you can find source code for this application at github.
Video capture in iOS At this moment (as far as i know) there OpenCV&rsquo;s cv::VideoCapture does not support iOS platform. Therefore we have to use iOS AVFoundation API to setup video capture."><meta property="og:title" content="OpenCV Tutorial - Part 3"><meta property="og:description" content="In Part 1 and Part 2 we created base application for our &ldquo;OpenCV Tutorial&rdquo; application. In this part we add video source to process frames using our samples and present the result to user. As usual, you can find source code for this application at github.
Video capture in iOS At this moment (as far as i know) there OpenCV&rsquo;s cv::VideoCapture does not support iOS platform. Therefore we have to use iOS AVFoundation API to setup video capture."><meta property="og:type" content="article"><meta property="og:url" content="https://computer-vision-talks.com/post/2012-06-27-opencv-tutorial-part-3/"><meta property="article:section" content="post"><meta property="article:published_time" content="2012-06-27T00:00:00+00:00"><meta property="article:modified_time" content="2012-06-27T00:00:00+00:00"><meta property="og:site_name" content="Computer Vision Talks"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"OpenCV Tutorial - Part 3","headline":"OpenCV Tutorial - Part 3","alternativeHeadline":"","description":"
      
        In Part 1 and Part 2 we created base application for our \u0026ldquo;OpenCV Tutorial\u0026rdquo; application. In this part we add video source to process frames using our samples and present the result to user. As usual, you can find source code for this application at github.\nVideo capture in iOS At this moment (as far as i know) there OpenCV\u0026rsquo;s cv::VideoCapture does not support iOS platform. Therefore we have to use iOS AVFoundation API to setup video capture.


      


    ","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/computer-vision-talks.com\/post\/2012-06-27-opencv-tutorial-part-3\/"},"author":{"@type":"Person","name":"Eugene Khvedchenya"},"creator":{"@type":"Person","name":"Eugene Khvedchenya"},"accountablePerson":{"@type":"Person","name":"Eugene Khvedchenya"},"copyrightHolder":{"@type":"Person","name":"Eugene Khvedchenya"},"copyrightYear":"2012","dateCreated":"2012-06-27T00:00:00.00Z","datePublished":"2012-06-27T00:00:00.00Z","dateModified":"2012-06-27T00:00:00.00Z","publisher":{"@type":"Organization","name":"Eugene Khvedchenya","url":"https://computer-vision-talks.com/","logo":{"@type":"ImageObject","url":"https:\/\/computer-vision-talks.com\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/computer-vision-talks.com\/post\/2012-06-27-opencv-tutorial-part-3\/","wordCount":"930","genre":[],"keywords":["opencv","xcode","tutorials"]}</script></head><body class="body theme--light"><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/images/4ECfkGJr_400x400.jpg alt="profile picture"><div class=sidebar__introduction-title><a href=/>Computer Vision Talks</a></div><div class=sidebar__introduction-description><p>A personal blog about Computer Vision, AI, Kaggle, Open-Source and research. You will never read this in books.</p></div></div><ul class=sidebar__list><li class=sidebar__list-item><a href=https://www.linkedin.com/in/cvtalks/ target=_blank rel=noopener aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://twitter.com/cvtalks target=_blank rel=noopener aria-label=Twitter title=Twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://github.com/BloodAxe target=_blank rel=noopener aria-label=GitHub title=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=mailto:ekhvedchenya@computer-vision-talks.com target=_blank rel=noopener aria-label=E-Mail title=E-Mail><i class="fas fa-envelope fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
Eugene Khvedchenya
2022</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.9c062c557275acbaba71f0c7cd4024da3e3cc825d248bc4b2130811b0965330b.js integrity="sha256-nAYsVXJ1rLq6cfDHzUAk2j48yCXSSLxLITCBGwllMws=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-10687369-5","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/ title>Home</a></li><li class=nav__list-item><a href=/post/ title>Posts</a></li><li class=nav__list-item><a href=/open-source title>Open-Source</a></li><li class=nav__list-item><a href=/challenges title>Wins</a></li><li class=nav__list-item><a href=/publications title>Publications</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>OpenCV Tutorial - Part 3</h1><ul class=post__meta><li class=post__meta-item><em class="fas fa-calendar-day post__meta-icon"></em>
<span class=post__meta-text>Wed, Jun 27, 2012</span></li><li class=post__meta-item><em class="fas fa-stopwatch post__meta-icon"></em>
<span class=post__meta-text>5-minute read</span></li></ul><p>In <a href=http://computer-vision-talks.com/2012/06/opencv-tutorial-a-collection-of-opencv-samples-for-iphoneipad-part-1/ title="OpenCV Tutorial – a collection of OpenCV samples for iPhone/iPad – Part 1">Part 1</a> and <a href=http://computer-vision-talks.com/2012/06/opencv-tutorial-part-2/ title="OpenCV Tutorial – Part 2">Part 2</a> we created base application for our &ldquo;OpenCV Tutorial&rdquo; application. In this part we add video source to process frames using our samples and present the result to user. As usual, you can find source code for this application at <a href=https://github.com/BloodAxe/OpenCV-Tutorial title="OpenCV Tutorial Repository">github</a>.</p><h2 id=video-capture-in-ios>Video capture in iOS</h2><p>At this moment (as far as i know) there OpenCV&rsquo;s cv::VideoCapture does not support iOS platform. Therefore we have to use iOS AVFoundation API to setup video capture. This is more complicated that write cv::VideoCapture(&ldquo;YourVideoFileName.avi&rdquo;) but it&rsquo;s not a rocket science.  There is a great Apple documentation article <a href=http://developer.apple.com/library/ios/#DOCUMENTATION/AudioVideo/Conceptual/AVFoundationPG/Articles/00_Introduction.html title="AV Foundation Programming Guide">AV Foundation Programming Guide</a> that i strongly advice you to read. <strong>By the way, before I forget - video capture is not supported on iOS simulator. You&rsquo;ll need real device to test your app!</strong> To manage the capture from a device such as a camera or microphone, you assemble objects to represent inputs and outputs, and use an instance of <code>AVCaptureSession</code> to coordinate the data flow between them. Minimally you need:</p><ul><li>An instance of <code>AVCaptureDevice</code> to represent the input device, such as a camera or microphone</li><li>An instance of a concrete subclass of <code>AVCaptureInput</code> to configure the ports from the input device</li><li>An instance of a concrete subclass of <code>AVCaptureOutput</code> to manage the output to a movie file or still image</li><li>An instance of <code>AVCaptureSession</code> to coordinate the data flow from the input to the output</li></ul><p>To put all things together we introduce VideoSource class which incapsulate initialization and video capture routine. Let&rsquo;s take a look on it&rsquo;s interface:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-objectivec data-lang=objectivec><span class=line><span class=cl><span class=k>@protocol</span> <span class=nc>VideoSourceDelegate</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>-</span> <span class=p>(</span><span class=kt>void</span><span class=p>)</span> <span class=nf>frameCaptured:</span><span class=p>(</span><span class=n>cv</span><span class=o>::</span><span class=n>Mat</span><span class=p>)</span> <span class=nv>frame</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>@end</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>@interface</span> <span class=nc>VideoSource</span> : <span class=nc>NSObject</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>@property</span> <span class=kt>id</span> <span class=n>delegate</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>-</span> <span class=p>(</span><span class=kt>bool</span><span class=p>)</span> <span class=nf>hasMultipleCameras</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>-</span> <span class=p>(</span><span class=kt>void</span><span class=p>)</span> <span class=nf>toggleCamera</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>-</span> <span class=p>(</span><span class=kt>void</span><span class=p>)</span> <span class=nf>startRunning</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>-</span> <span class=p>(</span><span class=kt>void</span><span class=p>)</span> <span class=nf>stopRunning</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>@end</span>
</span></span></code></pre></div><p>The VideoSourceDelegate protocol defines a callback procedure that user code can handle. The frameCaptured method is called when the frame is received from a camera device. We wrap it in cv::Mat structure for latter use. Initialization of VideoCamera is also not complicated: we create capture session, add video input and video output:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-objectivec data-lang=objectivec><span class=line><span class=cl><span class=p>-</span> <span class=p>(</span><span class=kt>id</span><span class=p>)</span> <span class=nf>init</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=nb>self</span> <span class=o>=</span> <span class=p>[</span><span class=nb>super</span> <span class=n>init</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>currentCameraIndex</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>session</span> <span class=o>=</span> <span class=p>[[</span><span class=n>AVCaptureSession</span> <span class=n>alloc</span><span class=p>]</span> <span class=n>init</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=n>session</span> <span class=nl>setSessionPreset</span><span class=p>:</span><span class=n>AVCaptureSessionPreset640x480</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>captureDevices</span> <span class=o>=</span> <span class=p>[</span><span class=n>AVCaptureDevice</span> <span class=nl>devicesWithMediaType</span><span class=p>:</span><span class=n>AVMediaTypeVideo</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>AVCaptureDevice</span> <span class=o>*</span><span class=n>videoDevice</span> <span class=o>=</span> <span class=p>[</span><span class=n>captureDevices</span> <span class=nl>objectAtIndex</span><span class=p>:</span><span class=mi>0</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>NSError</span> <span class=o>*</span> <span class=n>error</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>captureInput</span> <span class=o>=</span> <span class=p>[</span><span class=n>AVCaptureDeviceInput</span> <span class=nl>deviceInputWithDevice</span><span class=p>:</span><span class=n>videoDevice</span> <span class=nl>error</span><span class=p>:</span><span class=o>&amp;</span><span class=n>error</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>error</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>NSLog</span><span class=p>(</span><span class=s>@&#34;Couldn&#39;t create video input&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>captureOutput</span> <span class=o>=</span> <span class=p>[[</span><span class=n>AVCaptureVideoDataOutput</span> <span class=n>alloc</span><span class=p>]</span> <span class=n>init</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>captureOutput</span><span class=p>.</span><span class=n>alwaysDiscardsLateVideoFrames</span> <span class=o>=</span> <span class=nb>YES</span><span class=p>;</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Set the video output to store frame in BGRA (It is supposed to be faster)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>NSString</span><span class=o>*</span> <span class=n>key</span> <span class=o>=</span> <span class=p>(</span><span class=n>NSString</span><span class=o>*</span><span class=p>)</span><span class=n>kCVPixelBufferPixelFormatTypeKey</span><span class=p>;</span> 
</span></span><span class=line><span class=cl>    <span class=n>NSNumber</span><span class=o>*</span> <span class=n>value</span> <span class=o>=</span> <span class=p>[</span><span class=n>NSNumber</span> <span class=nl>numberWithUnsignedInt</span><span class=p>:</span><span class=n>kCVPixelFormatType_32BGRA</span><span class=p>];</span> 
</span></span><span class=line><span class=cl>    <span class=n>NSDictionary</span><span class=o>*</span> <span class=n>videoSettings</span> <span class=o>=</span> <span class=p>[</span><span class=n>NSDictionary</span> <span class=nl>dictionaryWithObject</span><span class=p>:</span><span class=n>value</span> <span class=nl>forKey</span><span class=p>:</span><span class=n>key</span><span class=p>];</span> 
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=n>captureOutput</span> <span class=nl>setVideoSettings</span><span class=p>:</span><span class=n>videoSettings</span><span class=p>];</span>    
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=cm>/*We create a serial queue to handle the processing of our frames*/</span>
</span></span><span class=line><span class=cl>    <span class=n>dispatch_queue_t</span> <span class=n>queue</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>queue</span> <span class=o>=</span> <span class=n>dispatch_queue_create</span><span class=p>(</span><span class=s>&#34;com.computer-vision-talks.cameraQueue&#34;</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=n>captureOutput</span> <span class=nl>setSampleBufferDelegate</span><span class=p>:</span><span class=nb>self</span> <span class=nl>queue</span><span class=p>:</span><span class=n>queue</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>dispatch_release</span><span class=p>(</span><span class=n>queue</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=n>session</span> <span class=nl>addInput</span><span class=p>:</span><span class=n>captureInput</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=n>session</span> <span class=nl>addOutput</span><span class=p>:</span><span class=n>captureOutput</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nb>self</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Please not that we query all video devices available using the [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo] call to get them all. This feature allows us to toggle between front and rear cameras in runtime. Also we set capture session preset to 640x480 because the larger the image the more it it need to be processed by our algorithms. 640x480 is a good choice. As our last step we configure capture output to pass our frames in BGRA format. For our image processing it&rsquo;s the ideal case. In case when our video source has several video inputs we can toggle between them using toggleCamera method:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-objectivec data-lang=objectivec><span class=line><span class=cl><span class=p>-</span> <span class=p>(</span><span class=kt>void</span><span class=p>)</span> <span class=nf>toggleCamera</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>currentCameraIndex</span><span class=o>++</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>camerasCount</span> <span class=o>=</span> <span class=p>[</span><span class=n>captureDevices</span> <span class=n>count</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=n>currentCameraIndex</span> <span class=o>=</span> <span class=n>currentCameraIndex</span> <span class=o>%</span> <span class=n>camerasCount</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>AVCaptureDevice</span> <span class=o>*</span><span class=n>videoDevice</span> <span class=o>=</span> <span class=p>[</span><span class=n>captureDevices</span> <span class=nl>objectAtIndex</span><span class=p>:</span><span class=n>currentCameraIndex</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=p>[</span><span class=n>session</span> <span class=n>beginConfiguration</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>captureInput</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=n>session</span> <span class=nl>removeInput</span><span class=p>:</span><span class=n>captureInput</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>NSError</span> <span class=o>*</span> <span class=n>error</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>captureInput</span> <span class=o>=</span> <span class=p>[</span><span class=n>AVCaptureDeviceInput</span> <span class=nl>deviceInputWithDevice</span><span class=p>:</span><span class=n>videoDevice</span> <span class=nl>error</span><span class=p>:</span><span class=o>&amp;</span><span class=n>error</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>error</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>NSLog</span><span class=p>(</span><span class=s>@&#34;Couldn&#39;t create video input&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=p>[</span><span class=n>session</span> <span class=nl>addInput</span><span class=p>:</span><span class=n>captureInput</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>[</span><span class=n>session</span> <span class=nl>setSessionPreset</span><span class=p>:</span><span class=n>AVCaptureSessionPreset640x480</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>[</span><span class=n>session</span> <span class=n>commitConfiguration</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>In this function we cycle through available inputs and add them to capture session (do not forget to remove previous input) and set the capture preset again (just in case). Switching between from and rear camera is initiated by the user by tapping on toggle button (we will talk about it a bit later). Just one thing left - we should implement AVCaptureVideoDataOutputSampleBufferDelegate in our VideoSource class to receive frames from AVFoundation API like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-objectivec data-lang=objectivec><span class=line><span class=cl><span class=p>-</span> <span class=p>(</span><span class=kt>void</span><span class=p>)</span><span class=nf>captureOutput:</span><span class=p>(</span><span class=n>AVCaptureOutput</span> <span class=o>*</span><span class=p>)</span><span class=nv>captureOutput</span> 
</span></span><span class=line><span class=cl><span class=nf>didOutputSampleBuffer:</span><span class=p>(</span><span class=n>CMSampleBufferRef</span><span class=p>)</span><span class=nv>sampleBuffer</span> 
</span></span><span class=line><span class=cl>       <span class=nf>fromConnection:</span><span class=p>(</span><span class=n>AVCaptureConnection</span> <span class=o>*</span><span class=p>)</span><span class=nv>connection</span> 
</span></span><span class=line><span class=cl><span class=p>{</span> 
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=n>delegate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>CVImageBufferRef</span> <span class=n>imageBuffer</span> <span class=o>=</span> <span class=n>CMSampleBufferGetImageBuffer</span><span class=p>(</span><span class=n>sampleBuffer</span><span class=p>);</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=cm>/*Lock the image buffer*/</span>
</span></span><span class=line><span class=cl>  <span class=n>CVPixelBufferLockBaseAddress</span><span class=p>(</span><span class=n>imageBuffer</span><span class=p>,</span><span class=mi>0</span><span class=p>);</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=cm>/*Get information about the image*/</span>
</span></span><span class=line><span class=cl>  <span class=n>uint8_t</span> <span class=o>*</span><span class=n>baseAddress</span> <span class=o>=</span> <span class=p>(</span><span class=n>uint8_t</span> <span class=o>*</span><span class=p>)</span><span class=n>CVPixelBufferGetBaseAddress</span><span class=p>(</span><span class=n>imageBuffer</span><span class=p>);</span> 
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>width</span> <span class=o>=</span> <span class=n>CVPixelBufferGetWidth</span><span class=p>(</span><span class=n>imageBuffer</span><span class=p>);</span> 
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>height</span> <span class=o>=</span> <span class=n>CVPixelBufferGetHeight</span><span class=p>(</span><span class=n>imageBuffer</span><span class=p>);</span>  
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>stride</span> <span class=o>=</span> <span class=n>CVPixelBufferGetBytesPerRow</span><span class=p>(</span><span class=n>imageBuffer</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=c1>//NSLog(@&#34;Frame captured: %lu x %lu&#34;, width,height);
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>  <span class=n>cv</span><span class=o>::</span><span class=n>Mat</span> <span class=n>frame</span><span class=p>(</span><span class=n>height</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>CV_8UC4</span><span class=p>,</span> <span class=p>(</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span><span class=n>baseAddress</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=p>[</span><span class=n>delegate</span> <span class=nl>frameCaptured</span><span class=p>:</span><span class=n>frame</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=cm>/*We unlock the  image buffer*/</span>
</span></span><span class=line><span class=cl>  <span class=n>CVPixelBufferUnlockBaseAddress</span><span class=p>(</span><span class=n>imageBuffer</span><span class=p>,</span><span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>In this method we obtain raw data in BGRA format from CoreMedia and put them input cv::Mat (no data is copied here). And then we call [delegate frameCaptured:] to inform user code that new frame is available.</p><h2 id=displaying-processed-frames>Displaying processed frames</h2><p>Since we working with video processing it&rsquo;s obviously to present the result of video processing in real-time on the screen. To secure the appropriate FPS we have to redraw our screen fast. The existing UIImageView control cannot be used here because we&rsquo;ll spent to much time on unnecessary image conversion (cv::Mat to UIImage) and drawing UIImage on the ImageView. For our goal the fastest way to draw the picture is to use OpenGL ES and GLView to draw our bitmap using GPU. In this section i&rsquo;ll show you how to create simple view that able to draw cv::Mat in real-time. To secure this&rsquo;ll UIView and to EAGLContext using it. This allows us to use OpenGL API to draw textured rectangle on top of our view as fast as possible. For user code we expose only single function that is can use:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-objectivec data-lang=objectivec><span class=line><span class=cl><span class=k>@interface</span> <span class=nc>GLESImageView</span> : <span class=nc>UIView</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>-</span> <span class=p>(</span><span class=kt>void</span><span class=p>)</span><span class=nf>drawFrame:</span><span class=p>(</span><span class=k>const</span> <span class=n>cv</span><span class=o>::</span><span class=n>Mat</span><span class=o>&amp;</span><span class=p>)</span> <span class=nv>bgraFrame</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>@end</span>
</span></span></code></pre></div></div><div class=post__footer><span><a class=tag href=/tags/opencv/>opencv</a><a class=tag href=/tags/xcode/>xcode</a><a class=tag href=/tags/tutorials/>tutorials</a></span></div><div id=comment><h2>comments</h2><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//cvtalks.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
Eugene Khvedchenya
2022</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.9c062c557275acbaba71f0c7cd4024da3e3cc825d248bc4b2130811b0965330b.js integrity="sha256-nAYsVXJ1rLq6cfDHzUAk2j48yCXSSLxLITCBGwllMws=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-10687369-5","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>